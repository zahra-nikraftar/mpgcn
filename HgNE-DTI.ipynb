{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c525cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f7f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='GRDTI')\n",
    "\n",
    "    parser.add_argument(\"--epochs\", type=int, default=4000,\n",
    "                        help=\"number of training epochs\")\n",
    "    parser.add_argument(\"--rounds\", type=int, default=1,\n",
    "                        help=\"number of training rounds\")\n",
    "    parser.add_argument(\"--device\", default='cuda',\n",
    "                        help=\"cuda or cpu\")\n",
    "    parser.add_argument(\"--dim-embedding\", type=int, default=1000,\n",
    "                        help=\"dimension of embeddings\")\n",
    "    parser.add_argument(\"--k\", type=int, default=3,\n",
    "                        help=\"Number of iterations in propagation\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001,\n",
    "                        help=\"learning rate\")\n",
    "    parser.add_argument('--weight-decay', type=float, default=0,\n",
    "                        help=\"weight decay\")\n",
    "    parser.add_argument('--reg_lambda', type=float, default=1,\n",
    "                        help=\"reg_lambda\")\n",
    "    parser.add_argument('--patience', type=int, default=6,\n",
    "                        help='Early stopping patience.')\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.9,\n",
    "                        help=\"Restart Probability\")\n",
    "    parser.add_argument(\"--edge-drop\", type=float, default=0.5,\n",
    "                        help=\"edge dropout in propagation\")\n",
    "    parser.add_argument('-f'),\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def row_normalize(t):\n",
    "    t = t.float()\n",
    "    row_sums = t.sum(1) + 1e-12\n",
    "    output = t / row_sums[:, None]\n",
    "    output[th.isnan(output) | th.isinf(output)] = 0.0\n",
    "    return output\n",
    "\n",
    "\n",
    "def col_normalize(a_matrix, substract_self_loop):\n",
    "    if substract_self_loop:\n",
    "        np.fill_diagonal(a_matrix, 0)\n",
    "    a_matrix = a_matrix.astype(float)\n",
    "    col_sums = a_matrix.sum(axis=0) + 1e-12\n",
    "    new_matrix = a_matrix / col_sums[np.newaxis, :]\n",
    "    new_matrix[np.isnan(new_matrix) | np.isinf(new_matrix)] = 0.0\n",
    "    return new_matrix\n",
    "\n",
    "\n",
    "def l2_norm(t, axit=1):\n",
    "    t = t.float()\n",
    "    norm = th.norm(t, 2, axit, True) + 1e-12\n",
    "    output = th.div(t, norm)\n",
    "    output[th.isnan(output) | th.isinf(output)] = 0.0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3efb27e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch import nn\n",
    "from dgl import function as fn\n",
    "\n",
    "\n",
    "class Propagation(nn.Module):\n",
    "    def __init__(self, k, alpha, edge_drop=0.):\n",
    "        super(Propagation, self).__init__()\n",
    "        self._k = k\n",
    "        self._alpha = alpha\n",
    "        self.edge_drop = nn.Dropout(edge_drop)\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        graph = graph.local_var().to('cuda')\n",
    "        norm = th.pow(graph.in_degrees().float().clamp(min=1e-12), -0.5)\n",
    "        shp = norm.shape + (1,) * (feat.dim() - 1)\n",
    "        norm = th.reshape(norm, shp).to(feat.device)\n",
    "        feat_0 = feat\n",
    "        for _ in range(self._k):\n",
    "            feat = feat * norm\n",
    "            graph.ndata['h'] = feat\n",
    "            #graph.edata['w'] = th.ones(graph.number_of_edges(), 1).to(feat.device)\n",
    "            graph.edata['w'] = self.edge_drop(th.ones(graph.number_of_edges(), 1).to(feat.device))\n",
    "            graph.update_all(fn.u_mul_e('h', 'w', 'm'), fn.sum('m', 'h'))\n",
    "            feat = graph.ndata.pop('h')\n",
    "            feat = feat * norm\n",
    "            feat = (1 - self._alpha) * feat + self._alpha * feat_0\n",
    "\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf10957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MetaPath2Vec\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "from dgl.base import NID\n",
    "from dgl.convert import to_homogeneous, to_heterogeneous\n",
    "from dgl.random import choice\n",
    "from dgl.sampling import random_walk\n",
    "\n",
    "\n",
    "class MetaPath2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 metapath,\n",
    "                 window_size,\n",
    "                 emb_dim=128,\n",
    "                 negative_size=5,\n",
    "                 sparse=True):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(metapath) + 1 >= window_size, \\\n",
    "            f'Expect len(metapath) >= window_size - 1, got {metapath} and {window_size}'\n",
    "\n",
    "        self.hg = g\n",
    "        self.emb_dim = emb_dim\n",
    "        self.metapath = metapath\n",
    "        self.window_size = window_size\n",
    "        self.negative_size = negative_size\n",
    "        self.device = th.device(args.device)\n",
    "        \n",
    "\n",
    "        # convert edge metapath to node metapath\n",
    "        # get initial source node type\n",
    "        src_type, _, _ = g.to_canonical_etype(metapath[0])\n",
    "        node_metapath = [src_type]\n",
    "        for etype in metapath:\n",
    "            _, _, dst_type = g.to_canonical_etype(etype)\n",
    "            node_metapath.append(dst_type)\n",
    "        self.node_metapath = node_metapath\n",
    "\n",
    "        # Convert the graph into a homogeneous one for global to local node ID mapping\n",
    "        g = to_homogeneous(g)\n",
    "        #g.to('cuda')\n",
    "        # Convert it back to the hetero one for local to global node ID mapping\n",
    "        hg = to_heterogeneous(g, self.hg.ntypes, self.hg.etypes)\n",
    "        #hg.to('cuda')\n",
    "        local_to_global_nid = hg.ndata[NID]\n",
    "        for key, val in local_to_global_nid.items():\n",
    "            #local_to_global_nid[key] = list(val.cpu().numpy())\n",
    "            local_to_global_nid[key] = list(val.to(torch.device(\"cuda\")))\n",
    "        self.local_to_global_nid = local_to_global_nid\n",
    "\n",
    "        num_nodes_total = hg.num_nodes()\n",
    "        node_frequency = torch.zeros(num_nodes_total)\n",
    "        # random walk\n",
    "        for idx in tqdm.trange(hg.num_nodes(node_metapath[0])):\n",
    "            traces, _ = random_walk(g=hg, nodes=[idx], metapath=metapath)\n",
    "            #for tr in traces.cpu().numpy():\n",
    "            for tr in traces.to(torch.device(\"cuda\")):\n",
    "                tr_nids = [\n",
    "                    self.local_to_global_nid[node_metapath[i]][tr[i]] for i in range(len(tr))]\n",
    "                \n",
    "                #node_frequency.append((torch.ones(size=(len(traces),)) * i).to('cuda'))\n",
    "                node_frequency[torch.LongTensor(tr_nids)] += 1\n",
    "                \n",
    "\n",
    "        neg_prob = node_frequency.pow(0.75)\n",
    "        self.neg_prob = neg_prob / neg_prob.sum()\n",
    "\n",
    "        # center node embedding\n",
    "        self.node_embed = nn.Embedding(num_nodes_total, self.emb_dim, sparse=sparse)\n",
    "        #self.node_embed.to(device)\n",
    "        self.context_embed = nn.Embedding(num_nodes_total, self.emb_dim, sparse=sparse)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters\"\"\"\n",
    "        init_range = 1.0 / self.emb_dim\n",
    "        init.uniform_(self.node_embed.weight.data, -init_range, init_range)\n",
    "        init.constant_(self.context_embed.weight.data, 0)\n",
    "\n",
    "\n",
    "    def sample(self, indices):\n",
    "        device = th.device(args.device)\n",
    "        \n",
    "        traces, _ = random_walk(g=self.hg, nodes=indices, metapath=self.metapath)\n",
    "        u_list = []\n",
    "        v_list = []\n",
    "        #for tr in traces.cpu().numpy():\n",
    "        for tr in traces.to(torch.device(\"cuda\")):\n",
    "            tr_nids = [\n",
    "                self.local_to_global_nid[self.node_metapath[i]][tr[i]] for i in range(len(tr))]\n",
    "            for i, u in enumerate(tr_nids):\n",
    "                for j, v in enumerate(tr_nids[max(i - self.window_size, 0):i + self.window_size]):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    u_list.append(u)\n",
    "                    v_list.append(v)\n",
    "\n",
    "        neg_v = choice(self.hg.num_nodes(), size=len(u_list) * self.negative_size,\n",
    "                       prob=self.neg_prob).reshape(len(u_list), self.negative_size)\n",
    "\n",
    "        return torch.cuda.LongTensor(u_list), torch.cuda.LongTensor(v_list), neg_v.to(device='cuda')\n",
    "        #return torch.LongTensor(u_list).to('cuda'), torch.LongTensor(v_list).to('cuda'), neg_v.to('cuda')\n",
    "\n",
    "    def forward(self, pos_u, pos_v, neg_v):\n",
    "\n",
    "        emb_u = self.node_embed(pos_u)\n",
    "        #emb_u.to('cuda')\n",
    "        emb_v = self.context_embed(pos_v)\n",
    "        #emb_v.to('cuda')\n",
    "        emb_neg_v = self.context_embed(neg_v)\n",
    "        #emb_neg_v.to('cuda')\n",
    "\n",
    "        score = torch.sum(torch.mul(emb_u, emb_v), dim=1)\n",
    "        score = torch.clamp(score, max=10, min=-10)\n",
    "        score = -F.logsigmoid(score)\n",
    "\n",
    "        neg_score = torch.bmm(emb_neg_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        neg_score = torch.clamp(neg_score, max=10, min=-10)\n",
    "        neg_score = -torch.sum(F.logsigmoid(-neg_score), dim=1)\n",
    "\n",
    "        return torch.mean(score + neg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45df235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch.nn.functional as F\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SparseAdam\n",
    "\n",
    "\n",
    "\n",
    "class GRDTI(nn.Module):\n",
    "    def __init__(self, g, n_disease, n_drug, n_protein, n_sideeffect, args):\n",
    "        super(GRDTI, self).__init__()\n",
    "        self.g = g\n",
    "        self.device = th.device(args.device)\n",
    "        self.dim_embedding = args.dim_embedding\n",
    "\n",
    "        self.activation = F.elu\n",
    "        self.reg_lambda = args.reg_lambda\n",
    "\n",
    "        self.num_disease = n_disease\n",
    "        self.num_drug = n_drug\n",
    "        self.num_protein = n_protein\n",
    "        self.num_sideeffect = n_sideeffect\n",
    "        \n",
    "        '''\n",
    "        Calculating Meta-Path2Vec for each node\n",
    "        \n",
    "        Constructing drug_feat_embedding\n",
    "        Constructing protein_feat_embedding\n",
    "        Constructing disease_feat_embedding\n",
    "        Constructing sideeffect_feat_embedding\n",
    "        '''\n",
    "        \n",
    "        '''metapath : Len(2) - Drug - MP2''' \n",
    "        #for metapath D-P-D\n",
    "        '''model_DPD = MetaPath2Vec(g, ['drug_protein interaction', 'protein_drug interaction'], window_size=1,emb_dim=1000)\n",
    "        model_DPD.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('drug')), batch_size=16, shuffle=True, collate_fn=model_DPD.sample)\n",
    "        optimizer = SparseAdam(model_DPD.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DPD(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Drug_P_nids = torch.cuda.LongTensor(model_DPD.local_to_global_nid['drug'])\n",
    "        Drug_P_emb = model_DPD.node_embed(Drug_P_nids)\n",
    "        Drug_P_emb = th.tensor(Drug_P_emb).to(self.device)\n",
    "        #print(Drug_P_emb)'''\n",
    "\n",
    "        #for metapath D-Di-D\n",
    "        model_DDiD = MetaPath2Vec(g, ['drug_disease association', 'disease_drug association'], window_size=1,emb_dim=1000)\n",
    "        model_DDiD.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('drug')), batch_size=16, shuffle=True, collate_fn=model_DDiD.sample)\n",
    "        optimizer = SparseAdam(model_DDiD.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DDiD(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Drug_Di_nids = torch.cuda.LongTensor(model_DDiD.local_to_global_nid['drug'])\n",
    "        Drug_Di_emb = model_DDiD.node_embed(Drug_Di_nids)\n",
    "        Drug_Di_emb = th.tensor(Drug_Di_emb).to(self.device)\n",
    "        #print(Drug_Di_emb)\n",
    "\n",
    "        #for metapath D-Si-D\n",
    "        '''model_DSiD = MetaPath2Vec(g, ['drug_sideeffect association', 'sideeffect_drug association'], window_size=1,emb_dim=1000)\n",
    "        model_DSiD.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('drug')), batch_size=16, shuffle=True, collate_fn=model_DSiD.sample)\n",
    "        optimizer = SparseAdam(model_DSiD.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DSiD(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Drug_Si_nids = torch.cuda.LongTensor(model_DSiD.local_to_global_nid['drug'])\n",
    "        Drug_Si_emb = model_DSiD.node_embed(Drug_Si_nids)\n",
    "        Drug_Si_emb = th.tensor(Drug_Si_emb).to(self.device)'''\n",
    "        \n",
    "        '''metapath : Len(3) - Drug'''\n",
    "        #for metapath D-P-Di-D\n",
    "        model_DPDiD = MetaPath2Vec(g, ['drug_protein interaction', 'protein_disease association',\n",
    "                                       'disease_drug association'],\n",
    "                                 window_size=1,emb_dim=1000)\n",
    "        model_DPDiD.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('drug')), batch_size=16, shuffle=True,\n",
    "                                collate_fn=model_DPDiD.sample)\n",
    "        optimizer = SparseAdam(model_DPDiD.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DPDiD(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Drug_PDi_nids = torch.cuda.LongTensor(model_DPDiD.local_to_global_nid['drug'])\n",
    "        Drug_PDi_emb = model_DPDiD.node_embed(Drug_PDi_nids)\n",
    "        Drug_PDi_emb = th.tensor(Drug_PDi_emb).to(self.device)\n",
    "        \n",
    "        '''#for metapath D-Di-P-D\n",
    "        model_DDiPD = MetaPath2Vec(g, ['drug_disease association', 'disease_protein association',\n",
    "                                       'protein_drug interaction'],\n",
    "                                 window_size=1,emb_dim=1000)\n",
    "        model_DDiPD.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('drug')), batch_size=16, shuffle=True,\n",
    "                                collate_fn=model_DDiPD.sample)\n",
    "        optimizer = SparseAdam(model_DDiPD.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DDiPD(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Drug_DiP_nids = torch.cuda.LongTensor(model_DDiPD.local_to_global_nid['drug'])\n",
    "        Drug_DiP_emb = model_DDiPD.node_embed(Drug_DiP_nids)\n",
    "        Drug_DiP_emb = th.tensor(Drug_DiP_emb).to(self.device)'''\n",
    "        \n",
    "        '''metapath : Len(4) - Drug'''\n",
    "        '''#for metapath D-P-Di-P-D\n",
    "        model_DD4_1 = MetaPath2Vec(g, ['drug_protein interaction','protein_disease association',\n",
    "                                     'disease_protein association', 'protein_drug interaction'],\n",
    "                                 window_size=1,emb_dim=1000)\n",
    "        model_DD4_1.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('drug')), batch_size=16, shuffle=True,\n",
    "                                collate_fn=model_DD4_1.sample)\n",
    "        optimizer = SparseAdam(model_DD4_1.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DD4_1(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Drug_D1_nids = torch.cuda.LongTensor(model_DD4_1.local_to_global_nid['drug'])\n",
    "        Drug_D1_emb = model_DD4_1.node_embed(Drug_D1_nids)\n",
    "        Drug_D1_emb = th.tensor(Drug_D1_emb).to(self.device)'''\n",
    "        \n",
    "        #for metapath D-Di-P-Di-D\n",
    "        model_DD4_2 = MetaPath2Vec(g, ['drug_disease association','disease_protein association',\n",
    "                                     'protein_disease association', 'disease_drug association'],\n",
    "                                 window_size=1,emb_dim=1000)\n",
    "        model_DD4_2.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('drug')), batch_size=16, shuffle=True,\n",
    "                                collate_fn=model_DD4_2.sample)\n",
    "        optimizer = SparseAdam(model_DD4_2.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DD4_2(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Drug_D2_nids = torch.cuda.LongTensor(model_DD4_2.local_to_global_nid['drug'])\n",
    "        Drug_D2_emb = model_DD4_2.node_embed(Drug_D2_nids)\n",
    "        Drug_D2_emb = th.tensor(Drug_D2_emb).to(self.device)\n",
    "        \n",
    "        '''#for metapath D-Si-D-P-D\n",
    "        model_DD4_3 = MetaPath2Vec(g, ['drug_sideeffect association','sideeffect_drug association',\n",
    "                                     'drug_protein interaction', 'protein_drug interaction'],\n",
    "                                 window_size=1,emb_dim=1000)\n",
    "        model_DD4_3.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('drug')), batch_size=16, shuffle=True,\n",
    "                                collate_fn=model_DD4_3.sample)\n",
    "        optimizer = SparseAdam(model_DD4_3.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DD4_3(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Drug_D3_nids = torch.cuda.LongTensor(model_DD4_3.local_to_global_nid['drug'])\n",
    "        Drug_D3_emb = model_DD4_3.node_embed(Drug_D3_nids)\n",
    "        Drug_D3_emb = th.tensor(Drug_D3_emb).to(self.device)'''\n",
    "        \n",
    "        #for metapath D-Si-D-Di-D\n",
    "        model_DD4_4 = MetaPath2Vec(g, ['drug_sideeffect association','sideeffect_drug association',\n",
    "                                      'drug_disease association', 'disease_drug association'],\n",
    "                                 window_size=1,emb_dim=1000)\n",
    "        model_DD4_4.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('drug')), batch_size=16, shuffle=True,\n",
    "                                collate_fn=model_DD4_4.sample)\n",
    "        optimizer = SparseAdam(model_DD4_4.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DD4_4(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Drug_D4_nids = torch.cuda.LongTensor(model_DD4_4.local_to_global_nid['drug'])\n",
    "        Drug_D4_emb = model_DD4_4.node_embed(Drug_D4_nids)\n",
    "        Drug_D4_emb = th.tensor(Drug_D4_emb).to(self.device)\n",
    "        \n",
    "        self.drug_feat_meta = torch.mean(torch.stack((Drug_Di_emb,\n",
    "                                                      Drug_PDi_emb,\n",
    "                                                      Drug_D2_emb, Drug_D4_emb),\n",
    "                                                     dim = 1), dim = 1)\n",
    "        \n",
    "        '''metapath : Len(2) - Protein'''\n",
    "        '''#for metapath P-D-P\n",
    "        model_PDP = MetaPath2Vec(g, ['protein_drug interaction', 'drug_protein interaction'],\n",
    "                                 window_size=1,emb_dim=1000)\n",
    "        model_PDP.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('protein')), batch_size=16, shuffle=True, collate_fn=model_PDP.sample)\n",
    "        optimizer = SparseAdam(model_PDP.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_PDP(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Protein_D_nids = torch.cuda.LongTensor(model_PDP.local_to_global_nid['protein'])\n",
    "        Protein_D_emb = model_PDP.node_embed(Protein_D_nids)\n",
    "        Protein_D_emb = th.tensor(Protein_D_emb).to(self.device)\n",
    "        #print(Protein_D_emb)'''\n",
    "\n",
    "        #for metapath P-Di-P\n",
    "        model_PDiP = MetaPath2Vec(g, ['protein_disease association', 'disease_protein association'],\n",
    "                                  window_size=1,emb_dim=1000)\n",
    "        model_PDiP.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('protein')), batch_size=16, shuffle=True,\n",
    "                                collate_fn=model_PDiP.sample)\n",
    "        optimizer = SparseAdam(model_PDiP.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_PDiP(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Protein_Di_nids = torch.cuda.LongTensor(model_PDiP.local_to_global_nid['protein'])\n",
    "        Protein_Di_emb = model_PDiP.node_embed(Protein_Di_nids)\n",
    "        Protein_Di_emb = th.tensor(Protein_Di_emb).to(self.device)\n",
    "        \n",
    "        '''metapath : Len(3) - Protein'''\n",
    "        #for metapath P-Di-D-P\n",
    "        model_PDiDP = MetaPath2Vec(g, ['protein_disease association','disease_drug association','drug_protein interaction'],\n",
    "                                 window_size=1,emb_dim=1000)\n",
    "        model_PDiDP.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('protein')), batch_size=16,\n",
    "                                shuffle=True, collate_fn=model_PDiDP.sample)\n",
    "        optimizer = SparseAdam(model_PDiDP.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_PDiDP(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Protein_DiP_nids = torch.cuda.LongTensor(model_PDiDP.local_to_global_nid['protein'])\n",
    "        Protein_DiDP_emb = model_PDiDP.node_embed(Protein_DiP_nids)\n",
    "        Protein_DiDP_emb = th.tensor(Protein_DiDP_emb).to(self.device)\n",
    "        \n",
    "        #for metapath P-D-Di-P\n",
    "        model_PDDiP = MetaPath2Vec(g, ['protein_drug interaction', 'drug_disease association', 'disease_protein association'],\n",
    "                                 window_size=1,emb_dim=1000)\n",
    "        model_PDDiP.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('protein')), batch_size=16,\n",
    "                                shuffle=True, collate_fn=model_PDDiP.sample)\n",
    "        optimizer = SparseAdam(model_PDDiP.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_PDDiP(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Protein_DDiP_nids = torch.cuda.LongTensor(model_PDDiP.local_to_global_nid['protein'])\n",
    "        Protein_DDiP_emb = model_PDDiP.node_embed(Protein_DDiP_nids)\n",
    "        Protein_DDiP_emb = th.tensor(Protein_DDiP_emb).to(self.device)\n",
    "        \n",
    "        '''metapath : Len(4) - Protein'''\n",
    "        '''#for metapath P-D-Di-D-P\n",
    "        \n",
    "        model_PP4_1 = MetaPath2Vec(g, ['protein_drug interaction',  'drug_disease association', \n",
    "                                       'disease_drug association','drug_protein interaction'],\n",
    "                                   window_size=1,emb_dim=1000)\n",
    "        model_PP4_1.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('protein')), batch_size=16,\n",
    "                                shuffle=True, collate_fn=model_PP4_1.sample)\n",
    "        optimizer = SparseAdam(model_PP4_1.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_PP4_1(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Protein_P1_nids = torch.cuda.LongTensor(model_PP4_1.local_to_global_nid['protein'])\n",
    "        Protein_P1_emb = model_PP4_1.node_embed(Protein_P1_nids)\n",
    "        Protein_P1_emb = th.tensor(Protein_P1_emb).to(self.device)'''\n",
    "        \n",
    "        #for metapath P-Di-D-Di-P\n",
    "        model_PP4_2 = MetaPath2Vec(g, ['protein_disease association',  'disease_drug association', \n",
    "                                       'drug_disease association','disease_protein association'],\n",
    "                                   window_size=1,emb_dim=1000)\n",
    "        model_PP4_2.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('protein')), batch_size=16,\n",
    "                                shuffle=True, collate_fn=model_PP4_2.sample)\n",
    "        optimizer = SparseAdam(model_PP4_2.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_PP4_2(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Protein_P2_nids = torch.cuda.LongTensor(model_PP4_2.local_to_global_nid['protein'])\n",
    "        Protein_P2_emb = model_PP4_2.node_embed(Protein_P2_nids)\n",
    "        Protein_P2_emb = th.tensor(Protein_P2_emb).to(self.device)\n",
    "        \n",
    "        '''#for metapath P-D-Si-D-P\n",
    "        model_PP4_3 = MetaPath2Vec(g, ['protein_drug interaction', 'drug_sideeffect association',\n",
    "                                       'sideeffect_drug association','drug_protein interaction'],\n",
    "                                   window_size=1,emb_dim=1000)\n",
    "        model_PP4_3.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('protein')), batch_size=16,\n",
    "                                shuffle=True, collate_fn=model_PP4_3.sample)\n",
    "        optimizer = SparseAdam(model_PP4_3.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_PP4_3(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Protein_P3_nids = torch.cuda.LongTensor(model_PP4_3.local_to_global_nid['protein'])\n",
    "        Protein_P3_emb = model_PP4_3.node_embed(Protein_P3_nids)\n",
    "        Protein_P3_emb = th.tensor(Protein_P3_emb).to(self.device)'''\n",
    "        \n",
    "        self.protein_feat_meta = torch.mean(torch.stack((Protein_Di_emb,\n",
    "                                                         Protein_DiDP_emb, Protein_DDiP_emb,\n",
    "                                                         Protein_P2_emb),\n",
    "                                                        dim = 1), dim = 1)\n",
    "        \n",
    "        '''metapath : Len(2) - Disease'''\n",
    "        #metapath for Disease\n",
    "        #for meta-path Di-D-Di\n",
    "        '''model_DiDDi = MetaPath2Vec(g, ['disease_drug association', 'drug_disease association'], window_size=1,emb_dim=1000)\n",
    "        model_DiDDi.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('disease')), batch_size=16, shuffle=True, collate_fn=model_DiDDi.sample)\n",
    "        optimizer = SparseAdam(model_DiDDi.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DiDDi(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Disease_D_nids = torch.cuda.LongTensor(model_DiDDi.local_to_global_nid['disease'])\n",
    "        Disease_D_emb = model_DiDDi.node_embed(Disease_D_nids)\n",
    "        Disease_D_emb = th.tensor(Disease_D_emb).to(self.device)'''\n",
    "\n",
    "        #for meta-path Di-P-Di\n",
    "        model_DiPDi = MetaPath2Vec(g, ['disease_protein association', 'protein_disease association'], window_size=1,emb_dim=1000)\n",
    "        model_DiPDi.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('disease')), batch_size=16, shuffle=True, collate_fn=model_DiPDi.sample)\n",
    "        optimizer = SparseAdam(model_DiPDi.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DiPDi(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Disease_P_nids = torch.cuda.LongTensor(model_DiPDi.local_to_global_nid['disease'])\n",
    "        Disease_P_emb = model_DiPDi.node_embed(Disease_P_nids)\n",
    "        Disease_P_emb = th.tensor(Disease_P_emb).to(self.device)\n",
    "        \n",
    "        '''metapath : Len(3) - Disease'''\n",
    "        '''#for meta-path Di-P-D-Di\n",
    "        model_DiPDDi = MetaPath2Vec(g, ['disease_protein association', 'protein_drug interaction', 'drug_disease association'],\n",
    "                                    window_size=1,emb_dim=1000)\n",
    "        model_DiPDDi.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('disease')), batch_size=16, shuffle=True,\n",
    "                                collate_fn=model_DiPDDi.sample)\n",
    "        optimizer = SparseAdam(model_DiPDDi.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DiPDDi(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Disease_PDDi_nids = torch.cuda.LongTensor(model_DiPDDi.local_to_global_nid['disease'])\n",
    "        Disease_PDDi_emb = model_DiPDDi.node_embed(Disease_PDDi_nids)\n",
    "        Disease_PDDi_emb = th.tensor(Disease_PDDi_emb).to(self.device)'''\n",
    "        \n",
    "        #for meta-path Di-D-P-Di\n",
    "        model_DiDPDi = MetaPath2Vec(g, ['disease_drug association', 'drug_protein interaction', 'protein_disease association'],\n",
    "                                    window_size=1,emb_dim=1000)\n",
    "        model_DiDPDi.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('disease')), batch_size=16, shuffle=True,\n",
    "                                collate_fn=model_DiDPDi.sample)\n",
    "        optimizer = SparseAdam(model_DiDPDi.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DiDPDi(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Disease_DPDi_nids = torch.cuda.LongTensor(model_DiDPDi.local_to_global_nid['disease'])\n",
    "        Disease_DPDi_emb = model_DiDPDi.node_embed(Disease_DPDi_nids)\n",
    "        Disease_DPDi_emb = th.tensor(Disease_DPDi_emb).to(self.device)\n",
    "        \n",
    "        '''metapath : Len(4) - Disease'''\n",
    "        '''#for metapath Di-D-P-D-Di\n",
    "        model_DiDi4_1 = MetaPath2Vec(g, ['disease_drug association','drug_protein interaction',\n",
    "                                         'protein_drug interaction','drug_disease association'],\n",
    "                                   window_size=1,emb_dim=1000)\n",
    "        model_DiDi4_1.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('disease')), batch_size=16,\n",
    "                                shuffle=True, collate_fn=model_DiDi4_1.sample)\n",
    "        optimizer = SparseAdam(model_DiDi4_1.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DiDi4_1(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Disease_Di1_nids = torch.cuda.LongTensor(model_DiDi4_1.local_to_global_nid['disease'])\n",
    "        Disease_Di1_emb = model_DiDi4_1.node_embed(Disease_Di1_nids)\n",
    "        Disease_Di1_emb = th.tensor(Disease_Di1_emb).to(self.device)'''\n",
    "        \n",
    "        #for metapath Di-P-D-P-Di\n",
    "        model_DiDi4_2 = MetaPath2Vec(g, ['disease_protein association', 'protein_drug interaction',\n",
    "                                         'drug_protein interaction','protein_disease association'],\n",
    "                                   window_size=1,emb_dim=1000)\n",
    "        model_DiDi4_2.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('disease')), batch_size=16,\n",
    "                                shuffle=True, collate_fn=model_DiDi4_2.sample)\n",
    "        optimizer = SparseAdam(model_DiDi4_2.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DiDi4_2(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Disease_Di2_nids = torch.cuda.LongTensor(model_DiDi4_2.local_to_global_nid['disease'])\n",
    "        Disease_Di2_emb = model_DiDi4_2.node_embed(Disease_Di2_nids)\n",
    "        Disease_Di2_emb = th.tensor(Disease_Di2_emb).to(self.device)\n",
    "        \n",
    "        #for metapath Di-D-Si-D-Di\n",
    "        model_DiDi4_3 = MetaPath2Vec(g, ['disease_drug association', 'drug_sideeffect association',\n",
    "                                         'sideeffect_drug association','drug_disease association'],\n",
    "                                   window_size=1,emb_dim=1000)\n",
    "        model_DiDi4_3.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('disease')), batch_size=16,\n",
    "                                shuffle=True, collate_fn=model_DiDi4_3.sample)\n",
    "        optimizer = SparseAdam(model_DiDi4_3.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_DiDi4_3(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Disease_Di3_nids = torch.cuda.LongTensor(model_DiDi4_3.local_to_global_nid['disease'])\n",
    "        Disease_Di3_emb = model_DiDi4_3.node_embed(Disease_Di3_nids)\n",
    "        Disease_Di3_emb = th.tensor(Disease_Di3_emb).to(self.device)\n",
    "        \n",
    "        self.disease_feat_meta = torch.mean(torch.stack((Disease_P_emb,\n",
    "                                                         Disease_DPDi_emb,\n",
    "                                                         Disease_Di2_emb, Disease_Di3_emb),\n",
    "                                                        dim = 1), dim = 1)\n",
    "\n",
    "        '''metapath : Len(2) - sideeffect'''\n",
    "        \n",
    "        model_SiDSi = MetaPath2Vec(g, ['sideeffect_drug association', 'drug_sideeffect association'],\n",
    "                                   window_size=1,emb_dim=1000)\n",
    "        model_SiDSi.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('sideeffect')), batch_size=16, shuffle=True, collate_fn=model_SiDSi.sample)\n",
    "        optimizer = SparseAdam(model_SiDSi.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_SiDSi(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Sideeffect_D_nids = torch.cuda.LongTensor(model_SiDSi.local_to_global_nid['sideeffect'])\n",
    "        Sideeffect_D_emb = model_SiDSi.node_embed(Sideeffect_D_nids)\n",
    "        Sideeffect_D_emb = th.tensor(Sideeffect_D_emb).to(self.device)\n",
    "        \n",
    "        '''metapath : Len(4) - sideeffect'''\n",
    "        #for metapath Si-D-P-D-Si \n",
    "        \n",
    "        model_SiSi4_1 = MetaPath2Vec(g, ['sideeffect_drug association', 'drug_protein interaction',\n",
    "                                         'protein_drug interaction','drug_sideeffect association'],\n",
    "                                     window_size=1,emb_dim=1000)\n",
    "        model_SiSi4_1.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('sideeffect')), batch_size=16,\n",
    "                                shuffle=True, collate_fn=model_SiSi4_1.sample)\n",
    "        optimizer = SparseAdam(model_SiSi4_1.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_SiSi4_1(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Sideeffect_Si1_nids = torch.cuda.LongTensor(model_SiSi4_1.local_to_global_nid['sideeffect'])\n",
    "        Sideeffect_Si1_emb = model_SiSi4_1.node_embed(Sideeffect_Si1_nids)\n",
    "        Sideeffect_Si1_emb = th.tensor(Sideeffect_Si1_emb).to(self.device)\n",
    "        \n",
    "        #for metapath Si-D-Di-D-Si\n",
    "        model_SiSi4_2 = MetaPath2Vec(g, ['sideeffect_drug association', 'drug_disease association',\n",
    "                                         'disease_drug association','drug_sideeffect association'],\n",
    "                                     window_size=1,emb_dim=1000)\n",
    "        model_SiSi4_2.to(self.device)\n",
    "        dataloader = DataLoader(torch.arange(g.num_nodes('sideeffect')), batch_size=16,\n",
    "                                shuffle=True, collate_fn=model_SiSi4_2.sample)\n",
    "        optimizer = SparseAdam(model_SiSi4_2.parameters(), lr=0.025)\n",
    "\n",
    "        for (pos_u, pos_v, neg_v) in dataloader:\n",
    "            loss = model_SiSi4_2(pos_u, pos_v, neg_v)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        Sideeffect_Si2_nids = torch.cuda.LongTensor(model_SiSi4_2.local_to_global_nid['sideeffect'])\n",
    "        Sideeffect_Si2_emb = model_SiSi4_2.node_embed(Sideeffect_Si2_nids)\n",
    "        Sideeffect_Si2_emb = th.tensor(Sideeffect_Si2_emb).to(self.device)\n",
    "        \n",
    "        self.sideeffect_feat_meta = torch.mean(torch.stack((Sideeffect_D_emb,\n",
    "                                                            Sideeffect_Si1_emb, Sideeffect_Si2_emb),\n",
    "                                                           dim = 1), dim = 1)\n",
    "\n",
    "        \n",
    "        self.node_feat_meta = th.cat((self.disease_feat_meta, self.drug_feat_meta, self.protein_feat_meta,\n",
    "                                      self.sideeffect_feat_meta), dim=0)\n",
    "\n",
    "        \n",
    "        self.drug_feat = nn.Parameter(th.FloatTensor(self.num_drug, self.dim_embedding))\n",
    "        nn.init.normal_(self.drug_feat, mean=0, std=0.1)\n",
    "        self.protein_feat = nn.Parameter(th.FloatTensor(self.num_protein, self.dim_embedding))\n",
    "        nn.init.normal_(self.protein_feat, mean=0, std=0.1)\n",
    "        self.disease_feat = nn.Parameter(th.FloatTensor(self.num_disease, self.dim_embedding))\n",
    "        nn.init.normal_(self.disease_feat, mean=0, std=0.1)\n",
    "        self.sideeffect_feat = nn.Parameter(th.FloatTensor(self.num_sideeffect, self.dim_embedding))\n",
    "        nn.init.normal_(self.sideeffect_feat, mean=0, std=0.1)\n",
    "\n",
    "        # 邻居信息的权重矩阵，对应论文公式（1）中的Wr、br\n",
    "        self.fc_DDI = nn.Linear(self.dim_embedding, self.dim_embedding).float()\n",
    "        self.fc_D_ch = nn.Linear(self.dim_embedding, self.dim_embedding).float()\n",
    "        self.fc_D_Di = nn.Linear(self.dim_embedding, self.dim_embedding).float()\n",
    "        self.fc_D_Side = nn.Linear(self.dim_embedding, self.dim_embedding).float()\n",
    "        self.fc_D_P = nn.Linear(self.dim_embedding, self.dim_embedding).float()\n",
    "        self.fc_PPI = nn.Linear(self.dim_embedding, self.dim_embedding).float()\n",
    "        self.fc_P_seq = nn.Linear(self.dim_embedding, self.dim_embedding).float()\n",
    "        self.fc_P_Di = nn.Linear(self.dim_embedding, self.dim_embedding).float()\n",
    "        self.fc_P_D = nn.Linear(self.dim_embedding, self.dim_embedding).float()\n",
    "        self.fc_Di_D = nn.Linear(self.dim_embedding, self.dim_embedding).float()\n",
    "        self.fc_Di_P = nn.Linear(self.dim_embedding, self.dim_embedding).float()\n",
    "        self.fc_Side_D = nn.Linear(self.dim_embedding, self.dim_embedding).float()\n",
    "\n",
    "        self.propagation = Propagation(args.k, args.alpha, args.edge_drop)\n",
    "\n",
    "        # Linear transformation for reconstruction\n",
    "        tmp = th.randn(self.dim_embedding).float()\n",
    "        self.re_DDI = nn.Parameter(th.diag(th.nn.init.normal_(tmp, mean=0, std=0.1)))\n",
    "        self.re_D_ch = nn.Parameter(th.diag(th.nn.init.normal_(tmp, mean=0, std=0.1)))\n",
    "        self.re_D_Di = nn.Parameter(th.diag(th.nn.init.normal_(tmp, mean=0, std=0.1)))\n",
    "        self.re_D_Side = nn.Parameter(th.diag(th.nn.init.normal_(tmp, mean=0, std=0.1)))\n",
    "        self.re_D_P = nn.Parameter(th.diag(th.nn.init.normal_(tmp, mean=0, std=0.1)))\n",
    "        self.re_PPI = nn.Parameter(th.diag(th.nn.init.normal_(tmp, mean=0, std=0.1)))\n",
    "        self.re_P_seq = nn.Parameter(th.diag(th.nn.init.normal_(tmp, mean=0, std=0.1)))\n",
    "        self.re_P_Di = nn.Parameter(th.diag(th.nn.init.normal_(tmp, mean=0, std=0.1)))\n",
    "        #self.re_P_D = nn.Parameter(th.diag(th.nn.init.normal_(tmp, mean=0, std=0.1)))\n",
    "        #self.re_Di_P = nn.Parameter(th.diag(th.nn.init.normal_(tmp, mean=0, std=0.1)))\n",
    "        #self.re_Di_D = nn.Parameter(th.diag(th.nn.init.normal_(tmp, mean=0, std=0.1)))\n",
    "        #self.re_Side_D = nn.Parameter(th.diag(th.nn.init.normal_(tmp, mean=0, std=0.1)))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for m in GRDTI.modules(self):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, mean=0, std=0.1)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, drug_drug, drug_chemical, drug_disease, drug_sideeffect, protein_protein,\n",
    "                protein_sequence, protein_disease, drug_protein, drug_protein_mask):\n",
    "\n",
    "        disease_feat = th.mean(th.stack((th.mm(row_normalize(drug_disease.T).float(),\n",
    "                                               F.relu(self.fc_Di_D(self.drug_feat))),\n",
    "                                         th.mm(row_normalize(protein_disease.T).float(),\n",
    "                                               F.relu(self.fc_Di_P(self.protein_feat))),\n",
    "                                         self.disease_feat), dim=1), dim=1)\n",
    "\n",
    "        drug_feat = th.mean(th.stack((th.mm(row_normalize(drug_drug).float(),\n",
    "                                            F.relu(self.fc_DDI(self.drug_feat))),\n",
    "                                      th.mm(row_normalize(drug_chemical).float(),\n",
    "                                            F.relu(self.fc_D_ch(self.drug_feat))),\n",
    "                                      th.mm(row_normalize(drug_disease).float(),\n",
    "                                            F.relu(self.fc_D_Di(self.disease_feat))),\n",
    "                                      th.mm(row_normalize(drug_sideeffect).float(),\n",
    "                                            F.relu(self.fc_D_Side(self.sideeffect_feat))),\n",
    "                                      th.mm(row_normalize(drug_protein).float(),\n",
    "                                            F.relu(self.fc_D_P(self.protein_feat))),\n",
    "                                      self.drug_feat), dim=1), dim=1)\n",
    "\n",
    "        protein_feat = th.mean(th.stack((th.mm(row_normalize(protein_protein).float(),\n",
    "                                               F.relu(self.fc_PPI(self.protein_feat))),\n",
    "                                         th.mm(row_normalize(protein_sequence).float(),\n",
    "                                               F.relu(self.fc_P_seq(self.protein_feat))),\n",
    "                                         th.mm(row_normalize(protein_disease).float(),\n",
    "                                               F.relu(self.fc_P_Di(self.disease_feat))),\n",
    "                                         th.mm(row_normalize(drug_protein.T).float(),\n",
    "                                               F.relu(self.fc_P_D(self.drug_feat))),\n",
    "                                         self.protein_feat), dim=1), dim=1)\n",
    "\n",
    "        sideeffect_feat = th.mean(th.stack((th.mm(row_normalize(drug_sideeffect.T).float(),\n",
    "                                                  F.relu(self.fc_Side_D(self.drug_feat))),\n",
    "                                            self.sideeffect_feat), dim=1), dim=1)\n",
    "\n",
    "        node_feat = th.cat((disease_feat, drug_feat, protein_feat, sideeffect_feat), dim=0)\n",
    "        \n",
    "        '''Concatenate node_feat from GCN with node_feat_meta from Meta-Path'''\n",
    "        beta = 0.5\n",
    "        node_feat = beta*node_feat + (1-beta)*self.node_feat_meta\n",
    "        #node_feat = torch.mean(torch.stack((node_feat,self.node_feat_meta), dim = 1), dim = 1)\n",
    "        \n",
    "        node_feat = self.propagation(dgl.to_homogeneous(self.g), node_feat)\n",
    "\n",
    "        disease_embedding = node_feat[:self.num_disease].to(self.device)\n",
    "        drug_embedding = node_feat[self.num_disease:self.num_disease + self.num_drug].to(self.device)\n",
    "        protein_embedding = node_feat[self.num_disease + self.num_drug:self.num_disease + self.num_drug +\n",
    "                                                                       self.num_protein].to(self.device)\n",
    "        sideeffect_embedding = node_feat[-self.num_sideeffect:].to(self.device)\n",
    "\n",
    "        disease_vector = l2_norm(disease_embedding)\n",
    "        drug_vector = l2_norm(drug_embedding)\n",
    "        protein_vector = l2_norm(protein_embedding)\n",
    "        sideeffect_vector = l2_norm(sideeffect_embedding)\n",
    "\n",
    "        drug_drug_reconstruct = th.mm(th.mm(drug_vector, self.re_DDI), drug_vector.t())\n",
    "        drug_drug_reconstruct_loss = th.sum(\n",
    "            (drug_drug_reconstruct - drug_drug.float()) ** 2)\n",
    "\n",
    "        drug_chemical_reconstruct = th.mm(th.mm(drug_vector, self.re_D_ch), drug_vector.t())\n",
    "        drug_chemical_reconstruct_loss = th.sum(\n",
    "            (drug_chemical_reconstruct - drug_chemical.float()) ** 2)\n",
    "\n",
    "        drug_disease_reconstruct = th.mm(th.mm(drug_vector, self.re_D_Di), disease_vector.t())\n",
    "        drug_disease_reconstruct_loss = th.sum(\n",
    "            (drug_disease_reconstruct - drug_disease.float()) ** 2)\n",
    "\n",
    "        drug_sideeffect_reconstruct = th.mm(th.mm(drug_vector, self.re_D_Side), sideeffect_vector.t())\n",
    "        drug_sideeffect_reconstruct_loss = th.sum(\n",
    "            (drug_sideeffect_reconstruct - drug_sideeffect.float()) ** 2)\n",
    "\n",
    "        protein_protein_reconstruct = th.mm(th.mm(protein_vector, self.re_PPI), protein_vector.t())\n",
    "        protein_protein_reconstruct_loss = th.sum(\n",
    "            (protein_protein_reconstruct - protein_protein.float()) ** 2)\n",
    "\n",
    "        protein_sequence_reconstruct = th.mm(th.mm(protein_vector, self.re_P_seq), protein_vector.t())\n",
    "        protein_sequence_reconstruct_loss = th.sum(\n",
    "            (protein_sequence_reconstruct - protein_sequence.float()) ** 2)\n",
    "\n",
    "        protein_disease_reconstruct = th.mm(th.mm(protein_vector, self.re_P_Di), disease_vector.t())\n",
    "        protein_disease_reconstruct_loss = th.sum(\n",
    "            (protein_disease_reconstruct - protein_disease.float()) ** 2)\n",
    "\n",
    "        drug_protein_reconstruct = th.mm(th.mm(drug_vector, self.re_D_P), protein_vector.t())\n",
    "        tmp = th.mul(drug_protein_mask.float(), (drug_protein_reconstruct - drug_protein.float()))\n",
    "        DTI_potential = drug_protein_reconstruct - drug_protein.float()\n",
    "        drug_protein_reconstruct_loss = th.sum(tmp ** 2)\n",
    "\n",
    "        other_loss = drug_drug_reconstruct_loss + drug_chemical_reconstruct_loss + drug_disease_reconstruct_loss + \\\n",
    "                     drug_sideeffect_reconstruct_loss + protein_protein_reconstruct_loss + \\\n",
    "                     protein_sequence_reconstruct_loss + protein_disease_reconstruct_loss\n",
    "\n",
    "        L2_loss = 0.\n",
    "        for name, param in GRDTI.named_parameters(self):\n",
    "            if 'bias' not in name:\n",
    "                L2_loss = L2_loss + th.sum(param.pow(2))\n",
    "        L2_loss = L2_loss * 0.5\n",
    "\n",
    "        tloss = drug_protein_reconstruct_loss + 1.0 * other_loss + self.reg_lambda * L2_loss\n",
    "\n",
    "        return tloss, drug_protein_reconstruct_loss, L2_loss, drug_protein_reconstruct, DTI_potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea7ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epochs=4000, rounds=1, device='cuda', dim_embedding=1000, k=3, lr=0.001, weight_decay=0, reg_lambda=1, patience=6, alpha=0.9, edge_drop=0.5, f='C:\\\\Users\\\\USER\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-1896fcd4-420a-4ca8-9110-bc6f9ce89b52.json')\n",
      "Load data finished.\n",
      "----------------------------------------\n",
      "--------------------------------------------------------------\n",
      "round  1  of  1 : KFold  1  of 10\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:00<00:00, 879.25it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_Di_emb = th.tensor(Drug_Di_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:00<00:00, 724.12it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_PDi_emb = th.tensor(Drug_PDi_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:01<00:00, 646.46it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_D2_emb = th.tensor(Drug_D2_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:01<00:00, 685.78it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:190: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_D4_emb = th.tensor(Drug_D4_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:01<00:00, 909.53it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:230: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_Di_emb = th.tensor(Protein_Di_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:01<00:00, 787.82it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_DiDP_emb = th.tensor(Protein_DiDP_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:01<00:00, 829.72it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:265: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_DDiP_emb = th.tensor(Protein_DDiP_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:02<00:00, 717.68it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:303: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_P2_emb = th.tensor(Protein_P2_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:05<00:00, 952.28it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:358: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_P_emb = th.tensor(Disease_P_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:06<00:00, 842.53it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:393: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_DPDi_emb = th.tensor(Disease_DPDi_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:07<00:00, 767.81it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:430: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_Di2_emb = th.tensor(Disease_Di2_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:07<00:00, 759.43it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:448: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_Di3_emb = th.tensor(Disease_Di3_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:04<00:00, 944.02it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:470: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_D_emb = th.tensor(Sideeffect_D_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:05<00:00, 761.25it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:490: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_Si1_emb = th.tensor(Sideeffect_Si1_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:05<00:00, 761.45it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:508: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_Si2_emb = th.tensor(Sideeffect_Si2_emb).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : Total_loss & DTIloss & L2_loss: 2082887.1 , 1650.5286865234375 , 120155.3203125\n",
      "Valid auc & aupr: 0.4964060859783488 0.001762270828380805 ;   Test auc & aupr: 0.5178592754900125 0.001848806593122057\n",
      "step 25 : Total_loss & DTIloss & L2_loss: 1147700.5 , 4630.6083984375 , 117214.0\n",
      "Valid auc & aupr: 0.4157440742383132 0.0013437488647997287 ;   Test auc & aupr: 0.5178592754900125 0.001848806593122057\n",
      "step 50 : Total_loss & DTIloss & L2_loss: 852685.8 , 4046.52880859375 , 108754.578125\n",
      "Valid auc & aupr: 0.6307563378905516 0.002498760226950481 ;   Test auc & aupr: 0.623971841501797 0.0026166794252606594\n",
      "step 75 : Total_loss & DTIloss & L2_loss: 791229.94 , 1819.2357177734375 , 97402.7890625\n",
      "Valid auc & aupr: 0.8399960415586237 0.02593402102318419 ;   Test auc & aupr: 0.8452322921907982 0.033791794778675215\n",
      "step 100 : Total_loss & DTIloss & L2_loss: 761383.6 , 1633.046875 , 88902.4609375\n",
      "Valid auc & aupr: 0.8629096548389441 0.036494416501659944 ;   Test auc & aupr: 0.8817687247607704 0.0387728151266791\n",
      "step 125 : Total_loss & DTIloss & L2_loss: 728188.56 , 1615.01953125 , 81197.765625\n",
      "Valid auc & aupr: 0.857362074624136 0.046109496233444 ;   Test auc & aupr: 0.8841776872786404 0.04377607491073775\n",
      "step 150 : Total_loss & DTIloss & L2_loss: 702777.5 , 1608.652587890625 , 74320.28125\n",
      "Valid auc & aupr: 0.854180289467289 0.05608600457296763 ;   Test auc & aupr: 0.8813269450562494 0.05627724139238722\n",
      "step 175 : Total_loss & DTIloss & L2_loss: 681480.44 , 1602.7276611328125 , 67593.0625\n",
      "Valid auc & aupr: 0.8588607705932902 0.08587305094340766 ;   Test auc & aupr: 0.8820098101181274 0.08111667824056354\n",
      "step 200 : Total_loss & DTIloss & L2_loss: 661489.3 , 1593.3326416015625 , 61474.18359375\n",
      "Valid auc & aupr: 0.8679321152357402 0.1263827225090049 ;   Test auc & aupr: 0.8874672029118766 0.10674497302349908\n",
      "step 225 : Total_loss & DTIloss & L2_loss: 652590.2 , 1584.250732421875 , 56802.19140625\n",
      "Valid auc & aupr: 0.87198250268698 0.14107677799592513 ;   Test auc & aupr: 0.8906395700689718 0.12370543063621947\n",
      "step 250 : Total_loss & DTIloss & L2_loss: 629242.44 , 1593.4224853515625 , 52274.73046875\n",
      "Valid auc & aupr: 0.8684301773532056 0.17452281775397485 ;   Test auc & aupr: 0.887619457139974 0.14083826197868632\n",
      "step 275 : Total_loss & DTIloss & L2_loss: 605719.44 , 1542.419677734375 , 48721.71484375\n",
      "Valid auc & aupr: 0.8802040351048611 0.21961822226377914 ;   Test auc & aupr: 0.9006640078423535 0.167485038203533\n",
      "step 300 : Total_loss & DTIloss & L2_loss: 580958.0 , 1508.89111328125 , 45575.06640625\n",
      "Valid auc & aupr: 0.8862186111883591 0.2813892452381282 ;   Test auc & aupr: 0.8998461020081023 0.20408069042485238\n",
      "step 325 : Total_loss & DTIloss & L2_loss: 555607.0 , 1467.733642578125 , 42685.98046875\n",
      "Valid auc & aupr: 0.8969765524982024 0.3579764301347417 ;   Test auc & aupr: 0.9032734222643491 0.23276438503072472\n",
      "step 350 : Total_loss & DTIloss & L2_loss: 528650.6 , 1412.939208984375 , 40105.59375\n",
      "Valid auc & aupr: 0.9178841379241232 0.4105940195041549 ;   Test auc & aupr: 0.9133742780834256 0.2694650929582492\n",
      "step 375 : Total_loss & DTIloss & L2_loss: 494892.3 , 1340.9727783203125 , 38082.421875\n",
      "Valid auc & aupr: 0.9210055443232693 0.43857445117389376 ;   Test auc & aupr: 0.9201516277552864 0.30223106703736957\n",
      "step 400 : Total_loss & DTIloss & L2_loss: 453528.0 , 1265.8671875 , 36782.5703125\n",
      "Valid auc & aupr: 0.9183606540948572 0.4550324787730878 ;   Test auc & aupr: 0.9197078600178505 0.3333456910407099\n",
      "step 425 : Total_loss & DTIloss & L2_loss: 410535.5 , 1192.1297607421875 , 35696.9296875\n",
      "Valid auc & aupr: 0.9295184480905532 0.4746403387182132 ;   Test auc & aupr: 0.9263339252337672 0.36436393207566237\n",
      "step 450 : Total_loss & DTIloss & L2_loss: 372580.78 , 1128.7288818359375 , 34790.8125\n",
      "Valid auc & aupr: 0.9306736615833263 0.49885961486789 ;   Test auc & aupr: 0.9291819520941285 0.3963997374022395\n",
      "step 475 : Total_loss & DTIloss & L2_loss: 331434.44 , 1051.674560546875 , 34163.12109375\n",
      "Valid auc & aupr: 0.9240678246360363 0.5196111376879143 ;   Test auc & aupr: 0.9209717155768828 0.4271884303730784\n",
      "step 500 : Total_loss & DTIloss & L2_loss: 297816.0 , 974.4246826171875 , 33655.28125\n",
      "Valid auc & aupr: 0.9203022946433269 0.5429551726249949 ;   Test auc & aupr: 0.9246824032563785 0.46122783263666783\n",
      "step 525 : Total_loss & DTIloss & L2_loss: 275287.8 , 932.4441528320312 , 33348.69140625\n",
      "Valid auc & aupr: 0.9160572420687115 0.5510183480610442 ;   Test auc & aupr: 0.9254910962551715 0.478149546888893\n",
      "step 550 : Total_loss & DTIloss & L2_loss: 259713.72 , 898.6206665039062 , 32954.93359375\n",
      "Valid auc & aupr: 0.9186207086612201 0.5608488079701347 ;   Test auc & aupr: 0.9268628874548868 0.4895912786999153\n",
      "step 575 : Total_loss & DTIloss & L2_loss: 245166.86 , 865.5313720703125 , 32676.265625\n",
      "Valid auc & aupr: 0.9198508318990248 0.5695657032187262 ;   Test auc & aupr: 0.9284666966423238 0.49968859219650674\n",
      "step 600 : Total_loss & DTIloss & L2_loss: 231305.3 , 835.344970703125 , 32508.677734375\n",
      "Valid auc & aupr: 0.9231170471029471 0.5782434391165301 ;   Test auc & aupr: 0.9298459065990132 0.5093768675013857\n",
      "step 625 : Total_loss & DTIloss & L2_loss: 220019.69 , 805.4501953125 , 32415.439453125\n",
      "Valid auc & aupr: 0.9275114180990963 0.5830885871606919 ;   Test auc & aupr: 0.9320442830937535 0.5168951322629407\n",
      "step 650 : Total_loss & DTIloss & L2_loss: 207740.89 , 767.3469848632812 , 32575.328125\n",
      "Valid auc & aupr: 0.9300077415087674 0.5899107280164886 ;   Test auc & aupr: 0.933925883514557 0.5253495626052294\n",
      "step 675 : Total_loss & DTIloss & L2_loss: 193955.34 , 733.8632202148438 , 32684.048828125\n",
      "Valid auc & aupr: 0.9345168571979747 0.5944637237109757 ;   Test auc & aupr: 0.934242902031532 0.5336563539601084\n",
      "step 700 : Total_loss & DTIloss & L2_loss: 184339.12 , 704.9024658203125 , 32644.501953125\n",
      "Valid auc & aupr: 0.9364627568289376 0.5972739429878526 ;   Test auc & aupr: 0.9362063027334823 0.5380060378811081\n",
      "step 725 : Total_loss & DTIloss & L2_loss: 177794.31 , 682.1647338867188 , 32735.228515625\n",
      "Valid auc & aupr: 0.9362746055964343 0.6013071251150983 ;   Test auc & aupr: 0.9379125683489351 0.5425412291258882\n",
      "step 750 : Total_loss & DTIloss & L2_loss: 173036.6 , 668.7092895507812 , 32790.37890625\n",
      "Valid auc & aupr: 0.9368606052406756 0.604614564820179 ;   Test auc & aupr: 0.9395591444883411 0.5467775900820321\n",
      "step 775 : Total_loss & DTIloss & L2_loss: 169115.06 , 656.57177734375 , 32821.3671875\n",
      "Valid auc & aupr: 0.9365201291754667 0.6066830014554456 ;   Test auc & aupr: 0.9397203206202508 0.5497659872426898\n",
      "step 800 : Total_loss & DTIloss & L2_loss: 164310.72 , 645.006591796875 , 32886.27734375\n",
      "Valid auc & aupr: 0.9362573187321963 0.6089663799398369 ;   Test auc & aupr: 0.9391100430040613 0.5518212420589947\n",
      "step 825 : Total_loss & DTIloss & L2_loss: 160086.23 , 633.9223022460938 , 32966.68359375\n",
      "Valid auc & aupr: 0.9371013686686859 0.6110870840450046 ;   Test auc & aupr: 0.9398081819773566 0.5539436257879974\n",
      "step 850 : Total_loss & DTIloss & L2_loss: 157526.31 , 622.67041015625 , 33056.609375\n",
      "Valid auc & aupr: 0.9370795721876901 0.6099228964984319 ;   Test auc & aupr: 0.9398081819773566 0.5539436257879974\n",
      "step 875 : Total_loss & DTIloss & L2_loss: 153769.27 , 611.9345092773438 , 33162.67578125\n",
      "Valid auc & aupr: 0.9380075511027266 0.6119486675064315 ;   Test auc & aupr: 0.9406154688065271 0.5562638708169353\n",
      "step 900 : Total_loss & DTIloss & L2_loss: 151390.52 , 603.55029296875 , 33272.23046875\n",
      "Valid auc & aupr: 0.9379083395340563 0.6120857435439748 ;   Test auc & aupr: 0.9403923727226173 0.5596047715911531\n",
      "step 925 : Total_loss & DTIloss & L2_loss: 148871.38 , 593.2371826171875 , 33424.484375\n",
      "Valid auc & aupr: 0.9358499500184143 0.6128219813869095 ;   Test auc & aupr: 0.9401780045880891 0.5609452722391006\n",
      "step 950 : Total_loss & DTIloss & L2_loss: 144114.44 , 580.797607421875 , 33672.0703125\n",
      "Valid auc & aupr: 0.9370497586102363 0.6138903051396114 ;   Test auc & aupr: 0.9415122656054644 0.5641103673212038\n",
      "step 975 : Total_loss & DTIloss & L2_loss: 141346.3 , 570.5086669921875 , 33865.8828125\n",
      "Valid auc & aupr: 0.9354796603757514 0.6149575940161385 ;   Test auc & aupr: 0.9415888291169822 0.56674941697409\n",
      "step 1000 : Total_loss & DTIloss & L2_loss: 138577.47 , 563.5226440429688 , 33964.46875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid auc & aupr: 0.9340195466833021 0.614775083949413 ;   Test auc & aupr: 0.9415888291169822 0.56674941697409\n",
      "step 1025 : Total_loss & DTIloss & L2_loss: 137848.33 , 556.9945068359375 , 34036.41015625\n",
      "Valid auc & aupr: 0.9349397590361446 0.6147382206675905 ;   Test auc & aupr: 0.9415888291169822 0.56674941697409\n",
      "step 1050 : Total_loss & DTIloss & L2_loss: 135152.48 , 551.252197265625 , 34129.62109375\n",
      "Valid auc & aupr: 0.9315154567114372 0.6169400123578452 ;   Test auc & aupr: 0.9448784902160415 0.5688727920971187\n",
      "step 1075 : Total_loss & DTIloss & L2_loss: 132549.9 , 545.6062622070312 , 34229.7734375\n",
      "Valid auc & aupr: 0.9315342467812611 0.6173128847783772 ;   Test auc & aupr: 0.9450767601328218 0.5698666284912717\n",
      "step 1100 : Total_loss & DTIloss & L2_loss: 130734.14 , 539.7762451171875 , 34327.21484375\n",
      "Valid auc & aupr: 0.9304406647175101 0.6163585978578102 ;   Test auc & aupr: 0.9450767601328218 0.5698666284912717\n",
      "step 1125 : Total_loss & DTIloss & L2_loss: 129861.59 , 533.949951171875 , 34429.65625\n",
      "Valid auc & aupr: 0.9306177924423832 0.6188001656311156 ;   Test auc & aupr: 0.947099122981704 0.5698927311604599\n",
      "step 1150 : Total_loss & DTIloss & L2_loss: 128498.65 , 528.8248291015625 , 34542.4765625\n",
      "Valid auc & aupr: 0.9302119269341871 0.6191551305609024 ;   Test auc & aupr: 0.947444167913915 0.5715495754357914\n",
      "step 1175 : Total_loss & DTIloss & L2_loss: 126368.055 , 522.2694091796875 , 34754.96875\n",
      "Valid auc & aupr: 0.9278765717893408 0.6186531049899126 ;   Test auc & aupr: 0.947444167913915 0.5715495754357914\n",
      "step 1200 : Total_loss & DTIloss & L2_loss: 123960.016 , 514.9906005859375 , 34972.85546875\n",
      "Valid auc & aupr: 0.9274340030114219 0.6189765747909068 ;   Test auc & aupr: 0.947444167913915 0.5715495754357914\n",
      "step 1225 : Total_loss & DTIloss & L2_loss: 122961.71 , 510.05035400390625 , 35139.046875\n",
      "Valid auc & aupr: 0.9254705659819565 0.6183869294889641 ;   Test auc & aupr: 0.947444167913915 0.5715495754357914\n",
      "step 1250 : Total_loss & DTIloss & L2_loss: 120696.516 , 505.8664245605469 , 35222.8984375\n",
      "Valid auc & aupr: 0.9251694237962456 0.6188766704614826 ;   Test auc & aupr: 0.947444167913915 0.5715495754357914\n",
      "step 1275 : Total_loss & DTIloss & L2_loss: 120459.12 , 501.37579345703125 , 35303.015625\n",
      "Valid auc & aupr: 0.9251318436565977 0.6190241232122462 ;   Test auc & aupr: 0.947444167913915 0.5715495754357914\n",
      "step 1300 : Total_loss & DTIloss & L2_loss: 119077.51 , 497.18218994140625 , 35390.484375\n",
      "Valid auc & aupr: 0.9250331331564562 0.6189470824315666 ;   Test auc & aupr: 0.947444167913915 0.5715495754357914\n",
      "step 1325 : Total_loss & DTIloss & L2_loss: 117378.875 , 494.10345458984375 , 35484.32421875\n",
      "Valid auc & aupr: 0.9256063555532172 0.619838499954186 ;   Test auc & aupr: 0.9478173847271845 0.5757263129316047\n",
      "step 1350 : Total_loss & DTIloss & L2_loss: 116325.03 , 489.9460144042969 , 35578.05859375\n",
      "Valid auc & aupr: 0.922532049595763 0.6188772966271183 ;   Test auc & aupr: 0.9478173847271845 0.5757263129316047\n",
      "step 1375 : Total_loss & DTIloss & L2_loss: 115423.03 , 485.77862548828125 , 35673.22265625\n",
      "Valid auc & aupr: 0.9221542439251704 0.6191771119850894 ;   Test auc & aupr: 0.9478173847271845 0.5757263129316047\n",
      "step 1400 : Total_loss & DTIloss & L2_loss: 114138.766 , 482.8845520019531 , 35771.9609375\n",
      "Valid auc & aupr: 0.9224619000017538 0.6197888974214767 ;   Test auc & aupr: 0.9478173847271845 0.5757263129316047\n",
      "step 1425 : Total_loss & DTIloss & L2_loss: 116343.41 , 480.67572021484375 , 35966.3515625\n",
      "Valid auc & aupr: 0.9205974240066942 0.6214105090009036 ;   Test auc & aupr: 0.9459376268734724 0.576981076632422\n",
      "step 1450 : Total_loss & DTIloss & L2_loss: 112801.87 , 474.988525390625 , 36171.1875\n",
      "Valid auc & aupr: 0.9196269795338561 0.6207705637784859 ;   Test auc & aupr: 0.9459376268734724 0.576981076632422\n",
      "step 1475 : Total_loss & DTIloss & L2_loss: 112902.336 , 469.499755859375 , 36335.796875\n",
      "Valid auc & aupr: 0.9197849413875089 0.6192068685172082 ;   Test auc & aupr: 0.9459376268734724 0.576981076632422\n",
      "step 1500 : Total_loss & DTIloss & L2_loss: 110998.875 , 466.4197082519531 , 36452.26953125\n",
      "Valid auc & aupr: 0.9191723851112498 0.6202074269537804 ;   Test auc & aupr: 0.9459376268734724 0.576981076632422\n",
      "step 1525 : Total_loss & DTIloss & L2_loss: 109286.91 , 464.0317077636719 , 36520.328125\n",
      "Valid auc & aupr: 0.9190441115679185 0.6207894514222047 ;   Test auc & aupr: 0.9459376268734724 0.576981076632422\n",
      "step 1550 : Total_loss & DTIloss & L2_loss: 108448.19 , 461.3273620605469 , 36589.80078125\n",
      "Valid auc & aupr: 0.9187980869203576 0.6211584688466987 ;   Test auc & aupr: 0.9459376268734724 0.576981076632422\n",
      "step 1575 : Total_loss & DTIloss & L2_loss: 108385.945 , 458.2958068847656 , 36655.6953125\n",
      "Valid auc & aupr: 0.9182752219107245 0.6214531687880087 ;   Test auc & aupr: 0.9445387305420672 0.5795806610191949\n",
      "step 1600 : Total_loss & DTIloss & L2_loss: 107329.88 , 455.4613952636719 , 36729.515625\n",
      "Valid auc & aupr: 0.9184691354313073 0.6203698626766135 ;   Test auc & aupr: 0.9445387305420672 0.5795806610191949\n",
      "step 1625 : Total_loss & DTIloss & L2_loss: 106260.945 , 453.2888488769531 , 36808.59375\n",
      "Valid auc & aupr: 0.9180722891566264 0.6208117932598434 ;   Test auc & aupr: 0.9445387305420672 0.5795806610191949\n",
      "step 1650 : Total_loss & DTIloss & L2_loss: 105394.78 , 451.2926940917969 , 36884.81640625\n",
      "Valid auc & aupr: 0.9176035395480862 0.6216378015481323 ;   Test auc & aupr: 0.9429354547293145 0.5811503847353194\n",
      "step 1675 : Total_loss & DTIloss & L2_loss: 104949.2 , 448.038330078125 , 36954.27734375\n",
      "Valid auc & aupr: 0.9177641320115145 0.620977845725066 ;   Test auc & aupr: 0.9429354547293145 0.5811503847353194\n",
      "step 1700 : Total_loss & DTIloss & L2_loss: 104128.57 , 445.7432556152344 , 37026.59375\n",
      "Valid auc & aupr: 0.9174554737978738 0.6212782282703248 ;   Test auc & aupr: 0.9429354547293145 0.5811503847353194\n",
      "step 1725 : Total_loss & DTIloss & L2_loss: 103481.64 , 444.00067138671875 , 37102.609375\n",
      "Valid auc & aupr: 0.9184536023069195 0.6208819211241865 ;   Test auc & aupr: 0.9429354547293145 0.5811503847353194\n",
      "step 1750 : Total_loss & DTIloss & L2_loss: 104154.35 , 442.17999267578125 , 37177.1171875\n",
      "Valid auc & aupr: 0.9177874316980962 0.6230234402612458 ;   Test auc & aupr: 0.9412848540354716 0.5824893066812274\n",
      "step 1775 : Total_loss & DTIloss & L2_loss: 102550.08 , 439.524658203125 , 37272.3671875\n",
      "Valid auc & aupr: 0.9184127652218357 0.6219815644914144 ;   Test auc & aupr: 0.9412848540354716 0.5824893066812274\n",
      "step 1800 : Total_loss & DTIloss & L2_loss: 101685.375 , 436.6552734375 , 37368.796875\n",
      "Valid auc & aupr: 0.918303031214064 0.6217453700636091 ;   Test auc & aupr: 0.9412848540354716 0.5824893066812274\n",
      "step 1825 : Total_loss & DTIloss & L2_loss: 101265.625 , 434.4197082519531 , 37482.32421875\n",
      "Valid auc & aupr: 0.9187585025065953 0.6212381864865849 ;   Test auc & aupr: 0.9412848540354716 0.5824893066812274\n",
      "step 1850 : Total_loss & DTIloss & L2_loss: 99745.42 , 431.1026916503906 , 37624.90234375\n",
      "Valid auc & aupr: 0.9188076072224017 0.6233808962729309 ;   Test auc & aupr: 0.9396172823289426 0.5825356060845367\n",
      "step 1875 : Total_loss & DTIloss & L2_loss: 98658.31 , 428.4153747558594 , 37724.17578125\n",
      "Valid auc & aupr: 0.9187682733429038 0.6209245025902767 ;   Test auc & aupr: 0.9396172823289426 0.5825356060845367\n",
      "step 1900 : Total_loss & DTIloss & L2_loss: 97829.92 , 425.4345703125 , 37805.92578125\n",
      "Valid auc & aupr: 0.9195258889582032 0.6214400970819148 ;   Test auc & aupr: 0.9396172823289426 0.5825356060845367\n",
      "step 1925 : Total_loss & DTIloss & L2_loss: 97002.2 , 422.6054382324219 , 37891.11328125\n",
      "Valid auc & aupr: 0.9195329039176043 0.6218526033374553 ;   Test auc & aupr: 0.9396172823289426 0.5825356060845367\n",
      "step 1950 : Total_loss & DTIloss & L2_loss: 96257.81 , 419.8711242675781 , 37972.1640625\n",
      "Valid auc & aupr: 0.9197082779026274 0.6223759840274306 ;   Test auc & aupr: 0.9396172823289426 0.5825356060845367\n",
      "step 1975 : Total_loss & DTIloss & L2_loss: 95628.09 , 417.55108642578125 , 38057.09375\n",
      "Valid auc & aupr: 0.9200635354894312 0.6216088484553487 ;   Test auc & aupr: 0.9396172823289426 0.5825356060845367\n",
      "step 2000 : Total_loss & DTIloss & L2_loss: 94917.016 , 415.2708740234375 , 38137.046875\n",
      "Valid auc & aupr: 0.9200049104715808 0.6220908593186093 ;   Test auc & aupr: 0.9396172823289426 0.5825356060845367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2025 : Total_loss & DTIloss & L2_loss: 94268.69 , 412.9160461425781 , 38214.6328125\n",
      "Valid auc & aupr: 0.9214747950003384 0.6235221848300329 ;   Test auc & aupr: 0.9367946634990029 0.5870771214258285\n",
      "step 2050 : Total_loss & DTIloss & L2_loss: 93771.09 , 410.8794860839844 , 38287.5625\n",
      "Valid auc & aupr: 0.9212407959974647 0.6233612626778574 ;   Test auc & aupr: 0.9367946634990029 0.5870771214258285\n",
      "step 2075 : Total_loss & DTIloss & L2_loss: 93072.78 , 408.7580261230469 , 38362.2890625\n",
      "Valid auc & aupr: 0.9204937028212664 0.6225458207040785 ;   Test auc & aupr: 0.9367946634990029 0.5870771214258285\n",
      "step 2100 : Total_loss & DTIloss & L2_loss: 92646.71 , 407.0987854003906 , 38430.08203125\n",
      "Valid auc & aupr: 0.9209409064830751 0.6233285343383869 ;   Test auc & aupr: 0.9367946634990029 0.5870771214258285\n",
      "step 2125 : Total_loss & DTIloss & L2_loss: 92043.48 , 405.0478210449219 , 38498.83984375\n",
      "Valid auc & aupr: 0.9214286966957036 0.6239048627115289 ;   Test auc & aupr: 0.9348361116578201 0.5863753422077664\n",
      "step 2150 : Total_loss & DTIloss & L2_loss: 91593.016 , 403.30859375 , 38560.25390625\n",
      "Valid auc & aupr: 0.9216186016680572 0.6232773850363292 ;   Test auc & aupr: 0.9348361116578201 0.5863753422077664\n",
      "step 2175 : Total_loss & DTIloss & L2_loss: 91238.14 , 401.896728515625 , 38625.63671875\n",
      "Valid auc & aupr: 0.9222384234379816 0.6238122458808766 ;   Test auc & aupr: 0.9348361116578201 0.5863753422077664\n",
      "step 2200 : Total_loss & DTIloss & L2_loss: 90766.336 , 400.07708740234375 , 38689.30078125\n",
      "Valid auc & aupr: 0.9221131563058222 0.6252449713375514 ;   Test auc & aupr: 0.9320903715153479 0.5862337315400574\n",
      "step 2225 : Total_loss & DTIloss & L2_loss: 90219.984 , 398.44024658203125 , 38751.03125\n",
      "Valid auc & aupr: 0.9222454383973825 0.6242396455992295 ;   Test auc & aupr: 0.9320903715153479 0.5862337315400574\n",
      "step 2250 : Total_loss & DTIloss & L2_loss: 89991.14 , 396.3702392578125 , 38815.5078125\n",
      "Valid auc & aupr: 0.9232921705537056 0.6256775831527494 ;   Test auc & aupr: 0.9328304288899779 0.5876931240890267\n",
      "step 2275 : Total_loss & DTIloss & L2_loss: 89443.94 , 395.1595153808594 , 38876.9296875\n",
      "Valid auc & aupr: 0.9229647222702413 0.6257839715397067 ;   Test auc & aupr: 0.9298442337420486 0.5870412505689229\n",
      "step 2300 : Total_loss & DTIloss & L2_loss: 89191.195 , 393.80279541015625 , 38938.51171875\n",
      "Valid auc & aupr: 0.9230203408769201 0.6268422283546058 ;   Test auc & aupr: 0.9300642992879787 0.5868812318652495\n",
      "step 2325 : Total_loss & DTIloss & L2_loss: 88689.64 , 392.40423583984375 , 38998.3984375\n",
      "Valid auc & aupr: 0.9237569116140169 0.6262259457078277 ;   Test auc & aupr: 0.9300642992879787 0.5868812318652495\n",
      "step 2350 : Total_loss & DTIloss & L2_loss: 88445.555 , 390.70953369140625 , 39049.7734375\n",
      "Valid auc & aupr: 0.923930030790661 0.6271575623469358 ;   Test auc & aupr: 0.9305889945116229 0.5881192886765735\n",
      "step 2375 : Total_loss & DTIloss & L2_loss: 88018.94 , 389.5542907714844 , 39107.8828125\n",
      "Valid auc & aupr: 0.9246144904007798 0.6268852035399581 ;   Test auc & aupr: 0.9305889945116229 0.5881192886765735\n",
      "step 2400 : Total_loss & DTIloss & L2_loss: 87719.375 , 388.2030334472656 , 39154.36328125\n",
      "Valid auc & aupr: 0.9244761954868758 0.6259260952320487 ;   Test auc & aupr: 0.9305889945116229 0.5881192886765735\n",
      "step 2425 : Total_loss & DTIloss & L2_loss: 87448.05 , 387.0555114746094 , 39205.23828125\n",
      "Valid auc & aupr: 0.9245027521188935 0.6266545165818798 ;   Test auc & aupr: 0.9305889945116229 0.5881192886765735\n",
      "step 2450 : Total_loss & DTIloss & L2_loss: 87194.93 , 385.50799560546875 , 39265.296875\n",
      "Valid auc & aupr: 0.924466174116303 0.6266116507987358 ;   Test auc & aupr: 0.9305889945116229 0.5881192886765735\n",
      "step 2475 : Total_loss & DTIloss & L2_loss: 86754.016 , 384.82159423828125 , 39315.23828125\n",
      "Valid auc & aupr: 0.9245984562078632 0.6266473812820169 ;   Test auc & aupr: 0.9305889945116229 0.5881192886765735\n",
      "step 2500 : Total_loss & DTIloss & L2_loss: 86612.33 , 383.2142639160156 , 39360.2578125\n",
      "Valid auc & aupr: 0.9257920014430773 0.6273265396797406 ;   Test auc & aupr: 0.9284788187942425 0.5884222830054088\n",
      "step 2525 : Total_loss & DTIloss & L2_loss: 86316.26 , 382.48724365234375 , 39411.12109375\n",
      "Valid auc & aupr: 0.9260613257772199 0.6273481826391737 ;   Test auc & aupr: 0.9281255792873347 0.5880141451567061\n",
      "step 2550 : Total_loss & DTIloss & L2_loss: 85948.04 , 381.1956481933594 , 39456.3984375\n",
      "Valid auc & aupr: 0.9248066501815121 0.6274729000287811 ;   Test auc & aupr: 0.928233660393841 0.5883813536962245\n",
      "step 2575 : Total_loss & DTIloss & L2_loss: 85692.445 , 380.7355041503906 , 39501.70703125\n",
      "Valid auc & aupr: 0.9274142108045407 0.6277265330752525 ;   Test auc & aupr: 0.9276167883270078 0.5889295665333858\n",
      "step 2600 : Total_loss & DTIloss & L2_loss: 85579.19 , 379.2882080078125 , 39549.203125\n",
      "Valid auc & aupr: 0.9275169298529113 0.6279682649981707 ;   Test auc & aupr: 0.9284564170574969 0.5886574108159284\n",
      "step 2625 : Total_loss & DTIloss & L2_loss: 85197.234 , 378.2086181640625 , 39593.37109375\n",
      "Valid auc & aupr: 0.9282735433311535 0.6273281415422758 ;   Test auc & aupr: 0.9284564170574969 0.5886574108159284\n",
      "step 2650 : Total_loss & DTIloss & L2_loss: 85207.38 , 376.9371337890625 , 39633.1484375\n",
      "Valid auc & aupr: 0.929264406346534 0.6280492765353566 ;   Test auc & aupr: 0.9285229919158339 0.5896227649650057\n",
      "step 2675 : Total_loss & DTIloss & L2_loss: 84967.81 , 376.44000244140625 , 39689.76953125\n",
      "Valid auc & aupr: 0.929959388395754 0.6285930348608804 ;   Test auc & aupr: 0.9280318508087003 0.5906730491657716\n",
      "step 2700 : Total_loss & DTIloss & L2_loss: 84498.67 , 375.3313903808594 , 39730.05859375\n",
      "Valid auc & aupr: 0.9294092151513103 0.6283071087447951 ;   Test auc & aupr: 0.9280318508087003 0.5906730491657716\n",
      "step 2725 : Total_loss & DTIloss & L2_loss: 84290.98 , 374.6436767578125 , 39768.60546875\n",
      "Valid auc & aupr: 0.9311055826550119 0.6302512355956192 ;   Test auc & aupr: 0.9278949189806279 0.5899567425141565\n",
      "step 2750 : Total_loss & DTIloss & L2_loss: 84096.08 , 373.55511474609375 , 39808.05078125\n",
      "Valid auc & aupr: 0.9304782448571579 0.6288741015894642 ;   Test auc & aupr: 0.9278949189806279 0.5899567425141565\n",
      "step 2775 : Total_loss & DTIloss & L2_loss: 83889.56 , 372.8711853027344 , 39848.203125\n",
      "Valid auc & aupr: 0.930972799494923 0.6293144103612662 ;   Test auc & aupr: 0.9278949189806279 0.5899567425141565\n",
      "step 2800 : Total_loss & DTIloss & L2_loss: 83609.17 , 371.869384765625 , 39883.7890625\n",
      "Valid auc & aupr: 0.9324474441747026 0.6293012128727686 ;   Test auc & aupr: 0.9278949189806279 0.5899567425141565\n",
      "step 2825 : Total_loss & DTIloss & L2_loss: 83650.21 , 371.0010070800781 , 39921.7421875\n",
      "Valid auc & aupr: 0.9322617982848425 0.6275788906595656 ;   Test auc & aupr: 0.9278949189806279 0.5899567425141565\n",
      "step 2850 : Total_loss & DTIloss & L2_loss: 83251.375 , 370.26153564453125 , 39960.0546875\n",
      "Valid auc & aupr: 0.9316562569679844 0.6300068676351616 ;   Test auc & aupr: 0.9278949189806279 0.5899567425141565\n",
      "step 2875 : Total_loss & DTIloss & L2_loss: 83019.586 , 369.9571838378906 , 39996.25390625\n",
      "Valid auc & aupr: 0.932008257609352 0.63006778134735 ;   Test auc & aupr: 0.9278949189806279 0.5899567425141565\n",
      "step 2900 : Total_loss & DTIloss & L2_loss: 82911.375 , 368.8670654296875 , 40032.82421875\n",
      "Early Stopping\n",
      "len(fpr): 120\n",
      "len(recall): 48158\n",
      "Time spent in this fold: 1360.5157837867737\n",
      "--------------------------------------------------------------\n",
      "round  1  of  1 : KFold  2  of 10\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:00<00:00, 813.93it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_Di_emb = th.tensor(Drug_Di_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:00<00:00, 735.90it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_PDi_emb = th.tensor(Drug_PDi_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:00<00:00, 712.98it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_D2_emb = th.tensor(Drug_D2_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:00<00:00, 731.41it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:190: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_D4_emb = th.tensor(Drug_D4_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:01<00:00, 875.31it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:230: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_Di_emb = th.tensor(Protein_Di_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:01<00:00, 807.70it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_DiDP_emb = th.tensor(Protein_DiDP_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:01<00:00, 798.04it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:265: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_DDiP_emb = th.tensor(Protein_DDiP_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:02<00:00, 735.94it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:303: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_P2_emb = th.tensor(Protein_P2_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:06<00:00, 921.41it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:358: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_P_emb = th.tensor(Disease_P_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:06<00:00, 849.60it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:393: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_DPDi_emb = th.tensor(Disease_DPDi_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:07<00:00, 771.97it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:430: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_Di2_emb = th.tensor(Disease_Di2_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:07<00:00, 769.89it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:448: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_Di3_emb = th.tensor(Disease_Di3_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:04<00:00, 948.80it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:470: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_D_emb = th.tensor(Sideeffect_D_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:05<00:00, 758.12it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:490: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_Si1_emb = th.tensor(Sideeffect_Si1_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:05<00:00, 755.27it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:508: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_Si2_emb = th.tensor(Sideeffect_Si2_emb).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : Total_loss & DTIloss & L2_loss: 2075125.4 , 1651.612060546875 , 120118.4453125\n",
      "Valid auc & aupr: 0.5076651569252049 0.0017476843415531464 ;   Test auc & aupr: 0.5278048222987082 0.001811925014032161\n",
      "step 25 : Total_loss & DTIloss & L2_loss: 1147878.4 , 6129.96484375 , 117150.3515625\n",
      "Valid auc & aupr: 0.4392373744724203 0.0013044129814199026 ;   Test auc & aupr: 0.5278048222987082 0.001811925014032161\n",
      "step 50 : Total_loss & DTIloss & L2_loss: 851635.5 , 3822.64306640625 , 108608.09375\n",
      "Valid auc & aupr: 0.8470856044200153 0.016559035391032643 ;   Test auc & aupr: 0.8354328658013292 0.018615306529690196\n",
      "step 75 : Total_loss & DTIloss & L2_loss: 806626.4 , 1654.189208984375 , 95436.7734375\n",
      "Valid auc & aupr: 0.9243412161607628 0.03201020437808136 ;   Test auc & aupr: 0.8750598894915538 0.026979998519862526\n",
      "step 100 : Total_loss & DTIloss & L2_loss: 743771.0 , 1963.623291015625 , 85380.1796875\n",
      "Valid auc & aupr: 0.9175583223814157 0.03148888658642222 ;   Test auc & aupr: 0.8750598894915538 0.026979998519862526\n",
      "step 125 : Total_loss & DTIloss & L2_loss: 717250.75 , 3205.72607421875 , 77852.03125\n",
      "Valid auc & aupr: 0.8208178264756637 0.04858359398896317 ;   Test auc & aupr: 0.7882803859557402 0.01718026802438069\n",
      "step 150 : Total_loss & DTIloss & L2_loss: 684110.6 , 1626.030517578125 , 70566.03125\n",
      "Valid auc & aupr: 0.933229777981557 0.05429547643010968 ;   Test auc & aupr: 0.886260272978254 0.04449140902212573\n",
      "step 175 : Total_loss & DTIloss & L2_loss: 661117.75 , 1627.6337890625 , 62629.01953125\n",
      "Valid auc & aupr: 0.9444598251559622 0.08702598986390929 ;   Test auc & aupr: 0.891465185591843 0.07098447079875528\n",
      "step 200 : Total_loss & DTIloss & L2_loss: 640362.3 , 1598.132080078125 , 57857.953125\n",
      "Valid auc & aupr: 0.9457981294379887 0.11644392253335774 ;   Test auc & aupr: 0.9073376646267025 0.09243612001253562\n",
      "step 225 : Total_loss & DTIloss & L2_loss: 623547.9 , 1585.4776611328125 , 54078.34375\n",
      "Valid auc & aupr: 0.9437389682910521 0.1370741740727585 ;   Test auc & aupr: 0.9161820351551134 0.11136617415483158\n",
      "step 250 : Total_loss & DTIloss & L2_loss: 608875.75 , 1565.8553466796875 , 50280.04296875\n",
      "Valid auc & aupr: 0.9382755024659597 0.15850913670725514 ;   Test auc & aupr: 0.923530992778543 0.13782706372649134\n",
      "step 275 : Total_loss & DTIloss & L2_loss: 581751.94 , 1538.165283203125 , 47024.0546875\n",
      "Valid auc & aupr: 0.9425544103308774 0.19705280046003532 ;   Test auc & aupr: 0.927248517351721 0.1870866282301231\n",
      "step 300 : Total_loss & DTIloss & L2_loss: 553501.25 , 1499.9432373046875 , 43870.8515625\n",
      "Valid auc & aupr: 0.9484286414624832 0.2580129355265868 ;   Test auc & aupr: 0.9260426056788597 0.25155956376673566\n",
      "step 325 : Total_loss & DTIloss & L2_loss: 532731.0 , 1451.401123046875 , 41004.12109375\n",
      "Valid auc & aupr: 0.9384516815251636 0.34821602181994343 ;   Test auc & aupr: 0.924360390412813 0.3283026729353025\n",
      "step 350 : Total_loss & DTIloss & L2_loss: 482183.53 , 1362.506103515625 , 38904.4609375\n",
      "Valid auc & aupr: 0.938301491612892 0.42646032898314246 ;   Test auc & aupr: 0.9327756852519137 0.38519025985654715\n",
      "step 375 : Total_loss & DTIloss & L2_loss: 438834.53 , 1283.0733642578125 , 37404.51171875\n",
      "Valid auc & aupr: 0.9479336166059158 0.45531714079958824 ;   Test auc & aupr: 0.9238152329967302 0.4075791188331262\n",
      "step 400 : Total_loss & DTIloss & L2_loss: 401368.06 , 1213.2470703125 , 36063.16796875\n",
      "Valid auc & aupr: 0.9444196103707092 0.47371616456864774 ;   Test auc & aupr: 0.9280349055909838 0.4241828507207707\n",
      "step 425 : Total_loss & DTIloss & L2_loss: 365897.94 , 1149.84130859375 , 34980.40625\n",
      "Valid auc & aupr: 0.9443342565407844 0.4933968353474094 ;   Test auc & aupr: 0.9250014098062682 0.45131742506390227\n",
      "step 450 : Total_loss & DTIloss & L2_loss: 334090.47 , 1090.604248046875 , 34243.9296875\n",
      "Valid auc & aupr: 0.9431144080547753 0.5098738150377009 ;   Test auc & aupr: 0.9272986545720563 0.4726163719161943\n",
      "step 475 : Total_loss & DTIloss & L2_loss: 304791.06 , 1023.68603515625 , 33923.36328125\n",
      "Valid auc & aupr: 0.9417282290283998 0.5281314009021214 ;   Test auc & aupr: 0.9307151618687917 0.4913859812892812\n",
      "step 500 : Total_loss & DTIloss & L2_loss: 282218.06 , 979.5001220703125 , 33462.0234375\n",
      "Valid auc & aupr: 0.9460708786957935 0.536608919870568 ;   Test auc & aupr: 0.9318060100756417 0.5040887101606174\n",
      "step 525 : Total_loss & DTIloss & L2_loss: 265013.4 , 941.0836181640625 , 32985.4140625\n",
      "Valid auc & aupr: 0.9489608718346587 0.5457829504230143 ;   Test auc & aupr: 0.9327298150290536 0.5153607235994797\n",
      "step 550 : Total_loss & DTIloss & L2_loss: 248349.97 , 904.9495849609375 , 32652.701171875\n",
      "Valid auc & aupr: 0.9526335486511357 0.5587052743661263 ;   Test auc & aupr: 0.9339719961804553 0.5242375134379154\n",
      "step 575 : Total_loss & DTIloss & L2_loss: 233540.86 , 872.6634521484375 , 32438.146484375\n",
      "Valid auc & aupr: 0.9486982446656592 0.5659584141166891 ;   Test auc & aupr: 0.9335355502227785 0.532277717315556\n",
      "step 600 : Total_loss & DTIloss & L2_loss: 222827.69 , 838.5021362304688 , 32409.431640625\n",
      "Valid auc & aupr: 0.9508375618131341 0.5797275875721063 ;   Test auc & aupr: 0.9330318020776496 0.5412891427276395\n",
      "step 625 : Total_loss & DTIloss & L2_loss: 205299.47 , 797.21728515625 , 32489.123046875\n",
      "Valid auc & aupr: 0.949355633298061 0.5879425708749715 ;   Test auc & aupr: 0.9333465901186713 0.5533619242300458\n",
      "step 650 : Total_loss & DTIloss & L2_loss: 195810.84 , 766.0882568359375 , 32497.4296875\n",
      "Valid auc & aupr: 0.9494349685886964 0.5954698029813207 ;   Test auc & aupr: 0.9334616050960752 0.5611642093557789\n",
      "step 675 : Total_loss & DTIloss & L2_loss: 188899.33 , 745.280029296875 , 32474.39453125\n",
      "Valid auc & aupr: 0.9519419637727703 0.6003319984839087 ;   Test auc & aupr: 0.9337650468029013 0.5650395475279566\n",
      "step 700 : Total_loss & DTIloss & L2_loss: 182514.0 , 726.967529296875 , 32446.478515625\n",
      "Valid auc & aupr: 0.953363706894948 0.6053282953156972 ;   Test auc & aupr: 0.9346025420346528 0.569011494644307\n",
      "step 725 : Total_loss & DTIloss & L2_loss: 176086.78 , 710.8819580078125 , 32474.34375\n",
      "Valid auc & aupr: 0.9528586967345595 0.608165250950714 ;   Test auc & aupr: 0.9350468916353806 0.572286970572369\n",
      "step 750 : Total_loss & DTIloss & L2_loss: 170958.66 , 695.3231201171875 , 32529.587890625\n",
      "Valid auc & aupr: 0.9535407066640549 0.612923371895911 ;   Test auc & aupr: 0.9353868937523931 0.5753271327261545\n",
      "step 775 : Total_loss & DTIloss & L2_loss: 169768.86 , 679.8973388671875 , 32629.306640625\n",
      "Valid auc & aupr: 0.9552929223072234 0.617007707181674 ;   Test auc & aupr: 0.9357379997605635 0.5794659221818227\n",
      "step 800 : Total_loss & DTIloss & L2_loss: 164488.8 , 668.78515625 , 32857.12890625\n",
      "Valid auc & aupr: 0.9572412876172385 0.6194907199926937 ;   Test auc & aupr: 0.9360853236573345 0.579532498178405\n",
      "step 825 : Total_loss & DTIloss & L2_loss: 158619.5 , 649.287841796875 , 33102.515625\n",
      "Valid auc & aupr: 0.9581941318147649 0.6241150382157692 ;   Test auc & aupr: 0.9365589118884902 0.5829222269566636\n",
      "step 850 : Total_loss & DTIloss & L2_loss: 153420.5 , 630.318359375 , 33347.9609375\n",
      "Valid auc & aupr: 0.9602294923745107 0.6273088161033096 ;   Test auc & aupr: 0.935527947112118 0.5891730149697927\n",
      "step 875 : Total_loss & DTIloss & L2_loss: 148415.97 , 618.2608032226562 , 33508.78515625\n",
      "Valid auc & aupr: 0.9604188027921645 0.62947356389264 ;   Test auc & aupr: 0.9339934766336552 0.5895290256185322\n",
      "step 900 : Total_loss & DTIloss & L2_loss: 145218.39 , 608.9853515625 , 33574.5390625\n",
      "Valid auc & aupr: 0.9612986038083129 0.631626708300309 ;   Test auc & aupr: 0.9350268173518035 0.5920248794702685\n",
      "step 925 : Total_loss & DTIloss & L2_loss: 143302.56 , 599.5569458007812 , 33647.875\n",
      "Valid auc & aupr: 0.9632034714934621 0.6347389358929348 ;   Test auc & aupr: 0.9342108753061632 0.5940581394112449\n",
      "step 950 : Total_loss & DTIloss & L2_loss: 141075.81 , 591.1304321289062 , 33740.8046875\n",
      "Valid auc & aupr: 0.9630442537722562 0.6355806847012144 ;   Test auc & aupr: 0.9351003745696453 0.5950329336379\n",
      "step 975 : Total_loss & DTIloss & L2_loss: 138125.23 , 583.4494018554688 , 33850.85546875\n",
      "Valid auc & aupr: 0.9629170437372722 0.6368104251299287 ;   Test auc & aupr: 0.9347562024323729 0.5955705906357848\n",
      "step 1000 : Total_loss & DTIloss & L2_loss: 135537.94 , 576.0452270507812 , 33960.3203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid auc & aupr: 0.9636491169708582 0.6396564806228191 ;   Test auc & aupr: 0.9349437078782495 0.5972895468373834\n",
      "step 1025 : Total_loss & DTIloss & L2_loss: 133885.28 , 568.009033203125 , 34071.27734375\n",
      "Valid auc & aupr: 0.9640991395677376 0.6418046287562879 ;   Test auc & aupr: 0.9361527713106098 0.5986798831507829\n",
      "step 1050 : Total_loss & DTIloss & L2_loss: 131907.19 , 561.0204467773438 , 34185.26171875\n",
      "Valid auc & aupr: 0.9642383866813009 0.641695392705352 ;   Test auc & aupr: 0.9361527713106098 0.5986798831507829\n",
      "step 1075 : Total_loss & DTIloss & L2_loss: 129662.76 , 555.1941528320312 , 34305.625\n",
      "Valid auc & aupr: 0.9643294854805475 0.6438646080159109 ;   Test auc & aupr: 0.9360796504902367 0.6011529917056834\n",
      "step 1100 : Total_loss & DTIloss & L2_loss: 128035.78 , 548.79931640625 , 34429.49609375\n",
      "Valid auc & aupr: 0.9642006340257572 0.6456850100294693 ;   Test auc & aupr: 0.9349329434073459 0.6025853659821486\n",
      "step 1125 : Total_loss & DTIloss & L2_loss: 126964.09 , 542.6446533203125 , 34583.3203125\n",
      "Valid auc & aupr: 0.9634510523142379 0.6456148293533504 ;   Test auc & aupr: 0.9349329434073459 0.6025853659821486\n",
      "step 1150 : Total_loss & DTIloss & L2_loss: 124394.59 , 534.71875 , 34842.30078125\n",
      "Valid auc & aupr: 0.9641806634181145 0.6482996026378253 ;   Test auc & aupr: 0.9362083877436121 0.604707156412792\n",
      "step 1175 : Total_loss & DTIloss & L2_loss: 121310.92 , 525.93310546875 , 35034.91015625\n",
      "Valid auc & aupr: 0.9650459652259743 0.652083944671653 ;   Test auc & aupr: 0.9352519984458432 0.6067142727606454\n",
      "step 1200 : Total_loss & DTIloss & L2_loss: 118992.55 , 517.8038940429688 , 35185.95703125\n",
      "Valid auc & aupr: 0.9646120832571896 0.6533707951841102 ;   Test auc & aupr: 0.9350983380481231 0.6075368630082719\n",
      "step 1225 : Total_loss & DTIloss & L2_loss: 116772.445 , 510.5993957519531 , 35334.5390625\n",
      "Valid auc & aupr: 0.9646610522814094 0.6553841468391007 ;   Test auc & aupr: 0.9345427555813902 0.6085278744334109\n",
      "step 1250 : Total_loss & DTIloss & L2_loss: 115021.516 , 503.9502258300781 , 35485.5625\n",
      "Valid auc & aupr: 0.9658636658594528 0.6567799411951396 ;   Test auc & aupr: 0.9346619405790538 0.6094587873720517\n",
      "step 1275 : Total_loss & DTIloss & L2_loss: 113273.92 , 497.94573974609375 , 35633.69140625\n",
      "Valid auc & aupr: 0.9652711133093979 0.6584063195773491 ;   Test auc & aupr: 0.9326958245150739 0.610391754544634\n",
      "step 1300 : Total_loss & DTIloss & L2_loss: 111728.06 , 492.2363586425781 , 35771.3984375\n",
      "Valid auc & aupr: 0.9649718277647255 0.6594427672971931 ;   Test auc & aupr: 0.9328315441279544 0.6115454659209686\n",
      "step 1325 : Total_loss & DTIloss & L2_loss: 110273.05 , 487.1297912597656 , 35902.70703125\n",
      "Valid auc & aupr: 0.9644758454132711 0.6614656701135015 ;   Test auc & aupr: 0.9318678330504265 0.6117620875722767\n",
      "step 1350 : Total_loss & DTIloss & L2_loss: 109017.89 , 482.3746643066406 , 36028.53515625\n",
      "Valid auc & aupr: 0.964869239026835 0.6631511724196097 ;   Test auc & aupr: 0.9322278124738009 0.6133983989036063\n",
      "step 1375 : Total_loss & DTIloss & L2_loss: 107837.21 , 478.0922546386719 , 36160.8359375\n",
      "Valid auc & aupr: 0.96393773328679 0.6628717186602492 ;   Test auc & aupr: 0.9322278124738009 0.6133983989036063\n",
      "step 1400 : Total_loss & DTIloss & L2_loss: 106655.32 , 473.7464599609375 , 36283.890625\n",
      "Valid auc & aupr: 0.9654790264848556 0.6644111931829203 ;   Test auc & aupr: 0.9299015715206237 0.6139788413142457\n",
      "step 1425 : Total_loss & DTIloss & L2_loss: 105570.766 , 469.9046630859375 , 36402.34765625\n",
      "Valid auc & aupr: 0.9642758657668768 0.6645523998884598 ;   Test auc & aupr: 0.9300919377943531 0.6135691263999374\n",
      "step 1450 : Total_loss & DTIloss & L2_loss: 104620.016 , 466.0917053222656 , 36513.921875\n",
      "Valid auc & aupr: 0.9652782261285582 0.6667361761538809 ;   Test auc & aupr: 0.9294058724843686 0.6139133033440983\n",
      "step 1475 : Total_loss & DTIloss & L2_loss: 103644.07 , 462.67803955078125 , 36631.265625\n",
      "Valid auc & aupr: 0.9654418209692475 0.6662082600105581 ;   Test auc & aupr: 0.9294058724843686 0.6139133033440983\n",
      "step 1500 : Total_loss & DTIloss & L2_loss: 102824.06 , 459.0121765136719 , 36737.92578125\n",
      "Valid auc & aupr: 0.9639311676075649 0.6678394166933699 ;   Test auc & aupr: 0.9279168843199044 0.6145794609395311\n",
      "step 1525 : Total_loss & DTIloss & L2_loss: 101880.5 , 456.1663818359375 , 36848.1328125\n",
      "Valid auc & aupr: 0.9649586964062754 0.6674337288723113 ;   Test auc & aupr: 0.9279168843199044 0.6145794609395311\n",
      "step 1550 : Total_loss & DTIloss & L2_loss: 101093.67 , 452.9539489746094 , 36949.76953125\n",
      "Valid auc & aupr: 0.9652180407356624 0.6684411146465544 ;   Test auc & aupr: 0.9272785802884791 0.6153166808166021\n",
      "step 1575 : Total_loss & DTIloss & L2_loss: 100266.28 , 450.2634582519531 , 37052.72265625\n",
      "Valid auc & aupr: 0.9634464016247868 0.6694412674665663 ;   Test auc & aupr: 0.9267623220825703 0.6156857620047801\n",
      "step 1600 : Total_loss & DTIloss & L2_loss: 99631.234 , 447.2272644042969 , 37151.88671875\n",
      "Valid auc & aupr: 0.9625356872022873 0.6686899361854663 ;   Test auc & aupr: 0.9267623220825703 0.6156857620047801\n",
      "step 1625 : Total_loss & DTIloss & L2_loss: 98778.234 , 444.5897521972656 , 37249.11328125\n",
      "Valid auc & aupr: 0.962548271420802 0.6685566039092671 ;   Test auc & aupr: 0.9267623220825703 0.6156857620047801\n",
      "step 1650 : Total_loss & DTIloss & L2_loss: 98223.83 , 442.060546875 , 37342.12890625\n",
      "Valid auc & aupr: 0.9631419182507279 0.6698122695289476 ;   Test auc & aupr: 0.9269410510904579 0.6166066431154283\n",
      "step 1675 : Total_loss & DTIloss & L2_loss: 97590.06 , 439.7629699707031 , 37436.76953125\n",
      "Valid auc & aupr: 0.9617108737496485 0.6695072545115437 ;   Test auc & aupr: 0.9269410510904579 0.6166066431154283\n",
      "step 1700 : Total_loss & DTIloss & L2_loss: 97102.52 , 436.9417724609375 , 37529.125\n",
      "Valid auc & aupr: 0.959553227414337 0.669545293380244 ;   Test auc & aupr: 0.9269410510904579 0.6166066431154283\n",
      "step 1725 : Total_loss & DTIloss & L2_loss: 96258.5 , 434.8283386230469 , 37620.74609375\n",
      "Valid auc & aupr: 0.9587626102076615 0.6692058696756873 ;   Test auc & aupr: 0.9269410510904579 0.6166066431154283\n",
      "step 1750 : Total_loss & DTIloss & L2_loss: 95856.3 , 432.36761474609375 , 37703.16796875\n",
      "Valid auc & aupr: 0.9587948914638509 0.6699839253329128 ;   Test auc & aupr: 0.9276048116409121 0.618518657934729\n",
      "step 1775 : Total_loss & DTIloss & L2_loss: 95641.22 , 430.53277587890625 , 37803.3515625\n",
      "Valid auc & aupr: 0.9573944867991547 0.6709397606520927 ;   Test auc & aupr: 0.9277540595753337 0.6185096652034779\n",
      "step 1800 : Total_loss & DTIloss & L2_loss: 94990.36 , 428.24224853515625 , 37894.4765625\n",
      "Valid auc & aupr: 0.9553974260348881 0.672684663352045 ;   Test auc & aupr: 0.9275274723116715 0.6187356800929767\n",
      "step 1825 : Total_loss & DTIloss & L2_loss: 94116.92 , 425.98248291015625 , 37977.46484375\n",
      "Valid auc & aupr: 0.9549567048169105 0.6715224117935277 ;   Test auc & aupr: 0.9275274723116715 0.6187356800929767\n",
      "step 1850 : Total_loss & DTIloss & L2_loss: 93916.41 , 423.93121337890625 , 38054.4375\n",
      "Valid auc & aupr: 0.95418578464791 0.6717318093756554 ;   Test auc & aupr: 0.9275274723116715 0.6187356800929767\n",
      "step 1875 : Total_loss & DTIloss & L2_loss: 93219.93 , 421.92059326171875 , 38137.609375\n",
      "Valid auc & aupr: 0.9567269760779478 0.6727139441103661 ;   Test auc & aupr: 0.9271703052275422 0.6193800837259376\n",
      "step 1900 : Total_loss & DTIloss & L2_loss: 92816.86 , 419.7754211425781 , 38208.12890625\n",
      "Valid auc & aupr: 0.9547230760644883 0.672947423735588 ;   Test auc & aupr: 0.9269537066170609 0.6199639434258248\n",
      "step 1925 : Total_loss & DTIloss & L2_loss: 92220.14 , 417.7723388671875 , 38281.38671875\n",
      "Valid auc & aupr: 0.9554080952636286 0.672860744849297 ;   Test auc & aupr: 0.9269537066170609 0.6199639434258248\n",
      "step 1950 : Total_loss & DTIloss & L2_loss: 91953.984 , 416.22113037109375 , 38349.48046875\n",
      "Valid auc & aupr: 0.9537633926177692 0.6736225924998059 ;   Test auc & aupr: 0.9274716619242384 0.6200113496500257\n",
      "step 1975 : Total_loss & DTIloss & L2_loss: 91421.54 , 414.3751220703125 , 38421.38671875\n",
      "Valid auc & aupr: 0.9522480885666356 0.6735186069112111 ;   Test auc & aupr: 0.9274716619242384 0.6200113496500257\n",
      "step 2000 : Total_loss & DTIloss & L2_loss: 91051.9 , 412.50299072265625 , 38494.68359375\n",
      "Valid auc & aupr: 0.953394620301299 0.6743211845429795 ;   Test auc & aupr: 0.9273352149822427 0.6206950646952221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2025 : Total_loss & DTIloss & L2_loss: 90643.07 , 411.19891357421875 , 38559.6953125\n",
      "Valid auc & aupr: 0.9519277381344498 0.6746928099093777 ;   Test auc & aupr: 0.927304958091054 0.6201931896245922\n",
      "step 2050 : Total_loss & DTIloss & L2_loss: 90419.5 , 408.9823913574219 , 38622.921875\n",
      "Valid auc & aupr: 0.9525583169100164 0.6746036706934473 ;   Test auc & aupr: 0.927304958091054 0.6201931896245922\n",
      "step 2075 : Total_loss & DTIloss & L2_loss: 89854.44 , 407.75836181640625 , 38688.67578125\n",
      "Valid auc & aupr: 0.9514812719471506 0.6755368396415944 ;   Test auc & aupr: 0.9277019828106916 0.6206607549235694\n",
      "step 2100 : Total_loss & DTIloss & L2_loss: 90554.97 , 405.900390625 , 38760.98046875\n",
      "Valid auc & aupr: 0.9527572022765398 0.6749928987260885 ;   Test auc & aupr: 0.9277019828106916 0.6206607549235694\n",
      "step 2125 : Total_loss & DTIloss & L2_loss: 89274.84 , 404.57720947265625 , 38852.34375\n",
      "Valid auc & aupr: 0.9522718891538261 0.676287111734675 ;   Test auc & aupr: 0.9275804461155559 0.6212297444243984\n",
      "step 2150 : Total_loss & DTIloss & L2_loss: 89095.89 , 403.01019287109375 , 38903.92578125\n",
      "Valid auc & aupr: 0.9513420248335873 0.6757269642896392 ;   Test auc & aupr: 0.9275804461155559 0.6212297444243984\n",
      "step 2175 : Total_loss & DTIloss & L2_loss: 88589.45 , 401.5863037109375 , 38961.87890625\n",
      "Valid auc & aupr: 0.9513792303491956 0.677052879497231 ;   Test auc & aupr: 0.9273741028455976 0.6210189767061162\n",
      "step 2200 : Total_loss & DTIloss & L2_loss: 88238.84 , 399.9550476074219 , 39013.82421875\n",
      "Valid auc & aupr: 0.9516864494229315 0.6775657251472055 ;   Test auc & aupr: 0.927694758008148 0.6218951770883142\n",
      "step 2225 : Total_loss & DTIloss & L2_loss: 88115.99 , 398.75201416015625 , 39068.67578125\n",
      "Valid auc & aupr: 0.9505068157221755 0.6764837028952977 ;   Test auc & aupr: 0.927694758008148 0.6218951770883142\n",
      "step 2250 : Total_loss & DTIloss & L2_loss: 87911.59 , 397.4122314453125 , 39128.453125\n",
      "Valid auc & aupr: 0.9503593615095809 0.6773520403505326 ;   Test auc & aupr: 0.927694758008148 0.6218951770883142\n",
      "step 2275 : Total_loss & DTIloss & L2_loss: 87340.92 , 396.2530517578125 , 39186.640625\n",
      "Valid auc & aupr: 0.951685081573093 0.6771892366235479 ;   Test auc & aupr: 0.927694758008148 0.6218951770883142\n",
      "step 2300 : Total_loss & DTIloss & L2_loss: 87304.22 , 394.6876220703125 , 39236.00390625\n",
      "Valid auc & aupr: 0.9509743467969881 0.6783530792655651 ;   Test auc & aupr: 0.9280495006618936 0.6216955357919973\n",
      "step 2325 : Total_loss & DTIloss & L2_loss: 86767.17 , 393.61669921875 , 39290.42578125\n",
      "Valid auc & aupr: 0.94994654442831 0.6779455461300758 ;   Test auc & aupr: 0.9280495006618936 0.6216955357919973\n",
      "step 2350 : Total_loss & DTIloss & L2_loss: 86611.26 , 392.0735168457031 , 39339.19921875\n",
      "Valid auc & aupr: 0.9488850929536037 0.6777505431148001 ;   Test auc & aupr: 0.9280495006618936 0.6216955357919973\n",
      "step 2375 : Total_loss & DTIloss & L2_loss: 86290.5 , 391.1696472167969 , 39390.2890625\n",
      "Valid auc & aupr: 0.9497714596489769 0.6787032795725876 ;   Test auc & aupr: 0.9280863035151187 0.6215522616883249\n",
      "step 2400 : Total_loss & DTIloss & L2_loss: 85954.734 , 390.1175537109375 , 39439.98828125\n",
      "Valid auc & aupr: 0.9504717987663088 0.6790100339567255 ;   Test auc & aupr: 0.9283423718522469 0.6215024514809098\n",
      "step 2425 : Total_loss & DTIloss & L2_loss: 85857.14 , 388.85693359375 , 39489.48046875\n",
      "Valid auc & aupr: 0.94815630255963 0.6789225059490093 ;   Test auc & aupr: 0.9283423718522469 0.6215024514809098\n",
      "step 2450 : Total_loss & DTIloss & L2_loss: 85596.914 , 387.7870788574219 , 39534.35546875\n",
      "Valid auc & aupr: 0.9479760199509106 0.6791676169033013 ;   Test auc & aupr: 0.928645377161604 0.621694637064364\n",
      "step 2475 : Total_loss & DTIloss & L2_loss: 85249.06 , 386.89410400390625 , 39583.12890625\n",
      "Valid auc & aupr: 0.948204177303979 0.6789543155474628 ;   Test auc & aupr: 0.928645377161604 0.621694637064364\n",
      "step 2500 : Total_loss & DTIloss & L2_loss: 85119.94 , 385.54443359375 , 39628.94140625\n",
      "Valid auc & aupr: 0.9494415342679212 0.6796085630912672 ;   Test auc & aupr: 0.9287215042756526 0.6215967370844685\n",
      "step 2525 : Total_loss & DTIloss & L2_loss: 84806.836 , 384.6676940917969 , 39674.68359375\n",
      "Valid auc & aupr: 0.9487690992872955 0.6796320037208851 ;   Test auc & aupr: 0.9296722688949314 0.621471299504765\n",
      "step 2550 : Total_loss & DTIloss & L2_loss: 84589.25 , 383.8390197753906 , 39719.8125\n",
      "Valid auc & aupr: 0.9476912336145267 0.6788999011124108 ;   Test auc & aupr: 0.9296722688949314 0.621471299504765\n",
      "step 2575 : Total_loss & DTIloss & L2_loss: 84381.555 , 382.76837158203125 , 39769.27734375\n",
      "Valid auc & aupr: 0.9472250703895525 0.6801437542634433 ;   Test auc & aupr: 0.9294472332667147 0.6209510846662214\n",
      "step 2600 : Total_loss & DTIloss & L2_loss: 84137.86 , 381.94805908203125 , 39810.05078125\n",
      "Valid auc & aupr: 0.9478419706667338 0.678945151603888 ;   Test auc & aupr: 0.9294472332667147 0.6209510846662214\n",
      "step 2625 : Total_loss & DTIloss & L2_loss: 84101.484 , 380.85089111328125 , 39854.76171875\n",
      "Valid auc & aupr: 0.9471736392356237 0.679307335580203 ;   Test auc & aupr: 0.9294472332667147 0.6209510846662214\n",
      "step 2650 : Total_loss & DTIloss & L2_loss: 84978.766 , 380.3402404785156 , 39967.55859375\n",
      "Valid auc & aupr: 0.9469356333637179 0.6778447787391833 ;   Test auc & aupr: 0.9294472332667147 0.6209510846662214\n",
      "step 2675 : Total_loss & DTIloss & L2_loss: 84640.53 , 377.9466857910156 , 40072.64453125\n",
      "Valid auc & aupr: 0.9475049324665177 0.6782457600858516 ;   Test auc & aupr: 0.9294472332667147 0.6209510846662214\n",
      "step 2700 : Total_loss & DTIloss & L2_loss: 83770.39 , 378.040771484375 , 40118.421875\n",
      "Valid auc & aupr: 0.9477177699013944 0.6799067496431266 ;   Test auc & aupr: 0.9294472332667147 0.6209510846662214\n",
      "step 2725 : Total_loss & DTIloss & L2_loss: 83294.39 , 377.33148193359375 , 40147.3359375\n",
      "Valid auc & aupr: 0.9474647176812646 0.6797889317079372 ;   Test auc & aupr: 0.9294472332667147 0.6209510846662214\n",
      "step 2750 : Total_loss & DTIloss & L2_loss: 83038.91 , 376.3446044921875 , 40169.00390625\n",
      "Early Stopping\n",
      "len(fpr): 98\n",
      "len(recall): 48162\n",
      "Time spent in this fold: 1315.259928226471\n",
      "--------------------------------------------------------------\n",
      "round  1  of  1 : KFold  3  of 10\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:01<00:00, 695.25it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_Di_emb = th.tensor(Drug_Di_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:01<00:00, 648.95it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_PDi_emb = th.tensor(Drug_PDi_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:01<00:00, 597.41it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_D2_emb = th.tensor(Drug_D2_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:01<00:00, 589.02it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:190: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_D4_emb = th.tensor(Drug_D4_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:01<00:00, 763.74it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:230: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_Di_emb = th.tensor(Protein_Di_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:02<00:00, 678.03it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_DiDP_emb = th.tensor(Protein_DiDP_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:02<00:00, 698.71it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:265: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_DDiP_emb = th.tensor(Protein_DDiP_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:02<00:00, 627.65it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:303: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_P2_emb = th.tensor(Protein_P2_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:06<00:00, 822.77it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:358: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_P_emb = th.tensor(Disease_P_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:07<00:00, 720.98it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:393: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_DPDi_emb = th.tensor(Disease_DPDi_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:08<00:00, 656.33it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:430: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_Di2_emb = th.tensor(Disease_Di2_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:08<00:00, 657.24it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:448: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_Di3_emb = th.tensor(Disease_Di3_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:04<00:00, 930.21it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:470: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_D_emb = th.tensor(Sideeffect_D_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:06<00:00, 654.00it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:490: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_Si1_emb = th.tensor(Sideeffect_Si1_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:06<00:00, 647.47it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:508: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_Si2_emb = th.tensor(Sideeffect_Si2_emb).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : Total_loss & DTIloss & L2_loss: 2064643.1 , 1655.5084228515625 , 120097.6328125\n",
      "Valid auc & aupr: 0.5040966464118504 0.0017929050397698227 ;   Test auc & aupr: 0.49122130729068403 0.0016735810625809503\n",
      "step 25 : Total_loss & DTIloss & L2_loss: 1143109.0 , 4129.2353515625 , 117149.234375\n",
      "Valid auc & aupr: 0.7204722026715421 0.005024838431215186 ;   Test auc & aupr: 0.7673893845151922 0.005956853640920093\n",
      "step 50 : Total_loss & DTIloss & L2_loss: 847997.25 , 4935.6650390625 , 108645.8671875\n",
      "Valid auc & aupr: 0.8434282608431498 0.042888533990038204 ;   Test auc & aupr: 0.8497768384564699 0.0442824133664447\n",
      "step 75 : Total_loss & DTIloss & L2_loss: 789470.3 , 4495.3837890625 , 97776.9140625\n",
      "Valid auc & aupr: 0.713885531780059 0.006127270065766568 ;   Test auc & aupr: 0.8497768384564699 0.0442824133664447\n",
      "step 100 : Total_loss & DTIloss & L2_loss: 757503.0 , 3171.2451171875 , 87466.8984375\n",
      "Valid auc & aupr: 0.8417962853396069 0.03350509976866723 ;   Test auc & aupr: 0.8497768384564699 0.0442824133664447\n",
      "step 125 : Total_loss & DTIloss & L2_loss: 729503.2 , 1619.0843505859375 , 79061.0859375\n",
      "Valid auc & aupr: 0.9043058075334457 0.05753900756181801 ;   Test auc & aupr: 0.9046427405450729 0.06434246555709777\n",
      "step 150 : Total_loss & DTIloss & L2_loss: 697088.3 , 1651.35400390625 , 72406.9609375\n",
      "Valid auc & aupr: 0.887585302499652 0.07285089395164529 ;   Test auc & aupr: 0.8780014266318149 0.08804671996250013\n",
      "step 175 : Total_loss & DTIloss & L2_loss: 677129.4 , 1603.0750732421875 , 66475.96875\n",
      "Valid auc & aupr: 0.9017230146868448 0.09104592568718316 ;   Test auc & aupr: 0.8962071047350532 0.11687793126070992\n",
      "step 200 : Total_loss & DTIloss & L2_loss: 659854.9 , 1591.2657470703125 , 60890.46875\n",
      "Valid auc & aupr: 0.911731483013341 0.10431352456884498 ;   Test auc & aupr: 0.9042517041684832 0.1471918474213625\n",
      "step 225 : Total_loss & DTIloss & L2_loss: 641008.06 , 1580.9581298828125 , 56092.56640625\n",
      "Valid auc & aupr: 0.9200724574690922 0.12140400888382863 ;   Test auc & aupr: 0.9050286613735533 0.16303870989775598\n",
      "step 250 : Total_loss & DTIloss & L2_loss: 619117.4 , 1565.1402587890625 , 51490.8515625\n",
      "Valid auc & aupr: 0.9236383145973946 0.13983615301612565 ;   Test auc & aupr: 0.9088768627444133 0.1782887313585293\n",
      "step 275 : Total_loss & DTIloss & L2_loss: 597182.1 , 1542.7548828125 , 47742.12109375\n",
      "Valid auc & aupr: 0.9153791258555954 0.16337428534144616 ;   Test auc & aupr: 0.9106494122865575 0.20844533679379174\n",
      "step 300 : Total_loss & DTIloss & L2_loss: 580025.8 , 1508.164306640625 , 44295.74609375\n",
      "Valid auc & aupr: 0.9106797187319194 0.21011463811301315 ;   Test auc & aupr: 0.9161492811006294 0.2673428360245329\n",
      "step 325 : Total_loss & DTIloss & L2_loss: 542475.3 , 1463.3531494140625 , 41386.6640625\n",
      "Valid auc & aupr: 0.910662193221787 0.2630226978271803 ;   Test auc & aupr: 0.9230063460435067 0.3332381288198968\n",
      "step 350 : Total_loss & DTIloss & L2_loss: 503170.1 , 1382.38818359375 , 38506.5\n",
      "Valid auc & aupr: 0.9158981146289872 0.34962206285612313 ;   Test auc & aupr: 0.9280436335403651 0.4211789172406575\n",
      "step 375 : Total_loss & DTIloss & L2_loss: 459411.1 , 1315.4674072265625 , 37036.78515625\n",
      "Valid auc & aupr: 0.9207790860376363 0.37322776148595843 ;   Test auc & aupr: 0.9284849768474173 0.44751429291558875\n",
      "step 400 : Total_loss & DTIloss & L2_loss: 416918.88 , 1248.154541015625 , 35815.96484375\n",
      "Valid auc & aupr: 0.9162416146275854 0.39891061128966515 ;   Test auc & aupr: 0.92682904240673 0.46500953496964903\n",
      "step 425 : Total_loss & DTIloss & L2_loss: 380922.62 , 1183.3458251953125 , 34810.046875\n",
      "Valid auc & aupr: 0.9207538493030456 0.42806953330265884 ;   Test auc & aupr: 0.9249651888163355 0.48298234502766335\n",
      "step 450 : Total_loss & DTIloss & L2_loss: 346067.06 , 1123.72802734375 , 34035.7265625\n",
      "Valid auc & aupr: 0.9304405492635079 0.4582675041671306 ;   Test auc & aupr: 0.9279003981932951 0.5023784597449165\n",
      "step 475 : Total_loss & DTIloss & L2_loss: 318825.0 , 1072.8642578125 , 33492.69921875\n",
      "Valid auc & aupr: 0.9308861645678115 0.47507062280579776 ;   Test auc & aupr: 0.9314519462624035 0.515997305833836\n",
      "step 500 : Total_loss & DTIloss & L2_loss: 288123.88 , 1014.48095703125 , 33180.0625\n",
      "Valid auc & aupr: 0.9333619349658697 0.5038055463927419 ;   Test auc & aupr: 0.9332031608171707 0.5366629181773457\n",
      "step 525 : Total_loss & DTIloss & L2_loss: 259235.19 , 954.571533203125 , 32828.6015625\n",
      "Valid auc & aupr: 0.9330060502734446 0.5263013953554552 ;   Test auc & aupr: 0.9391887400143167 0.5514536541400409\n",
      "step 550 : Total_loss & DTIloss & L2_loss: 238262.33 , 898.8718872070312 , 32502.400390625\n",
      "Valid auc & aupr: 0.9366312604627295 0.5442776034633057 ;   Test auc & aupr: 0.9378353017526062 0.5673707528618035\n",
      "step 575 : Total_loss & DTIloss & L2_loss: 221356.97 , 853.8168334960938 , 32408.01171875\n",
      "Valid auc & aupr: 0.9380024563755002 0.5608734311597404 ;   Test auc & aupr: 0.9356783102845161 0.5803465263567787\n",
      "step 600 : Total_loss & DTIloss & L2_loss: 207597.28 , 816.2797241210938 , 32298.638671875\n",
      "Valid auc & aupr: 0.9381648594360618 0.5749879990599137 ;   Test auc & aupr: 0.9325649537629609 0.5910148229671853\n",
      "step 625 : Total_loss & DTIloss & L2_loss: 198344.19 , 786.2627563476562 , 32414.490234375\n",
      "Valid auc & aupr: 0.9376825573972141 0.5827037772273961 ;   Test auc & aupr: 0.9313283003128339 0.5985815534607946\n",
      "step 650 : Total_loss & DTIloss & L2_loss: 192139.19 , 768.7903442382812 , 32429.052734375\n",
      "Valid auc & aupr: 0.9379337563757805 0.5914531782069075 ;   Test auc & aupr: 0.9310688862617762 0.6031954235799787\n",
      "step 675 : Total_loss & DTIloss & L2_loss: 186111.44 , 752.0194091796875 , 32385.9453125\n",
      "Valid auc & aupr: 0.9380473216814396 0.597643089487943 ;   Test auc & aupr: 0.9301916303717316 0.6078220977397326\n",
      "step 700 : Total_loss & DTIloss & L2_loss: 179788.67 , 736.6777954101562 , 32409.841796875\n",
      "Valid auc & aupr: 0.9372537665826377 0.603520202572946 ;   Test auc & aupr: 0.9291390881649442 0.6128392145258185\n",
      "step 725 : Total_loss & DTIloss & L2_loss: 174171.08 , 721.13330078125 , 32463.462890625\n",
      "Valid auc & aupr: 0.9375040308673306 0.6083184469119445 ;   Test auc & aupr: 0.9284750851714516 0.6169162193429106\n",
      "step 750 : Total_loss & DTIloss & L2_loss: 170297.94 , 705.1180419921875 , 32533.30859375\n",
      "Valid auc & aupr: 0.9378021982130521 0.61314627116861 ;   Test auc & aupr: 0.9277324821449188 0.6200859753501262\n",
      "step 775 : Total_loss & DTIloss & L2_loss: 165832.47 , 691.3447265625 , 32623.3828125\n",
      "Valid auc & aupr: 0.9368731125025588 0.6174734262097546 ;   Test auc & aupr: 0.9269984616019443 0.6236235794520851\n",
      "step 800 : Total_loss & DTIloss & L2_loss: 161698.56 , 677.4910888671875 , 32726.814453125\n",
      "Valid auc & aupr: 0.9376360563769957 0.6210442269787915 ;   Test auc & aupr: 0.9263928873806992 0.6271170273647939\n",
      "step 825 : Total_loss & DTIloss & L2_loss: 157376.06 , 663.9649658203125 , 32830.63671875\n",
      "Valid auc & aupr: 0.9384835900470057 0.6228085159166319 ;   Test auc & aupr: 0.9259765641952071 0.6292093281680773\n",
      "step 850 : Total_loss & DTIloss & L2_loss: 156069.66 , 652.8338623046875 , 32973.23828125\n",
      "Valid auc & aupr: 0.9355526237324382 0.6281964267743406 ;   Test auc & aupr: 0.9252468591383157 0.6335469464632402\n",
      "step 875 : Total_loss & DTIloss & L2_loss: 149701.81 , 636.811767578125 , 33227.8359375\n",
      "Valid auc & aupr: 0.9363910441371794 0.6310921627745004 ;   Test auc & aupr: 0.9245610847599773 0.6374530282713682\n",
      "step 900 : Total_loss & DTIloss & L2_loss: 144576.64 , 618.8883666992188 , 33450.76171875\n",
      "Valid auc & aupr: 0.9374965533163405 0.6371745384812806 ;   Test auc & aupr: 0.9245969663296564 0.6413582544411424\n",
      "step 925 : Total_loss & DTIloss & L2_loss: 140020.06 , 603.8621826171875 , 33629.20703125\n",
      "Valid auc & aupr: 0.9377101308664894 0.63963348553477 ;   Test auc & aupr: 0.9243610692533205 0.644729031878832\n",
      "step 950 : Total_loss & DTIloss & L2_loss: 136149.47 , 590.4894409179688 , 33773.51171875\n",
      "Valid auc & aupr: 0.9380092329060846 0.6450056379243135 ;   Test auc & aupr: 0.9246285324132523 0.6486893872019137\n",
      "step 975 : Total_loss & DTIloss & L2_loss: 132876.08 , 579.1016845703125 , 33920.40234375\n",
      "Valid auc & aupr: 0.937454725765491 0.6458169452293746 ;   Test auc & aupr: 0.9250919865254039 0.6507577618621251\n",
      "step 1000 : Total_loss & DTIloss & L2_loss: 129927.72 , 568.3753662109375 , 34073.05859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid auc & aupr: 0.937410561479957 0.6495929190007146 ;   Test auc & aupr: 0.9254729130272936 0.6531958414777271\n",
      "step 1025 : Total_loss & DTIloss & L2_loss: 127329.42 , 559.07421875 , 34215.85546875\n",
      "Valid auc & aupr: 0.9373105492354672 0.6520999802535189 ;   Test auc & aupr: 0.9261619846309539 0.6554370002588666\n",
      "step 1050 : Total_loss & DTIloss & L2_loss: 125062.625 , 550.3356323242188 , 34363.6484375\n",
      "Valid auc & aupr: 0.9372252584194888 0.6534084577244194 ;   Test auc & aupr: 0.9281584060747303 0.658017520562462\n",
      "step 1075 : Total_loss & DTIloss & L2_loss: 122832.85 , 542.5899658203125 , 34504.46875\n",
      "Valid auc & aupr: 0.9351922992441131 0.6573907607958502 ;   Test auc & aupr: 0.9289765543520199 0.6599396896103841\n",
      "step 1100 : Total_loss & DTIloss & L2_loss: 120886.58 , 535.1491088867188 , 34643.62890625\n",
      "Valid auc & aupr: 0.9352612329173011 0.6579856376250297 ;   Test auc & aupr: 0.9294327351730202 0.6612049264694398\n",
      "step 1125 : Total_loss & DTIloss & L2_loss: 119052.2 , 528.63232421875 , 34778.69140625\n",
      "Valid auc & aupr: 0.9340110798611793 0.6596009068903194 ;   Test auc & aupr: 0.9305466639571232 0.6631017941497025\n",
      "step 1150 : Total_loss & DTIloss & L2_loss: 117661.45 , 522.2254638671875 , 34915.7265625\n",
      "Valid auc & aupr: 0.9342459216969552 0.6616416796725066 ;   Test auc & aupr: 0.9310806205048333 0.6645970072401985\n",
      "step 1175 : Total_loss & DTIloss & L2_loss: 115809.24 , 516.639404296875 , 35061.7421875\n",
      "Valid auc & aupr: 0.9339110676166895 0.6619184411843151 ;   Test auc & aupr: 0.9319087574353037 0.6655735578964486\n",
      "step 1200 : Total_loss & DTIloss & L2_loss: 114332.14 , 510.4660339355469 , 35190.9765625\n",
      "Valid auc & aupr: 0.9327518135397885 0.6639454909087015 ;   Test auc & aupr: 0.9320640179570769 0.6675362196354386\n",
      "step 1225 : Total_loss & DTIloss & L2_loss: 112962.47 , 505.7522277832031 , 35321.23046875\n",
      "Valid auc & aupr: 0.932294981909 0.6657968730033206 ;   Test auc & aupr: 0.932741064386034 0.6682991766021432\n",
      "step 1250 : Total_loss & DTIloss & L2_loss: 111658.93 , 500.52838134765625 , 35444.609375\n",
      "Valid auc & aupr: 0.9332595859866956 0.6645438038358078 ;   Test auc & aupr: 0.932741064386034 0.6682991766021432\n",
      "step 1275 : Total_loss & DTIloss & L2_loss: 110488.51 , 495.9228515625 , 35570.2265625\n",
      "Valid auc & aupr: 0.931523392116231 0.6675571778922091 ;   Test auc & aupr: 0.9333167696249516 0.6700338160480682\n",
      "step 1300 : Total_loss & DTIloss & L2_loss: 109406.84 , 490.7759704589844 , 35693.55078125\n",
      "Valid auc & aupr: 0.9318229615027634 0.6679425357746915 ;   Test auc & aupr: 0.9339125976360544 0.6706002249002475\n",
      "step 1325 : Total_loss & DTIloss & L2_loss: 108077.69 , 487.2444152832031 , 35818.66015625\n",
      "Valid auc & aupr: 0.9301704227339982 0.6712190958987934 ;   Test auc & aupr: 0.9340953027097714 0.6727141947489623\n",
      "step 1350 : Total_loss & DTIloss & L2_loss: 107008.64 , 482.9888916015625 , 35927.94140625\n",
      "Valid auc & aupr: 0.930026012530506 0.6716450649305502 ;   Test auc & aupr: 0.9333405290427121 0.6726484470774203\n",
      "step 1375 : Total_loss & DTIloss & L2_loss: 106146.64 , 479.4168701171875 , 36040.23046875\n",
      "Valid auc & aupr: 0.9298900145718775 0.6714824675339519 ;   Test auc & aupr: 0.9333405290427121 0.6726484470774203\n",
      "step 1400 : Total_loss & DTIloss & L2_loss: 105300.266 , 475.5576171875 , 36159.8359375\n",
      "Valid auc & aupr: 0.930516960487686 0.6720490765117828 ;   Test auc & aupr: 0.9335491270329274 0.6743848826772691\n",
      "step 1425 : Total_loss & DTIloss & L2_loss: 104309.05 , 471.91204833984375 , 36270.44140625\n",
      "Valid auc & aupr: 0.9322804941539572 0.6738593645933049 ;   Test auc & aupr: 0.9342147786390809 0.6760648258460148\n",
      "step 1450 : Total_loss & DTIloss & L2_loss: 103353.22 , 468.89630126953125 , 36374.0703125\n",
      "Valid auc & aupr: 0.9321690319095142 0.67470088231621 ;   Test auc & aupr: 0.9329374917357229 0.6757176184392558\n",
      "step 1475 : Total_loss & DTIloss & L2_loss: 102582.15 , 465.277099609375 , 36473.515625\n",
      "Valid auc & aupr: 0.9307824135478268 0.676749937370613 ;   Test auc & aupr: 0.9326889391327842 0.6769941327388881\n",
      "step 1500 : Total_loss & DTIloss & L2_loss: 101798.66 , 462.502197265625 , 36574.1171875\n",
      "Valid auc & aupr: 0.9309707543633847 0.6756294888114314 ;   Test auc & aupr: 0.9326889391327842 0.6769941327388881\n",
      "step 1525 : Total_loss & DTIloss & L2_loss: 101061.44 , 459.073486328125 , 36671.515625\n",
      "Valid auc & aupr: 0.9334161472105464 0.6778795955215967 ;   Test auc & aupr: 0.9321517823369677 0.6777992641268029\n",
      "step 1550 : Total_loss & DTIloss & L2_loss: 100317.766 , 455.9425354003906 , 36768.71484375\n",
      "Valid auc & aupr: 0.9339842074123094 0.6771563870876169 ;   Test auc & aupr: 0.9321517823369677 0.6777992641268029\n",
      "step 1575 : Total_loss & DTIloss & L2_loss: 99609.53 , 453.4212341308594 , 36861.47265625\n",
      "Valid auc & aupr: 0.9357192329154318 0.6774511342992929 ;   Test auc & aupr: 0.9321517823369677 0.6777992641268029\n",
      "step 1600 : Total_loss & DTIloss & L2_loss: 98886.5 , 450.55694580078125 , 36953.5625\n",
      "Valid auc & aupr: 0.9339699533307348 0.6802209949390383 ;   Test auc & aupr: 0.9313764010116468 0.6780970787237236\n",
      "step 1625 : Total_loss & DTIloss & L2_loss: 98369.97 , 447.82098388671875 , 37040.33203125\n",
      "Valid auc & aupr: 0.9337465614949119 0.6805565309541775 ;   Test auc & aupr: 0.93107902038078 0.6795870598223844\n",
      "step 1650 : Total_loss & DTIloss & L2_loss: 97748.72 , 445.4778747558594 , 37138.30078125\n",
      "Valid auc & aupr: 0.9343473359822556 0.6806523901915243 ;   Test auc & aupr: 0.9311550990062212 0.6793045869844396\n",
      "step 1675 : Total_loss & DTIloss & L2_loss: 97109.66 , 442.6676940917969 , 37226.73828125\n",
      "Valid auc & aupr: 0.9353815747535444 0.6816202504745599 ;   Test auc & aupr: 0.930473591625358 0.6799125225263242\n",
      "step 1700 : Total_loss & DTIloss & L2_loss: 96602.414 , 440.44097900390625 , 37312.859375\n",
      "Valid auc & aupr: 0.9361265257709123 0.6817821388425427 ;   Test auc & aupr: 0.9305885581141539 0.6798687834904065\n",
      "step 1725 : Total_loss & DTIloss & L2_loss: 96135.74 , 438.07598876953125 , 37395.31640625\n",
      "Valid auc & aupr: 0.9359423910777864 0.6825746361241375 ;   Test auc & aupr: 0.9304003738277696 0.680047532657433\n",
      "step 1750 : Total_loss & DTIloss & L2_loss: 95547.18 , 435.9657897949219 , 37476.51953125\n",
      "Valid auc & aupr: 0.9366057900546703 0.6832466907561966 ;   Test auc & aupr: 0.9303333625719638 0.6808855853906196\n",
      "step 1775 : Total_loss & DTIloss & L2_loss: 95386.03 , 433.52557373046875 , 37560.59375\n",
      "Valid auc & aupr: 0.9373631257658648 0.6836116896854113 ;   Test auc & aupr: 0.9305308566710214 0.6808108302118399\n",
      "step 1800 : Total_loss & DTIloss & L2_loss: 94532.62 , 431.76129150390625 , 37650.4140625\n",
      "Valid auc & aupr: 0.9397659900417715 0.6832918830717071 ;   Test auc & aupr: 0.9305308566710214 0.6808108302118399\n",
      "step 1825 : Total_loss & DTIloss & L2_loss: 94011.734 , 429.3106689453125 , 37722.2890625\n",
      "Valid auc & aupr: 0.9372166125011566 0.683782428546712 ;   Test auc & aupr: 0.9306065473876011 0.6816914157459777\n",
      "step 1850 : Total_loss & DTIloss & L2_loss: 93493.17 , 427.976806640625 , 37797.3046875\n",
      "Valid auc & aupr: 0.9378788431106987 0.6828673000639592 ;   Test auc & aupr: 0.9306065473876011 0.6816914157459777\n",
      "step 1875 : Total_loss & DTIloss & L2_loss: 93480.516 , 425.35870361328125 , 37867.453125\n",
      "Valid auc & aupr: 0.9366959880134857 0.6830409905050344 ;   Test auc & aupr: 0.9306065473876011 0.6816914157459777\n",
      "step 1900 : Total_loss & DTIloss & L2_loss: 92862.25 , 423.7048645019531 , 37951.29296875\n",
      "Valid auc & aupr: 0.9384597553532255 0.6840354934965971 ;   Test auc & aupr: 0.9312571190367679 0.6822636999064354\n",
      "step 1925 : Total_loss & DTIloss & L2_loss: 92276.625 , 421.89984130859375 , 38024.32421875\n",
      "Valid auc & aupr: 0.938560702291589 0.683491612856731 ;   Test auc & aupr: 0.9312571190367679 0.6822636999064354\n",
      "step 1950 : Total_loss & DTIloss & L2_loss: 91891.18 , 420.52349853515625 , 38094.8984375\n",
      "Valid auc & aupr: 0.9394250604513263 0.6833344090363637 ;   Test auc & aupr: 0.9312571190367679 0.6822636999064354\n",
      "step 1975 : Total_loss & DTIloss & L2_loss: 91458.36 , 418.7044372558594 , 38162.375\n",
      "Valid auc & aupr: 0.9378173869885004 0.6842406989352765 ;   Test auc & aupr: 0.9301465359665945 0.6830884683797311\n",
      "step 2000 : Total_loss & DTIloss & L2_loss: 90991.516 , 417.21942138671875 , 38229.26171875\n",
      "Valid auc & aupr: 0.9383426849455401 0.6836247237385782 ;   Test auc & aupr: 0.9301465359665945 0.6830884683797311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2025 : Total_loss & DTIloss & L2_loss: 91156.86 , 415.5186462402344 , 38301.546875\n",
      "Valid auc & aupr: 0.939238121676579 0.684144544217645 ;   Test auc & aupr: 0.9301465359665945 0.6830884683797311\n",
      "step 2050 : Total_loss & DTIloss & L2_loss: 90424.42 , 413.8144226074219 , 38376.78515625\n",
      "Valid auc & aupr: 0.9419751390123464 0.6837954295633011 ;   Test auc & aupr: 0.9301465359665945 0.6830884683797311\n",
      "step 2075 : Total_loss & DTIloss & L2_loss: 90004.84 , 412.0242919921875 , 38435.7421875\n",
      "Valid auc & aupr: 0.9411568145258906 0.6830021367506904 ;   Test auc & aupr: 0.9301465359665945 0.6830884683797311\n",
      "step 2100 : Total_loss & DTIloss & L2_loss: 89615.766 , 410.7459716796875 , 38497.94140625\n",
      "Valid auc & aupr: 0.9436641308421871 0.6851862788036102 ;   Test auc & aupr: 0.9288817106354088 0.6839140574122009\n",
      "step 2125 : Total_loss & DTIloss & L2_loss: 89806.125 , 408.7608642578125 , 38562.25390625\n",
      "Valid auc & aupr: 0.9445906461445279 0.6839000416981647 ;   Test auc & aupr: 0.9288817106354088 0.6839140574122009\n",
      "step 2150 : Total_loss & DTIloss & L2_loss: 89132.44 , 408.0631103515625 , 38635.85546875\n",
      "Valid auc & aupr: 0.946082183893542 0.6844848597361247 ;   Test auc & aupr: 0.9288817106354088 0.6839140574122009\n",
      "step 2175 : Total_loss & DTIloss & L2_loss: 88686.01 , 406.3900146484375 , 38695.5390625\n",
      "Valid auc & aupr: 0.9433119849252571 0.6838697377098356 ;   Test auc & aupr: 0.9288817106354088 0.6839140574122009\n",
      "step 2200 : Total_loss & DTIloss & L2_loss: 88308.41 , 405.17034912109375 , 38754.13671875\n",
      "Valid auc & aupr: 0.9440293624733496 0.6840087562520846 ;   Test auc & aupr: 0.9288817106354088 0.6839140574122009\n",
      "step 2225 : Total_loss & DTIloss & L2_loss: 87993.42 , 403.8810729980469 , 38807.90625\n",
      "Valid auc & aupr: 0.9449177890003355 0.6842572933045021 ;   Test auc & aupr: 0.9288817106354088 0.6839140574122009\n",
      "step 2250 : Total_loss & DTIloss & L2_loss: 87993.83 , 402.3058776855469 , 38870.31640625\n",
      "Valid auc & aupr: 0.9447873992049494 0.6840585625668208 ;   Test auc & aupr: 0.9288817106354088 0.6839140574122009\n",
      "step 2275 : Total_loss & DTIloss & L2_loss: 87437.64 , 400.9845275878906 , 38925.40234375\n",
      "Early Stopping\n",
      "len(fpr): 112\n",
      "len(recall): 48163\n",
      "Time spent in this fold: 1462.1446042060852\n",
      "--------------------------------------------------------------\n",
      "round  1  of  1 : KFold  4  of 10\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:00<00:00, 788.52it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_Di_emb = th.tensor(Drug_Di_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:01<00:00, 705.18it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_PDi_emb = th.tensor(Drug_PDi_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:01<00:00, 621.94it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_D2_emb = th.tensor(Drug_D2_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:01<00:00, 677.51it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:190: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_D4_emb = th.tensor(Drug_D4_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:01<00:00, 824.43it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:230: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_Di_emb = th.tensor(Protein_Di_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:01<00:00, 772.23it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_DiDP_emb = th.tensor(Protein_DiDP_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:01<00:00, 778.18it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:265: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_DDiP_emb = th.tensor(Protein_DDiP_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:02<00:00, 697.71it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:303: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_P2_emb = th.tensor(Protein_P2_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:06<00:00, 892.03it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:358: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_P_emb = th.tensor(Disease_P_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:07<00:00, 796.73it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:393: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_DPDi_emb = th.tensor(Disease_DPDi_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:07<00:00, 717.24it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:430: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_Di2_emb = th.tensor(Disease_Di2_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:08<00:00, 693.11it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:448: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_Di3_emb = th.tensor(Disease_Di3_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:04<00:00, 841.12it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:470: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_D_emb = th.tensor(Sideeffect_D_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:06<00:00, 697.97it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:490: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_Si1_emb = th.tensor(Sideeffect_Si1_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:05<00:00, 712.02it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:508: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_Si2_emb = th.tensor(Sideeffect_Si2_emb).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : Total_loss & DTIloss & L2_loss: 2063226.5 , 1665.590576171875 , 120177.0625\n",
      "Valid auc & aupr: 0.4981919146237499 0.014894108467646017 ;   Test auc & aupr: 0.4988419941651538 0.0018198227697487392\n",
      "step 25 : Total_loss & DTIloss & L2_loss: 1145675.6 , 3629.94189453125 , 117232.0625\n",
      "Valid auc & aupr: 0.7462467250951281 0.007571093508556738 ;   Test auc & aupr: 0.4988419941651538 0.0018198227697487392\n",
      "step 50 : Total_loss & DTIloss & L2_loss: 853887.0 , 2660.3154296875 , 108804.921875\n",
      "Valid auc & aupr: 0.8681169816813259 0.032909033138328846 ;   Test auc & aupr: 0.8764545686019453 0.030987773898313982\n",
      "step 75 : Total_loss & DTIloss & L2_loss: 787191.75 , 3745.8662109375 , 97521.234375\n",
      "Valid auc & aupr: 0.7344166510718816 0.010488804480466483 ;   Test auc & aupr: 0.8764545686019453 0.030987773898313982\n",
      "step 100 : Total_loss & DTIloss & L2_loss: 764816.94 , 4321.70068359375 , 87375.625\n",
      "Valid auc & aupr: 0.7145570821117418 0.01064895658745281 ;   Test auc & aupr: 0.8764545686019453 0.030987773898313982\n",
      "step 125 : Total_loss & DTIloss & L2_loss: 727972.6 , 3413.892578125 , 79106.28125\n",
      "Valid auc & aupr: 0.7881048177489448 0.01865321333963439 ;   Test auc & aupr: 0.8764545686019453 0.030987773898313982\n",
      "step 150 : Total_loss & DTIloss & L2_loss: 697915.25 , 1650.474365234375 , 72568.796875\n",
      "Valid auc & aupr: 0.8904105067265506 0.045469151206507914 ;   Test auc & aupr: 0.9258881870878487 0.046196229794604464\n",
      "step 175 : Total_loss & DTIloss & L2_loss: 678129.3 , 1615.2806396484375 , 66200.671875\n",
      "Valid auc & aupr: 0.9018230303786414 0.05592947165146707 ;   Test auc & aupr: 0.9327526561729896 0.06321145563431342\n",
      "step 200 : Total_loss & DTIloss & L2_loss: 656718.25 , 1604.1953125 , 60511.18359375\n",
      "Valid auc & aupr: 0.9064175139833239 0.07551767502053178 ;   Test auc & aupr: 0.9320289543132005 0.09802056415192617\n",
      "step 225 : Total_loss & DTIloss & L2_loss: 640534.5 , 1593.9090576171875 , 55744.1015625\n",
      "Valid auc & aupr: 0.9112734701515814 0.10000965847032998 ;   Test auc & aupr: 0.9320907575162054 0.13940343160584306\n",
      "step 250 : Total_loss & DTIloss & L2_loss: 620999.6 , 1580.9210205078125 , 51620.5625\n",
      "Valid auc & aupr: 0.9162916120849187 0.122607232960376 ;   Test auc & aupr: 0.9349843464379519 0.162945610185205\n",
      "step 275 : Total_loss & DTIloss & L2_loss: 601935.1 , 1561.3720703125 , 47949.25\n",
      "Valid auc & aupr: 0.9305587611502714 0.1540588153650767 ;   Test auc & aupr: 0.9393987425680187 0.18835513795570133\n",
      "step 300 : Total_loss & DTIloss & L2_loss: 577165.6 , 1534.3804931640625 , 44823.3046875\n",
      "Valid auc & aupr: 0.9422657663277401 0.20668969116256614 ;   Test auc & aupr: 0.9450536869022441 0.24929024461943272\n",
      "step 325 : Total_loss & DTIloss & L2_loss: 553795.75 , 1496.845947265625 , 41998.71484375\n",
      "Valid auc & aupr: 0.9395327802382883 0.2814955053168472 ;   Test auc & aupr: 0.9506113448065034 0.31890692744553073\n",
      "step 350 : Total_loss & DTIloss & L2_loss: 528091.6 , 1447.461669921875 , 39655.4296875\n",
      "Valid auc & aupr: 0.943000540619217 0.3470329414199615 ;   Test auc & aupr: 0.9544919815705577 0.3709028079121226\n",
      "step 375 : Total_loss & DTIloss & L2_loss: 500443.12 , 1373.101806640625 , 37657.15625\n",
      "Valid auc & aupr: 0.9463762917680328 0.38407001345264113 ;   Test auc & aupr: 0.9486644464304654 0.4165872276254145\n",
      "step 400 : Total_loss & DTIloss & L2_loss: 440867.0 , 1270.0146484375 , 36281.62890625\n",
      "Valid auc & aupr: 0.9558072380595929 0.4050012542296539 ;   Test auc & aupr: 0.9498048325035718 0.45219533304310167\n",
      "step 425 : Total_loss & DTIloss & L2_loss: 387685.12 , 1169.212890625 , 34967.84375\n",
      "Valid auc & aupr: 0.9670817478635144 0.4485311987652983 ;   Test auc & aupr: 0.9471309666410876 0.4906729565148352\n",
      "step 450 : Total_loss & DTIloss & L2_loss: 349023.88 , 1101.1871337890625 , 34248.59375\n",
      "Valid auc & aupr: 0.9694984197284429 0.48071638648795173 ;   Test auc & aupr: 0.9489175081260488 0.5144239542379856\n",
      "step 475 : Total_loss & DTIloss & L2_loss: 322024.44 , 1054.8326416015625 , 33549.2265625\n",
      "Valid auc & aupr: 0.9688850248476909 0.49528304531794964 ;   Test auc & aupr: 0.9469919581750236 0.5272639831288906\n",
      "step 500 : Total_loss & DTIloss & L2_loss: 296936.84 , 1011.5482788085938 , 33051.26171875\n",
      "Valid auc & aupr: 0.9683402470213962 0.5057560521727998 ;   Test auc & aupr: 0.9453047502292762 0.5411261174570925\n",
      "step 525 : Total_loss & DTIloss & L2_loss: 274328.34 , 972.0460205078125 , 32712.380859375\n",
      "Valid auc & aupr: 0.9677497244921299 0.5133251034532054 ;   Test auc & aupr: 0.9426159697137011 0.5506678073575834\n",
      "step 550 : Total_loss & DTIloss & L2_loss: 259414.98 , 935.4283447265625 , 32552.08984375\n",
      "Valid auc & aupr: 0.96585235897116 0.5168117081194683 ;   Test auc & aupr: 0.9402265058145702 0.5550092625689715\n",
      "step 575 : Total_loss & DTIloss & L2_loss: 234415.4 , 884.5807495117188 , 32585.130859375\n",
      "Valid auc & aupr: 0.9680171750566611 0.5382720322241449 ;   Test auc & aupr: 0.9350139807813485 0.5720140295956202\n",
      "step 600 : Total_loss & DTIloss & L2_loss: 216924.4 , 836.2393798828125 , 32535.892578125\n",
      "Valid auc & aupr: 0.97233485122575 0.5508373757930798 ;   Test auc & aupr: 0.9365079318659654 0.5850239615474985\n",
      "step 625 : Total_loss & DTIloss & L2_loss: 206199.72 , 811.4291381835938 , 32512.212890625\n",
      "Valid auc & aupr: 0.9734870459318404 0.5565145389201499 ;   Test auc & aupr: 0.9377240609812398 0.5907776246274385\n",
      "step 650 : Total_loss & DTIloss & L2_loss: 199480.17 , 788.7069091796875 , 32391.35546875\n",
      "Valid auc & aupr: 0.975484218077475 0.5633645298477264 ;   Test auc & aupr: 0.9378672124064957 0.5926805143229538\n",
      "step 675 : Total_loss & DTIloss & L2_loss: 191775.94 , 768.3037719726562 , 32354.5703125\n",
      "Valid auc & aupr: 0.9749617927764956 0.5678203911087931 ;   Test auc & aupr: 0.9385044970116104 0.5971443180733983\n",
      "step 700 : Total_loss & DTIloss & L2_loss: 184590.84 , 750.62060546875 , 32369.630859375\n",
      "Valid auc & aupr: 0.9736476722184102 0.5712947000841531 ;   Test auc & aupr: 0.9379554818076327 0.5999546365820131\n",
      "step 725 : Total_loss & DTIloss & L2_loss: 178281.77 , 732.6160888671875 , 32416.14453125\n",
      "Valid auc & aupr: 0.9723148379182002 0.5727219399526471 ;   Test auc & aupr: 0.9392207902855504 0.6025934013969357\n",
      "step 750 : Total_loss & DTIloss & L2_loss: 173288.64 , 716.0631713867188 , 32477.953125\n",
      "Valid auc & aupr: 0.9702620443723619 0.5759571631106895 ;   Test auc & aupr: 0.9397819906636221 0.605565614717498\n",
      "step 775 : Total_loss & DTIloss & L2_loss: 167815.23 , 698.2578125 , 32644.66796875\n",
      "Valid auc & aupr: 0.9675290582829101 0.5811764502576752 ;   Test auc & aupr: 0.9392691897970515 0.609006226318771\n",
      "step 800 : Total_loss & DTIloss & L2_loss: 160368.0 , 676.5968017578125 , 32880.15234375\n",
      "Valid auc & aupr: 0.9624927224336182 0.5834593979403999 ;   Test auc & aupr: 0.9422355485784872 0.6120641113737652\n",
      "step 825 : Total_loss & DTIloss & L2_loss: 154177.77 , 655.8739624023438 , 33024.33984375\n",
      "Valid auc & aupr: 0.959434844987836 0.5871220669572287 ;   Test auc & aupr: 0.9421102849888013 0.615269577567126\n",
      "step 850 : Total_loss & DTIloss & L2_loss: 148931.94 , 637.6854248046875 , 33158.30078125\n",
      "Valid auc & aupr: 0.9571065435718296 0.5910703159524066 ;   Test auc & aupr: 0.9413289228851996 0.6183882405033575\n",
      "step 875 : Total_loss & DTIloss & L2_loss: 144221.83 , 622.0625 , 33300.38671875\n",
      "Valid auc & aupr: 0.9559436924292517 0.5946097335622934 ;   Test auc & aupr: 0.9426320054028088 0.6209488346133226\n",
      "step 900 : Total_loss & DTIloss & L2_loss: 140502.94 , 608.539306640625 , 33462.98828125\n",
      "Valid auc & aupr: 0.9538279999168279 0.5956269794870829 ;   Test auc & aupr: 0.9414381020450817 0.6239592389935796\n",
      "step 925 : Total_loss & DTIloss & L2_loss: 136773.75 , 595.2850341796875 , 33614.3828125\n",
      "Valid auc & aupr: 0.952596791632878 0.5983596097278477 ;   Test auc & aupr: 0.9412866647014417 0.6256527619498942\n",
      "step 950 : Total_loss & DTIloss & L2_loss: 133744.95 , 584.3394165039062 , 33756.80078125\n",
      "Valid auc & aupr: 0.9513416713451023 0.6000607833052769 ;   Test auc & aupr: 0.943135837981246 0.6270575209296357\n",
      "step 975 : Total_loss & DTIloss & L2_loss: 133065.84 , 573.7490844726562 , 33966.234375\n",
      "Valid auc & aupr: 0.9504699228577964 0.6023667479638924 ;   Test auc & aupr: 0.9443526007255734 0.6281562024560138\n",
      "step 1000 : Total_loss & DTIloss & L2_loss: 128838.055 , 562.084716796875 , 34244.0859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid auc & aupr: 0.9510763000852515 0.6030679014511515 ;   Test auc & aupr: 0.9453007534921735 0.6290477329946483\n",
      "step 1025 : Total_loss & DTIloss & L2_loss: 125885.14 , 555.5535888671875 , 34388.42578125\n",
      "Valid auc & aupr: 0.9499802466055352 0.6064079560296294 ;   Test auc & aupr: 0.9442123250014037 0.6297893266078727\n",
      "step 1050 : Total_loss & DTIloss & L2_loss: 123617.96 , 547.3773193359375 , 34503.7265625\n",
      "Valid auc & aupr: 0.9501879171605017 0.610184831437998 ;   Test auc & aupr: 0.942691810237262 0.6307927059132348\n",
      "step 1075 : Total_loss & DTIloss & L2_loss: 121594.14 , 539.922119140625 , 34629.9375\n",
      "Valid auc & aupr: 0.9494853720915726 0.612253865584599 ;   Test auc & aupr: 0.9436285089402134 0.6318128538030259\n",
      "step 1100 : Total_loss & DTIloss & L2_loss: 119648.34 , 532.4805297851562 , 34756.1171875\n",
      "Valid auc & aupr: 0.9500813527956251 0.613532974879713 ;   Test auc & aupr: 0.9444177182959317 0.632006108076364\n",
      "step 1125 : Total_loss & DTIloss & L2_loss: 117901.57 , 526.2767944335938 , 34882.828125\n",
      "Valid auc & aupr: 0.9502235252531553 0.6149684884039932 ;   Test auc & aupr: 0.9433833919781393 0.6330718199573888\n",
      "step 1150 : Total_loss & DTIloss & L2_loss: 116267.86 , 519.9312744140625 , 35007.4609375\n",
      "Valid auc & aupr: 0.9504717422493918 0.6147493395577763 ;   Test auc & aupr: 0.9433833919781393 0.6330718199573888\n",
      "step 1175 : Total_loss & DTIloss & L2_loss: 114797.9 , 514.3639526367188 , 35133.6796875\n",
      "Valid auc & aupr: 0.950726976898925 0.6166507583332798 ;   Test auc & aupr: 0.9424479605332935 0.6342907670191179\n",
      "step 1200 : Total_loss & DTIloss & L2_loss: 113430.984 , 508.7008972167969 , 35252.49609375\n",
      "Valid auc & aupr: 0.9504205393716341 0.6167760261218077 ;   Test auc & aupr: 0.9425868715179646 0.6356534412052236\n",
      "step 1225 : Total_loss & DTIloss & L2_loss: 112353.516 , 503.6827087402344 , 35381.0859375\n",
      "Valid auc & aupr: 0.950020273220635 0.6163172866853018 ;   Test auc & aupr: 0.9425868715179646 0.6356534412052236\n",
      "step 1250 : Total_loss & DTIloss & L2_loss: 110915.09 , 498.86334228515625 , 35497.43359375\n",
      "Valid auc & aupr: 0.9501879171605015 0.6156067782761963 ;   Test auc & aupr: 0.9425868715179646 0.6356534412052236\n",
      "step 1275 : Total_loss & DTIloss & L2_loss: 109650.875 , 494.6692810058594 , 35610.203125\n",
      "Valid auc & aupr: 0.9494580812176409 0.6174387775063117 ;   Test auc & aupr: 0.9409624415891495 0.6371288064600586\n",
      "step 1300 : Total_loss & DTIloss & L2_loss: 108680.42 , 489.68658447265625 , 35718.421875\n",
      "Valid auc & aupr: 0.9496291040276131 0.6178790067390124 ;   Test auc & aupr: 0.9404893156494288 0.6378278167805396\n",
      "step 1325 : Total_loss & DTIloss & L2_loss: 107652.81 , 486.05279541015625 , 35830.48828125\n",
      "Valid auc & aupr: 0.9496283242883581 0.6169421288791557 ;   Test auc & aupr: 0.9404893156494288 0.6378278167805396\n",
      "step 1350 : Total_loss & DTIloss & L2_loss: 106850.82 , 481.7712707519531 , 35950.625\n",
      "Valid auc & aupr: 0.9497502235252533 0.617502079777569 ;   Test auc & aupr: 0.9404893156494288 0.6378278167805396\n",
      "step 1375 : Total_loss & DTIloss & L2_loss: 105603.3 , 478.09967041015625 , 36069.80859375\n",
      "Valid auc & aupr: 0.9494206537333917 0.6171123685442181 ;   Test auc & aupr: 0.9404893156494288 0.6378278167805396\n",
      "step 1400 : Total_loss & DTIloss & L2_loss: 104745.51 , 474.1296081542969 , 36172.6484375\n",
      "Valid auc & aupr: 0.9491350092529058 0.618172390691151 ;   Test auc & aupr: 0.9409712149144971 0.6381969786467057\n",
      "step 1425 : Total_loss & DTIloss & L2_loss: 103897.06 , 470.4251708984375 , 36279.37890625\n",
      "Valid auc & aupr: 0.9491121369014202 0.6178229909165486 ;   Test auc & aupr: 0.9409712149144971 0.6381969786467057\n",
      "step 1450 : Total_loss & DTIloss & L2_loss: 103082.72 , 467.07537841796875 , 36390.78125\n",
      "Valid auc & aupr: 0.9493042126712827 0.6183197767635042 ;   Test auc & aupr: 0.9405635964707058 0.6382919349244485\n",
      "step 1475 : Total_loss & DTIloss & L2_loss: 102317.54 , 464.04345703125 , 36487.4921875\n",
      "Valid auc & aupr: 0.9490799076788722 0.619245217431045 ;   Test auc & aupr: 0.940037806208551 0.639026484632203\n",
      "step 1500 : Total_loss & DTIloss & L2_loss: 101545.484 , 460.7679443359375 , 36581.21875\n",
      "Valid auc & aupr: 0.9482273927598611 0.6188693475994096 ;   Test auc & aupr: 0.940037806208551 0.639026484632203\n",
      "step 1525 : Total_loss & DTIloss & L2_loss: 100701.06 , 457.8183288574219 , 36674.25390625\n",
      "Valid auc & aupr: 0.9483342170378226 0.6181019571415924 ;   Test auc & aupr: 0.940037806208551 0.639026484632203\n",
      "step 1550 : Total_loss & DTIloss & L2_loss: 100097.83 , 454.52447509765625 , 36763.6796875\n",
      "Valid auc & aupr: 0.9478198490424802 0.6197567450842214 ;   Test auc & aupr: 0.9397719013394724 0.6386036211987152\n",
      "step 1575 : Total_loss & DTIloss & L2_loss: 99432.62 , 452.3464050292969 , 36857.38671875\n",
      "Valid auc & aupr: 0.9467341920861664 0.619481391055424 ;   Test auc & aupr: 0.9397719013394724 0.6386036211987152\n",
      "step 1600 : Total_loss & DTIloss & L2_loss: 98759.445 , 448.8980712890625 , 36941.5390625\n",
      "Valid auc & aupr: 0.9475531782172042 0.6203241490020445 ;   Test auc & aupr: 0.9393920650926152 0.6389328891148872\n",
      "step 1625 : Total_loss & DTIloss & L2_loss: 98153.664 , 446.70819091796875 , 37025.44921875\n",
      "Valid auc & aupr: 0.9465974778034225 0.6199128834203811 ;   Test auc & aupr: 0.9393920650926152 0.6389328891148872\n",
      "step 1650 : Total_loss & DTIloss & L2_loss: 97831.33 , 443.5598449707031 , 37106.9765625\n",
      "Valid auc & aupr: 0.9461496475578565 0.6210387605568122 ;   Test auc & aupr: 0.9394153631454828 0.6391803094824509\n",
      "step 1675 : Total_loss & DTIloss & L2_loss: 97775.125 , 441.50164794921875 , 37202.48046875\n",
      "Valid auc & aupr: 0.9463890275092008 0.6211084868575735 ;   Test auc & aupr: 0.9403207703213611 0.6398852573762054\n",
      "step 1700 : Total_loss & DTIloss & L2_loss: 96580.34 , 438.97900390625 , 37296.4375\n",
      "Valid auc & aupr: 0.9452851766369327 0.6203038937675257 ;   Test auc & aupr: 0.9403207703213611 0.6398852573762054\n",
      "step 1725 : Total_loss & DTIloss & L2_loss: 95890.44 , 436.6768493652344 , 37375.24609375\n",
      "Valid auc & aupr: 0.9449956334601709 0.6218385138544227 ;   Test auc & aupr: 0.9395898060978121 0.6389691955980957\n",
      "step 1750 : Total_loss & DTIloss & L2_loss: 95785.516 , 434.27667236328125 , 37454.15234375\n",
      "Valid auc & aupr: 0.944576133740877 0.6220096207991906 ;   Test auc & aupr: 0.9404885357982868 0.6393881155962631\n",
      "step 1775 : Total_loss & DTIloss & L2_loss: 95176.03 , 432.0285339355469 , 37544.08984375\n",
      "Valid auc & aupr: 0.9442769737799679 0.622264905297196 ;   Test auc & aupr: 0.94066010304953 0.6395785319354049\n",
      "step 1800 : Total_loss & DTIloss & L2_loss: 94510.766 , 429.5693054199219 , 37617.453125\n",
      "Valid auc & aupr: 0.9442564406462479 0.6216718970099195 ;   Test auc & aupr: 0.94066010304953 0.6395785319354049\n",
      "step 1825 : Total_loss & DTIloss & L2_loss: 93962.86 , 427.93011474609375 , 37690.76953125\n",
      "Valid auc & aupr: 0.9434790406088204 0.6224745171798611 ;   Test auc & aupr: 0.9415392877307579 0.6394838942388453\n",
      "step 1850 : Total_loss & DTIloss & L2_loss: 93645.47 , 425.59326171875 , 37762.07421875\n",
      "Valid auc & aupr: 0.9438028923128104 0.6215772123873958 ;   Test auc & aupr: 0.9415392877307579 0.6394838942388453\n",
      "step 1875 : Total_loss & DTIloss & L2_loss: 93003.78 , 424.2051086425781 , 37834.44140625\n",
      "Valid auc & aupr: 0.9425545297652467 0.6211138995529603 ;   Test auc & aupr: 0.9415392877307579 0.6394838942388453\n",
      "step 1900 : Total_loss & DTIloss & L2_loss: 92605.664 , 421.943359375 , 37901.8984375\n",
      "Valid auc & aupr: 0.9419263198386459 0.6226769914592585 ;   Test auc & aupr: 0.9409394359804601 0.6401363287144646\n",
      "step 1925 : Total_loss & DTIloss & L2_loss: 92348.36 , 420.22125244140625 , 37974.86328125\n",
      "Valid auc & aupr: 0.9416791424947497 0.6233854640375455 ;   Test auc & aupr: 0.9409762839469202 0.6405253717559584\n",
      "step 1950 : Total_loss & DTIloss & L2_loss: 92226.92 , 417.8193359375 , 38043.88671875\n",
      "Valid auc & aupr: 0.9410020689081572 0.6232453154890094 ;   Test auc & aupr: 0.9409762839469202 0.6405253717559584\n",
      "step 1975 : Total_loss & DTIloss & L2_loss: 91579.33 , 416.93475341796875 , 38118.53515625\n",
      "Valid auc & aupr: 0.9402537791362569 0.6253839793754279 ;   Test auc & aupr: 0.941973177409896 0.6413531134679872\n",
      "step 2000 : Total_loss & DTIloss & L2_loss: 91125.3 , 414.7468566894531 , 38181.4375\n",
      "Valid auc & aupr: 0.9404429958621838 0.6242948059131644 ;   Test auc & aupr: 0.941973177409896 0.6413531134679872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2025 : Total_loss & DTIloss & L2_loss: 90722.6 , 413.31298828125 , 38243.51953125\n",
      "Valid auc & aupr: 0.939729014617512 0.6245010432052464 ;   Test auc & aupr: 0.941973177409896 0.6413531134679872\n",
      "step 2050 : Total_loss & DTIloss & L2_loss: 90887.82 , 411.367431640625 , 38304.8046875\n",
      "Valid auc & aupr: 0.9392835235897115 0.6248338827926528 ;   Test auc & aupr: 0.941973177409896 0.6413531134679872\n",
      "step 2075 : Total_loss & DTIloss & L2_loss: 90212.77 , 410.41314697265625 , 38386.44140625\n",
      "Valid auc & aupr: 0.9392934002869441 0.6251386995534426 ;   Test auc & aupr: 0.941973177409896 0.6413531134679872\n",
      "step 2100 : Total_loss & DTIloss & L2_loss: 89699.31 , 408.59014892578125 , 38449.2890625\n",
      "Valid auc & aupr: 0.9392273823633378 0.6256384775305369 ;   Test auc & aupr: 0.9421394319252341 0.641536955755724\n",
      "step 2125 : Total_loss & DTIloss & L2_loss: 89361.016 , 407.2244873046875 , 38509.7890625\n",
      "Valid auc & aupr: 0.938432308236126 0.6259351443336132 ;   Test auc & aupr: 0.9432549602431888 0.6423135951382798\n",
      "step 2150 : Total_loss & DTIloss & L2_loss: 89072.484 , 405.7802734375 , 38565.94921875\n",
      "Valid auc & aupr: 0.9381165138377726 0.6258562778395184 ;   Test auc & aupr: 0.9432549602431888 0.6423135951382798\n",
      "step 2175 : Total_loss & DTIloss & L2_loss: 88670.31 , 404.2755126953125 , 38622.6015625\n",
      "Valid auc & aupr: 0.9377159877736886 0.6258378854346001 ;   Test auc & aupr: 0.9432549602431888 0.6423135951382798\n",
      "step 2200 : Total_loss & DTIloss & L2_loss: 88557.6 , 402.5472717285156 , 38687.2734375\n",
      "Valid auc & aupr: 0.9372140956064294 0.6259753395175736 ;   Test auc & aupr: 0.9436557549894876 0.6436084250959107\n",
      "step 2225 : Total_loss & DTIloss & L2_loss: 88040.266 , 401.50457763671875 , 38742.796875\n",
      "Valid auc & aupr: 0.9373892770257626 0.6265088181529134 ;   Test auc & aupr: 0.9435940005271792 0.6433866755471312\n",
      "step 2250 : Total_loss & DTIloss & L2_loss: 87912.61 , 400.4220275878906 , 38794.3359375\n",
      "Valid auc & aupr: 0.9373726425883184 0.6273371809970573 ;   Test auc & aupr: 0.9446370026889268 0.6443081717901652\n",
      "step 2275 : Total_loss & DTIloss & L2_loss: 87658.2 , 399.32861328125 , 38848.62890625\n",
      "Valid auc & aupr: 0.9372042189091968 0.6264448930830847 ;   Test auc & aupr: 0.9446370026889268 0.6443081717901652\n",
      "step 2300 : Total_loss & DTIloss & L2_loss: 87326.58 , 397.6469421386719 , 38902.875\n",
      "Valid auc & aupr: 0.93713664150708 0.6268379724545846 ;   Test auc & aupr: 0.9446370026889268 0.6443081717901652\n",
      "step 2325 : Total_loss & DTIloss & L2_loss: 86958.51 , 396.64739990234375 , 38955.9375\n",
      "Valid auc & aupr: 0.9369198739941363 0.6274484003969574 ;   Test auc & aupr: 0.9456144973547449 0.643903866046478\n",
      "step 2350 : Total_loss & DTIloss & L2_loss: 86701.125 , 395.47918701171875 , 39002.046875\n",
      "Valid auc & aupr: 0.9372512631775933 0.6282722424629315 ;   Test auc & aupr: 0.9471709340121157 0.6443217228834147\n",
      "step 2375 : Total_loss & DTIloss & L2_loss: 86739.82 , 394.22235107421875 , 39064.8515625\n",
      "Valid auc & aupr: 0.9371740689913294 0.6286829668288652 ;   Test auc & aupr: 0.9470635095173033 0.644297500691731\n",
      "step 2400 : Total_loss & DTIloss & L2_loss: 86325.0 , 392.8495178222656 , 39121.7734375\n",
      "Valid auc & aupr: 0.9369669182625331 0.6290922673632169 ;   Test auc & aupr: 0.9470284649566091 0.6452722306324749\n",
      "step 2425 : Total_loss & DTIloss & L2_loss: 85973.62 , 392.1540832519531 , 39169.08984375\n",
      "Valid auc & aupr: 0.9366674983885388 0.6288122450370288 ;   Test auc & aupr: 0.9470284649566091 0.6452722306324749\n",
      "step 2450 : Total_loss & DTIloss & L2_loss: 85693.28 , 391.2740173339844 , 39210.34375\n",
      "Valid auc & aupr: 0.9367984945834115 0.6294130336136898 ;   Test auc & aupr: 0.949149903766369 0.6451972609848962\n",
      "step 2475 : Total_loss & DTIloss & L2_loss: 85899.83 , 389.8893737792969 , 39261.60546875\n",
      "Valid auc & aupr: 0.9365523568918552 0.6295047965573038 ;   Test auc & aupr: 0.9480753663740665 0.6446702563960192\n",
      "step 2500 : Total_loss & DTIloss & L2_loss: 85444.92 , 388.64642333984375 , 39315.1328125\n",
      "Valid auc & aupr: 0.9363808142557128 0.6300446452027407 ;   Test auc & aupr: 0.949422851666074 0.645086642606823\n",
      "step 2525 : Total_loss & DTIloss & L2_loss: 85073.84 , 388.0526123046875 , 39362.02734375\n",
      "Valid auc & aupr: 0.9363428669452936 0.6305285593419638 ;   Test auc & aupr: 0.9475398036022884 0.6454153903270546\n",
      "step 2550 : Total_loss & DTIloss & L2_loss: 84885.58 , 386.6912536621094 , 39403.48046875\n",
      "Valid auc & aupr: 0.9362994614600878 0.6304843651178312 ;   Test auc & aupr: 0.9475398036022884 0.6454153903270546\n",
      "step 2575 : Total_loss & DTIloss & L2_loss: 84725.484 , 386.06488037109375 , 39449.80078125\n",
      "Valid auc & aupr: 0.9357250535420955 0.6300729327852258 ;   Test auc & aupr: 0.9475398036022884 0.6454153903270546\n",
      "step 2600 : Total_loss & DTIloss & L2_loss: 84772.58 , 384.5930480957031 , 39491.58984375\n",
      "Valid auc & aupr: 0.9364161624352817 0.6306254127616957 ;   Test auc & aupr: 0.9485418148383837 0.6449064768160283\n",
      "step 2625 : Total_loss & DTIloss & L2_loss: 84242.375 , 384.2339172363281 , 39541.2578125\n",
      "Valid auc & aupr: 0.9363948495623062 0.6314804892910262 ;   Test auc & aupr: 0.9492378319826311 0.6463314428605488\n",
      "step 2650 : Total_loss & DTIloss & L2_loss: 84054.55 , 382.6994934082031 , 39582.31640625\n",
      "Valid auc & aupr: 0.9363181752022123 0.6322547644832708 ;   Test auc & aupr: 0.9499316070548454 0.6471691137976343\n",
      "step 2675 : Total_loss & DTIloss & L2_loss: 83766.27 , 382.05181884765625 , 39623.99609375\n",
      "Valid auc & aupr: 0.9366007007256774 0.6325540952550274 ;   Test auc & aupr: 0.9497442965586729 0.6462055600808542\n",
      "step 2700 : Total_loss & DTIloss & L2_loss: 83757.05 , 381.125244140625 , 39665.49609375\n",
      "Valid auc & aupr: 0.935988865323436 0.6339885733675004 ;   Test auc & aupr: 0.9497345484193978 0.6465510412387226\n",
      "step 2725 : Total_loss & DTIloss & L2_loss: 83498.28 , 380.42987060546875 , 39707.26953125\n",
      "Valid auc & aupr: 0.9366144761191858 0.6320047867673171 ;   Test auc & aupr: 0.9497345484193978 0.6465510412387226\n",
      "step 2750 : Total_loss & DTIloss & L2_loss: 83339.91 , 379.3964538574219 , 39745.71875\n",
      "Valid auc & aupr: 0.9361702846568107 0.6338001067799309 ;   Test auc & aupr: 0.9497345484193978 0.6465510412387226\n",
      "step 2775 : Total_loss & DTIloss & L2_loss: 83125.7 , 378.90087890625 , 39787.16796875\n",
      "Valid auc & aupr: 0.9366745160418357 0.6336172385148794 ;   Test auc & aupr: 0.9497345484193978 0.6465510412387226\n",
      "step 2800 : Total_loss & DTIloss & L2_loss: 82923.59 , 377.8465270996094 , 39822.49609375\n",
      "Valid auc & aupr: 0.9365240263655834 0.635217114994032 ;   Test auc & aupr: 0.9508225869845963 0.6474859233986773\n",
      "step 2825 : Total_loss & DTIloss & L2_loss: 82703.89 , 376.96148681640625 , 39862.0703125\n",
      "Valid auc & aupr: 0.9365442995862183 0.6348185601501004 ;   Test auc & aupr: 0.9508225869845963 0.6474859233986773\n",
      "step 2850 : Total_loss & DTIloss & L2_loss: 82778.03 , 376.39166259765625 , 39901.25390625\n",
      "Valid auc & aupr: 0.9367012870895973 0.6345548211731932 ;   Test auc & aupr: 0.9508225869845963 0.6474859233986773\n",
      "step 2875 : Total_loss & DTIloss & L2_loss: 82427.87 , 375.34100341796875 , 39944.51171875\n",
      "Valid auc & aupr: 0.936796675191816 0.6343841008385203 ;   Test auc & aupr: 0.9508225869845963 0.6474859233986773\n",
      "step 2900 : Total_loss & DTIloss & L2_loss: 82460.22 , 374.4394226074219 , 39980.14453125\n",
      "Valid auc & aupr: 0.9371358617678248 0.635258604702722 ;   Test auc & aupr: 0.9504273974183808 0.6475341625382933\n",
      "step 2925 : Total_loss & DTIloss & L2_loss: 82076.766 , 373.84808349609375 , 40018.9453125\n",
      "Valid auc & aupr: 0.9368465785041481 0.6344650741361292 ;   Test auc & aupr: 0.9504273974183808 0.6475341625382933\n",
      "step 2950 : Total_loss & DTIloss & L2_loss: 81886.516 , 373.0813293457031 , 40051.75\n",
      "Valid auc & aupr: 0.9363426070322084 0.6350670957358745 ;   Test auc & aupr: 0.9504273974183808 0.6475341625382933\n",
      "step 2975 : Total_loss & DTIloss & L2_loss: 82382.766 , 373.61468505859375 , 40094.1328125\n",
      "Valid auc & aupr: 0.9367280581373589 0.6360915698030009 ;   Test auc & aupr: 0.9501056600816037 0.6490341428299059\n",
      "step 3000 : Total_loss & DTIloss & L2_loss: 82503.94 , 371.2717590332031 , 40175.703125\n",
      "Valid auc & aupr: 0.9373235190152415 0.6364386173025184 ;   Test auc & aupr: 0.9508379890446512 0.6483759944920414\n",
      "step 3025 : Total_loss & DTIloss & L2_loss: 82236.875 , 370.8818054199219 , 40222.09765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid auc & aupr: 0.9370849188031521 0.6365789883607308 ;   Test auc & aupr: 0.9509880616487925 0.6488939603373943\n",
      "step 3050 : Total_loss & DTIloss & L2_loss: 81392.45 , 370.5084533691406 , 40261.07421875\n",
      "Valid auc & aupr: 0.9370628261909218 0.6358518896982089 ;   Test auc & aupr: 0.9509880616487925 0.6488939603373943\n",
      "step 3075 : Total_loss & DTIloss & L2_loss: 81676.516 , 369.957275390625 , 40289.08203125\n",
      "Valid auc & aupr: 0.9371907034287733 0.6373113710521761 ;   Test auc & aupr: 0.9507641956303381 0.6486541835533259\n",
      "step 3100 : Total_loss & DTIloss & L2_loss: 81129.37 , 369.33465576171875 , 40316.96875\n",
      "Valid auc & aupr: 0.9372528226561038 0.637046941871287 ;   Test auc & aupr: 0.9507641956303381 0.6486541835533259\n",
      "step 3125 : Total_loss & DTIloss & L2_loss: 80890.44 , 368.47027587890625 , 40342.6953125\n",
      "Valid auc & aupr: 0.9378147547460128 0.6377131454580166 ;   Test auc & aupr: 0.9510131631074261 0.6500315109859675\n",
      "step 3150 : Total_loss & DTIloss & L2_loss: 80817.875 , 367.77850341796875 , 40365.43359375\n",
      "Valid auc & aupr: 0.9371584742062254 0.6376032900185665 ;   Test auc & aupr: 0.9510131631074261 0.6500315109859675\n",
      "step 3175 : Total_loss & DTIloss & L2_loss: 80870.21 , 367.375 , 40396.53515625\n",
      "Valid auc & aupr: 0.9372263115214272 0.6377753717365674 ;   Test auc & aupr: 0.9511664525975282 0.650770570741691\n",
      "step 3200 : Total_loss & DTIloss & L2_loss: 80695.734 , 366.5201110839844 , 40435.1953125\n",
      "Valid auc & aupr: 0.9372559416131246 0.6381902267092535 ;   Test auc & aupr: 0.9511073788735205 0.6510155307892286\n",
      "step 3225 : Total_loss & DTIloss & L2_loss: 80372.16 , 365.8265686035156 , 40466.71484375\n",
      "Valid auc & aupr: 0.937330276755453 0.6366502596780595 ;   Test auc & aupr: 0.9511073788735205 0.6510155307892286\n",
      "step 3250 : Total_loss & DTIloss & L2_loss: 80153.59 , 365.42620849609375 , 40490.0\n",
      "Valid auc & aupr: 0.9373567878901296 0.6380853481291353 ;   Test auc & aupr: 0.9511073788735205 0.6510155307892286\n",
      "step 3275 : Total_loss & DTIloss & L2_loss: 80142.55 , 364.5321350097656 , 40520.71484375\n",
      "Valid auc & aupr: 0.9375267710477617 0.6384998535847387 ;   Test auc & aupr: 0.9523681057259792 0.6506007612739444\n",
      "step 3300 : Total_loss & DTIloss & L2_loss: 79999.58 , 364.23724365234375 , 40544.27734375\n",
      "Valid auc & aupr: 0.9371012933275111 0.6372043735702927 ;   Test auc & aupr: 0.9523681057259792 0.6506007612739444\n",
      "step 3325 : Total_loss & DTIloss & L2_loss: 79884.63 , 363.66717529296875 , 40573.50390625\n",
      "Valid auc & aupr: 0.9374121493772483 0.6379259437053356 ;   Test auc & aupr: 0.9523681057259792 0.6506007612739444\n",
      "step 3350 : Total_loss & DTIloss & L2_loss: 79685.86 , 362.76129150390625 , 40598.58203125\n",
      "Valid auc & aupr: 0.9373136423180088 0.6389273030775283 ;   Test auc & aupr: 0.9521561324374402 0.6512799677223153\n",
      "step 3375 : Total_loss & DTIloss & L2_loss: 79581.984 , 362.74786376953125 , 40623.26171875\n",
      "Valid auc & aupr: 0.9373068845777972 0.6387728587128269 ;   Test auc & aupr: 0.9521561324374402 0.6512799677223153\n",
      "step 3400 : Total_loss & DTIloss & L2_loss: 79501.62 , 361.9281005859375 , 40649.8828125\n",
      "Valid auc & aupr: 0.9372730958767388 0.6384663685970663 ;   Test auc & aupr: 0.9521561324374402 0.6512799677223153\n",
      "step 3425 : Total_loss & DTIloss & L2_loss: 80469.34 , 361.31195068359375 , 40685.84375\n",
      "Valid auc & aupr: 0.9377536751710228 0.6390239686586343 ;   Test auc & aupr: 0.9529162923381186 0.6523057628663136\n",
      "step 3450 : Total_loss & DTIloss & L2_loss: 79687.44 , 360.7178955078125 , 40732.37890625\n",
      "Valid auc & aupr: 0.938194747676377 0.6396650180227689 ;   Test auc & aupr: 0.9526581128694156 0.6525267900520206\n",
      "step 3475 : Total_loss & DTIloss & L2_loss: 79178.17 , 360.4958801269531 , 40762.0546875\n",
      "Valid auc & aupr: 0.9371979809951552 0.6387576279883191 ;   Test auc & aupr: 0.9526581128694156 0.6525267900520206\n",
      "step 3500 : Total_loss & DTIloss & L2_loss: 79054.06 , 359.66107177734375 , 40780.58984375\n",
      "Valid auc & aupr: 0.9376031854947705 0.6397946970743054 ;   Test auc & aupr: 0.9529113695277847 0.6532265209635472\n",
      "step 3525 : Total_loss & DTIloss & L2_loss: 79103.07 , 359.59429931640625 , 40802.38671875\n",
      "Valid auc & aupr: 0.9371803069053709 0.6382520472984278 ;   Test auc & aupr: 0.9529113695277847 0.6532265209635472\n",
      "step 3550 : Total_loss & DTIloss & L2_loss: 78838.86 , 358.67181396484375 , 40821.8046875\n",
      "Valid auc & aupr: 0.937516374524359 0.6395435480307617 ;   Test auc & aupr: 0.9529113695277847 0.6532265209635472\n",
      "step 3575 : Total_loss & DTIloss & L2_loss: 78642.69 , 358.4608154296875 , 40841.67578125\n",
      "Valid auc & aupr: 0.937488043998087 0.6393026357415366 ;   Test auc & aupr: 0.9529113695277847 0.6532265209635472\n",
      "step 3600 : Total_loss & DTIloss & L2_loss: 78677.28 , 357.5564270019531 , 40866.984375\n",
      "Valid auc & aupr: 0.9378815524088745 0.6399514710509032 ;   Test auc & aupr: 0.9529869176071671 0.6538155467666388\n",
      "step 3625 : Total_loss & DTIloss & L2_loss: 78499.59 , 357.4071960449219 , 40892.48046875\n",
      "Valid auc & aupr: 0.9376808995072048 0.6402341990900626 ;   Test auc & aupr: 0.9533494996475071 0.6540518173546309\n",
      "step 3650 : Total_loss & DTIloss & L2_loss: 78376.95 , 357.0735778808594 , 40910.47265625\n",
      "Valid auc & aupr: 0.93798447799056 0.6386216032324471 ;   Test auc & aupr: 0.9533494996475071 0.6540518173546309\n",
      "step 3675 : Total_loss & DTIloss & L2_loss: 78678.414 , 356.21856689453125 , 40933.671875\n",
      "Valid auc & aupr: 0.9377152080344334 0.6387790296562363 ;   Test auc & aupr: 0.9533494996475071 0.6540518173546309\n",
      "step 3700 : Total_loss & DTIloss & L2_loss: 78368.09 , 355.8769836425781 , 40960.7734375\n",
      "Valid auc & aupr: 0.9382017653296738 0.6407083808301344 ;   Test auc & aupr: 0.9540502933799997 0.6545787014053951\n",
      "step 3725 : Total_loss & DTIloss & L2_loss: 78171.445 , 355.77276611328125 , 40985.0859375\n",
      "Valid auc & aupr: 0.9375514627908428 0.6400826751649182 ;   Test auc & aupr: 0.9540502933799997 0.6545787014053951\n",
      "step 3750 : Total_loss & DTIloss & L2_loss: 77981.33 , 355.1528625488281 , 41003.31640625\n",
      "Valid auc & aupr: 0.9385942340881209 0.6396617669877213 ;   Test auc & aupr: 0.9540502933799997 0.6545787014053951\n",
      "step 3775 : Total_loss & DTIloss & L2_loss: 77831.47 , 354.5763244628906 , 41021.22265625\n",
      "Valid auc & aupr: 0.9380112490383214 0.640185145933784 ;   Test auc & aupr: 0.9540502933799997 0.6545787014053951\n",
      "step 3800 : Total_loss & DTIloss & L2_loss: 77866.45 , 354.2454528808594 , 41042.59375\n",
      "Valid auc & aupr: 0.937766670825276 0.6406514102687895 ;   Test auc & aupr: 0.9540502933799997 0.6545787014053951\n",
      "step 3825 : Total_loss & DTIloss & L2_loss: 77690.875 , 353.72113037109375 , 41061.7265625\n",
      "Valid auc & aupr: 0.9382545276859418 0.6392707874618396 ;   Test auc & aupr: 0.9540502933799997 0.6545787014053951\n",
      "step 3850 : Total_loss & DTIloss & L2_loss: 77872.016 , 353.26373291015625 , 41079.328125\n",
      "Valid auc & aupr: 0.9387384858503317 0.6393685730241064 ;   Test auc & aupr: 0.9540502933799997 0.6545787014053951\n",
      "step 3875 : Total_loss & DTIloss & L2_loss: 77542.66 , 352.9514465332031 , 41101.83203125\n",
      "Early Stopping\n",
      "len(fpr): 122\n",
      "len(recall): 48153\n",
      "Time spent in this fold: 2188.365339756012\n",
      "--------------------------------------------------------------\n",
      "round  1  of  1 : KFold  5  of 10\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:01<00:00, 684.72it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_Di_emb = th.tensor(Drug_Di_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:00<00:00, 718.78it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_PDi_emb = th.tensor(Drug_PDi_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:01<00:00, 637.26it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_D2_emb = th.tensor(Drug_D2_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:01<00:00, 637.50it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:190: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_D4_emb = th.tensor(Drug_D4_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:01<00:00, 808.55it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:230: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_Di_emb = th.tensor(Protein_Di_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:02<00:00, 730.08it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_DiDP_emb = th.tensor(Protein_DiDP_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:02<00:00, 709.80it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:265: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_DDiP_emb = th.tensor(Protein_DDiP_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:02<00:00, 672.20it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:303: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_P2_emb = th.tensor(Protein_P2_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:06<00:00, 855.71it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:358: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_P_emb = th.tensor(Disease_P_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:07<00:00, 765.19it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:393: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_DPDi_emb = th.tensor(Disease_DPDi_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:08<00:00, 664.44it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:430: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_Di2_emb = th.tensor(Disease_Di2_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:08<00:00, 692.56it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:448: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_Di3_emb = th.tensor(Disease_Di3_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:04<00:00, 966.46it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:470: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_D_emb = th.tensor(Sideeffect_D_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:06<00:00, 690.21it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:490: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_Si1_emb = th.tensor(Sideeffect_Si1_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:06<00:00, 684.48it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:508: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_Si2_emb = th.tensor(Sideeffect_Si2_emb).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : Total_loss & DTIloss & L2_loss: 2072838.1 , 1652.953857421875 , 120177.6015625\n",
      "Valid auc & aupr: 0.485676278359243 0.0018717217381316277 ;   Test auc & aupr: 0.46628277031980137 0.0016560681113814556\n",
      "step 25 : Total_loss & DTIloss & L2_loss: 1149187.8 , 4503.40625 , 117209.25\n",
      "Valid auc & aupr: 0.40783210080218146 0.0015340973682548117 ;   Test auc & aupr: 0.46628277031980137 0.0016560681113814556\n",
      "step 50 : Total_loss & DTIloss & L2_loss: 856912.44 , 2595.232666015625 , 108600.703125\n",
      "Valid auc & aupr: 0.7267439493422395 0.0073346051058987075 ;   Test auc & aupr: 0.7523105527116983 0.005407601276134788\n",
      "step 75 : Total_loss & DTIloss & L2_loss: 787311.5 , 2921.82958984375 , 97761.28125\n",
      "Valid auc & aupr: 0.7477453995264001 0.023097227854910606 ;   Test auc & aupr: 0.7901428619055195 0.018724951760479162\n",
      "step 100 : Total_loss & DTIloss & L2_loss: 757555.5 , 1629.2008056640625 , 87925.3046875\n",
      "Valid auc & aupr: 0.8776645529764178 0.06850157170774614 ;   Test auc & aupr: 0.8907781920087094 0.0425831092683579\n",
      "step 125 : Total_loss & DTIloss & L2_loss: 723859.25 , 1825.758544921875 , 78781.328125\n",
      "Valid auc & aupr: 0.8151431177322075 0.08240198330239386 ;   Test auc & aupr: 0.8396699650470718 0.072624447084144\n",
      "step 150 : Total_loss & DTIloss & L2_loss: 695960.56 , 1622.9962158203125 , 72217.0390625\n",
      "Valid auc & aupr: 0.8536068518435748 0.08614921251441233 ;   Test auc & aupr: 0.8702201948692033 0.08143225342196178\n",
      "step 175 : Total_loss & DTIloss & L2_loss: 678378.6 , 1593.79052734375 , 66078.71875\n",
      "Valid auc & aupr: 0.8682197695017022 0.09863211753075812 ;   Test auc & aupr: 0.8849858232810521 0.09801551478080416\n",
      "step 200 : Total_loss & DTIloss & L2_loss: 657973.8 , 1583.5284423828125 , 60536.546875\n",
      "Valid auc & aupr: 0.8738680531554229 0.12138621783203593 ;   Test auc & aupr: 0.8899439462495398 0.1300559095349283\n",
      "step 225 : Total_loss & DTIloss & L2_loss: 639447.75 , 1571.751708984375 , 55849.15625\n",
      "Valid auc & aupr: 0.8805525303434552 0.1436519446694488 ;   Test auc & aupr: 0.8880236602937231 0.1491033204528776\n",
      "step 250 : Total_loss & DTIloss & L2_loss: 619532.25 , 1555.7720947265625 , 51992.8125\n",
      "Valid auc & aupr: 0.8885630608842804 0.17092593677021106 ;   Test auc & aupr: 0.8893555241925422 0.1717341293288478\n",
      "step 275 : Total_loss & DTIloss & L2_loss: 602063.75 , 1532.7003173828125 , 48436.46484375\n",
      "Valid auc & aupr: 0.886870368051164 0.18493516831479187 ;   Test auc & aupr: 0.8905669498306164 0.20531421268254146\n",
      "step 300 : Total_loss & DTIloss & L2_loss: 578030.56 , 1501.18505859375 , 45372.3125\n",
      "Valid auc & aupr: 0.8834672810219706 0.21358750598593745 ;   Test auc & aupr: 0.897070859614317 0.2570793260992502\n",
      "step 325 : Total_loss & DTIloss & L2_loss: 556416.6 , 1456.041748046875 , 42612.6875\n",
      "Valid auc & aupr: 0.886359462462711 0.23923409456090067 ;   Test auc & aupr: 0.8912709117083731 0.3030908907834775\n",
      "step 350 : Total_loss & DTIloss & L2_loss: 527717.75 , 1383.5196533203125 , 39962.90625\n",
      "Valid auc & aupr: 0.8925865806852463 0.26801648211979917 ;   Test auc & aupr: 0.8988803579672711 0.351182885822124\n",
      "step 375 : Total_loss & DTIloss & L2_loss: 486242.62 , 1313.6474609375 , 38478.50390625\n",
      "Valid auc & aupr: 0.8976996193764428 0.2826223419946469 ;   Test auc & aupr: 0.9003474041874888 0.3785124081863522\n",
      "step 400 : Total_loss & DTIloss & L2_loss: 444730.16 , 1235.745849609375 , 36950.13671875\n",
      "Valid auc & aupr: 0.908126164694366 0.2912104523965423 ;   Test auc & aupr: 0.9057968577457935 0.4012013574474658\n",
      "step 425 : Total_loss & DTIloss & L2_loss: 403735.5 , 1164.3848876953125 , 35759.421875\n",
      "Valid auc & aupr: 0.9108535021925349 0.30844949823687634 ;   Test auc & aupr: 0.9118878363497976 0.421818255598829\n",
      "step 450 : Total_loss & DTIloss & L2_loss: 368513.16 , 1100.906494140625 , 34765.94140625\n",
      "Valid auc & aupr: 0.9142800435276516 0.32873367953102217 ;   Test auc & aupr: 0.9102751529288089 0.44193704019722985\n",
      "step 475 : Total_loss & DTIloss & L2_loss: 336479.12 , 1045.968505859375 , 34047.421875\n",
      "Valid auc & aupr: 0.9180345026116148 0.34799224499667053 ;   Test auc & aupr: 0.9110500325197927 0.45735139216166015\n",
      "step 500 : Total_loss & DTIloss & L2_loss: 311093.9 , 997.6876831054688 , 33501.66015625\n",
      "Valid auc & aupr: 0.920000017701363 0.3601950543112659 ;   Test auc & aupr: 0.9171794187925407 0.46803867238301994\n",
      "step 525 : Total_loss & DTIloss & L2_loss: 284876.03 , 953.3850708007812 , 33223.16796875\n",
      "Valid auc & aupr: 0.916818197709178 0.3749335974522126 ;   Test auc & aupr: 0.9211480325135538 0.47955081897367074\n",
      "step 550 : Total_loss & DTIloss & L2_loss: 265068.12 , 913.4920654296875 , 32914.1171875\n",
      "Valid auc & aupr: 0.9178298306023819 0.3873259530099406 ;   Test auc & aupr: 0.9278864337875186 0.48783693796390837\n",
      "step 575 : Total_loss & DTIloss & L2_loss: 248193.84 , 878.09423828125 , 32687.38671875\n",
      "Valid auc & aupr: 0.9192120857825751 0.39927811037299116 ;   Test auc & aupr: 0.9324984247006931 0.4947129865186618\n",
      "step 600 : Total_loss & DTIloss & L2_loss: 233866.95 , 845.4930419921875 , 32560.29296875\n",
      "Valid auc & aupr: 0.9205821712757328 0.4079378325119043 ;   Test auc & aupr: 0.9340014902955324 0.5030840353243857\n",
      "step 625 : Total_loss & DTIloss & L2_loss: 221759.98 , 814.1145629882812 , 32623.169921875\n",
      "Valid auc & aupr: 0.9217531164355828 0.4179216616851778 ;   Test auc & aupr: 0.9306489102360143 0.5107736296241291\n",
      "step 650 : Total_loss & DTIloss & L2_loss: 210556.52 , 786.863037109375 , 32559.38671875\n",
      "Valid auc & aupr: 0.9243662801426552 0.4264483840883079 ;   Test auc & aupr: 0.9295634061870269 0.5150793810390922\n",
      "step 675 : Total_loss & DTIloss & L2_loss: 201050.3 , 761.9879150390625 , 32477.654296875\n",
      "Valid auc & aupr: 0.9251057545803383 0.43006494887120605 ;   Test auc & aupr: 0.9290604509411243 0.521024978432527\n",
      "step 700 : Total_loss & DTIloss & L2_loss: 193434.19 , 735.842041015625 , 32645.267578125\n",
      "Valid auc & aupr: 0.9256013927432377 0.4372253840682125 ;   Test auc & aupr: 0.9278437369374934 0.526110746325594\n",
      "step 725 : Total_loss & DTIloss & L2_loss: 182375.45 , 704.791015625 , 32702.87890625\n",
      "Valid auc & aupr: 0.9294631663401502 0.44601253053697143 ;   Test auc & aupr: 0.9292064780674665 0.5328235614636007\n",
      "step 750 : Total_loss & DTIloss & L2_loss: 173082.12 , 676.0897216796875 , 32848.1328125\n",
      "Valid auc & aupr: 0.930957825175144 0.4528523467184247 ;   Test auc & aupr: 0.9297066063529795 0.5388267914745768\n",
      "step 775 : Total_loss & DTIloss & L2_loss: 165091.9 , 652.694091796875 , 32952.77734375\n",
      "Valid auc & aupr: 0.9318019589213321 0.4599807999247456 ;   Test auc & aupr: 0.9297232269304434 0.5449650403975467\n",
      "step 800 : Total_loss & DTIloss & L2_loss: 158751.4 , 631.8544311523438 , 33042.40234375\n",
      "Valid auc & aupr: 0.9337181314618274 0.4643149498909088 ;   Test auc & aupr: 0.9303170348343908 0.5489065609544287\n",
      "step 825 : Total_loss & DTIloss & L2_loss: 153164.92 , 614.96826171875 , 33145.26953125\n",
      "Valid auc & aupr: 0.9334745164540806 0.46789044906500454 ;   Test auc & aupr: 0.9303954586148596 0.5537294222588757\n",
      "step 850 : Total_loss & DTIloss & L2_loss: 148677.0 , 600.431396484375 , 33250.40234375\n",
      "Valid auc & aupr: 0.9351032631135016 0.4715972139961208 ;   Test auc & aupr: 0.9304682041042006 0.5572380724512903\n",
      "step 875 : Total_loss & DTIloss & L2_loss: 144602.62 , 587.4577026367188 , 33374.68359375\n",
      "Valid auc & aupr: 0.9346054122802321 0.47483596895895014 ;   Test auc & aupr: 0.9296455830011168 0.5596032364454611\n",
      "step 900 : Total_loss & DTIloss & L2_loss: 141030.66 , 575.8858032226562 , 33496.3984375\n",
      "Valid auc & aupr: 0.9352077011549698 0.47796603307379754 ;   Test auc & aupr: 0.9299311791115312 0.5629437478008896\n",
      "step 925 : Total_loss & DTIloss & L2_loss: 138061.52 , 566.18798828125 , 33632.23046875\n",
      "Valid auc & aupr: 0.9344007402709991 0.47979754433931165 ;   Test auc & aupr: 0.9287019631192798 0.5656048556109917\n",
      "step 950 : Total_loss & DTIloss & L2_loss: 134801.83 , 555.7161865234375 , 33778.21875\n",
      "Valid auc & aupr: 0.9339338668229107 0.48235182391554626 ;   Test auc & aupr: 0.9276506262984523 0.5680708364734555\n",
      "step 975 : Total_loss & DTIloss & L2_loss: 132058.28 , 546.8106689453125 , 33917.296875\n",
      "Valid auc & aupr: 0.932333663611264 0.4822150409829275 ;   Test auc & aupr: 0.9276506262984523 0.5680708364734555\n",
      "step 1000 : Total_loss & DTIloss & L2_loss: 129547.35 , 538.3445434570312 , 34055.2734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid auc & aupr: 0.9340071062121607 0.48465312350282036 ;   Test auc & aupr: 0.9259241820921222 0.5733384996480296\n",
      "step 1025 : Total_loss & DTIloss & L2_loss: 127185.48 , 531.2709350585938 , 34184.8203125\n",
      "Valid auc & aupr: 0.9344288411846992 0.48882185832056374 ;   Test auc & aupr: 0.9245526676368016 0.575104635803664\n",
      "step 1050 : Total_loss & DTIloss & L2_loss: 125486.17 , 523.511474609375 , 34323.36328125\n",
      "Valid auc & aupr: 0.9349481549205585 0.4907550138475369 ;   Test auc & aupr: 0.9238479259079028 0.5771625465651871\n",
      "step 1075 : Total_loss & DTIloss & L2_loss: 123345.97 , 517.1658935546875 , 34475.80078125\n",
      "Valid auc & aupr: 0.9358854420893273 0.4928586183507336 ;   Test auc & aupr: 0.9233750924123604 0.5784919939193343\n",
      "step 1100 : Total_loss & DTIloss & L2_loss: 121345.14 , 511.1755676269531 , 34608.93359375\n",
      "Valid auc & aupr: 0.93386660164366 0.49616875049591974 ;   Test auc & aupr: 0.9216622468603194 0.5808956058838873\n",
      "step 1125 : Total_loss & DTIloss & L2_loss: 119514.766 , 505.10467529296875 , 34737.66796875\n",
      "Valid auc & aupr: 0.9355115007967825 0.4962615424121737 ;   Test auc & aupr: 0.9225538604191232 0.5830241376357099\n",
      "step 1150 : Total_loss & DTIloss & L2_loss: 118031.13 , 499.56024169921875 , 34863.3984375\n",
      "Valid auc & aupr: 0.9379934531509091 0.49524177774443084 ;   Test auc & aupr: 0.9225538604191232 0.5830241376357099\n",
      "step 1175 : Total_loss & DTIloss & L2_loss: 116528.53 , 494.9256591796875 , 34985.140625\n",
      "Valid auc & aupr: 0.93597992311413 0.4971025541870651 ;   Test auc & aupr: 0.9226444206329896 0.5869229796170725\n",
      "step 1200 : Total_loss & DTIloss & L2_loss: 115088.44 , 489.56256103515625 , 35106.44921875\n",
      "Valid auc & aupr: 0.9360383376119004 0.49765218040267434 ;   Test auc & aupr: 0.9223362332098051 0.588428992516168\n",
      "step 1225 : Total_loss & DTIloss & L2_loss: 113811.95 , 485.3330383300781 , 35223.4296875\n",
      "Valid auc & aupr: 0.9377529358816805 0.49825150821222225 ;   Test auc & aupr: 0.9252063778565949 0.5895373738226787\n",
      "step 1250 : Total_loss & DTIloss & L2_loss: 112583.47 , 480.8798828125 , 35345.59765625\n",
      "Valid auc & aupr: 0.9375013552606016 0.5002326169661371 ;   Test auc & aupr: 0.9258174887077555 0.5927248262304794\n",
      "step 1275 : Total_loss & DTIloss & L2_loss: 111512.31 , 477.31622314453125 , 35466.1875\n",
      "Valid auc & aupr: 0.9349890893224051 0.5007875943622603 ;   Test auc & aupr: 0.9266189319782641 0.5922295996505319\n",
      "step 1300 : Total_loss & DTIloss & L2_loss: 110281.41 , 472.669189453125 , 35578.96484375\n",
      "Valid auc & aupr: 0.9375256946346727 0.5004935695944345 ;   Test auc & aupr: 0.9266189319782641 0.5922295996505319\n",
      "step 1325 : Total_loss & DTIloss & L2_loss: 109157.72 , 469.533203125 , 35690.953125\n",
      "Valid auc & aupr: 0.9357644090200835 0.5010603427017913 ;   Test auc & aupr: 0.9270362498206342 0.5965465605119398\n",
      "step 1350 : Total_loss & DTIloss & L2_loss: 108156.71 , 465.73114013671875 , 35801.046875\n",
      "Valid auc & aupr: 0.9341323433551074 0.5032167919343025 ;   Test auc & aupr: 0.9292450319582999 0.5977505465581565\n",
      "step 1375 : Total_loss & DTIloss & L2_loss: 107110.09 , 462.17022705078125 , 35906.07421875\n",
      "Valid auc & aupr: 0.9351074671872048 0.5033014719186294 ;   Test auc & aupr: 0.9296233572435694 0.5974619689900508\n",
      "step 1400 : Total_loss & DTIloss & L2_loss: 106572.34 , 459.1337890625 , 36013.96484375\n",
      "Valid auc & aupr: 0.9361713191011426 0.5061648397563994 ;   Test auc & aupr: 0.9293777041338349 0.5980696209615919\n",
      "step 1425 : Total_loss & DTIloss & L2_loss: 105630.16 , 456.1452331542969 , 36132.99609375\n",
      "Valid auc & aupr: 0.9355360614378906 0.5042787727487875 ;   Test auc & aupr: 0.9293777041338349 0.5980696209615919\n",
      "step 1450 : Total_loss & DTIloss & L2_loss: 104605.91 , 452.3795471191406 , 36229.7421875\n",
      "Valid auc & aupr: 0.9360704213322665 0.5068425321830334 ;   Test auc & aupr: 0.9309179101393127 0.5980079519481751\n",
      "step 1475 : Total_loss & DTIloss & L2_loss: 103824.72 , 450.1399841308594 , 36329.73828125\n",
      "Valid auc & aupr: 0.9335189911285194 0.5088075589551062 ;   Test auc & aupr: 0.9318996452457156 0.5993264626600984\n",
      "step 1500 : Total_loss & DTIloss & L2_loss: 102963.01 , 446.885009765625 , 36425.0\n",
      "Valid auc & aupr: 0.9324113283412541 0.5076693321312589 ;   Test auc & aupr: 0.9318996452457156 0.5993264626600984\n",
      "step 1525 : Total_loss & DTIloss & L2_loss: 102237.44 , 444.65875244140625 , 36526.33203125\n",
      "Valid auc & aupr: 0.9340409600688229 0.5088658228459331 ;   Test auc & aupr: 0.9329958235072088 0.6008128373195032\n",
      "step 1550 : Total_loss & DTIloss & L2_loss: 101677.08 , 441.416015625 , 36616.796875\n",
      "Valid auc & aupr: 0.9351320278283127 0.5089585567592856 ;   Test auc & aupr: 0.9330399338374291 0.5999510170745739\n",
      "step 1575 : Total_loss & DTIloss & L2_loss: 100866.41 , 439.71429443359375 , 36711.1953125\n",
      "Valid auc & aupr: 0.9349782472375917 0.5108436092299831 ;   Test auc & aupr: 0.9334461388010257 0.6004078050620926\n",
      "step 1600 : Total_loss & DTIloss & L2_loss: 100591.16 , 437.1155700683594 , 36808.65625\n",
      "Valid auc & aupr: 0.9340781329310405 0.5123023811432793 ;   Test auc & aupr: 0.9340712382320463 0.6011771460558243\n",
      "step 1625 : Total_loss & DTIloss & L2_loss: 99683.3 , 434.6961364746094 , 36914.69140625\n",
      "Valid auc & aupr: 0.9340314455862315 0.5128747087680169 ;   Test auc & aupr: 0.93397005254637 0.6009443618733842\n",
      "step 1650 : Total_loss & DTIloss & L2_loss: 98852.375 , 432.0862731933594 , 37004.11328125\n",
      "Valid auc & aupr: 0.9353123604634748 0.5135285580939282 ;   Test auc & aupr: 0.934146542607947 0.6015353731582479\n",
      "step 1675 : Total_loss & DTIloss & L2_loss: 98636.16 , 430.99395751953125 , 37091.578125\n",
      "Valid auc & aupr: 0.9352594776416298 0.5135861121466443 ;   Test auc & aupr: 0.9344899695546114 0.6016503694403103\n",
      "step 1700 : Total_loss & DTIloss & L2_loss: 98359.25 , 426.8933410644531 , 37219.87109375\n",
      "Valid auc & aupr: 0.9354882677578967 0.5150079000761538 ;   Test auc & aupr: 0.9349280996743342 0.6019839525720112\n",
      "step 1725 : Total_loss & DTIloss & L2_loss: 97286.17 , 425.84844970703125 , 37321.59375\n",
      "Valid auc & aupr: 0.9345051783124672 0.5139392228630275 ;   Test auc & aupr: 0.9349280996743342 0.6019839525720112\n",
      "step 1750 : Total_loss & DTIloss & L2_loss: 96747.25 , 423.7872314453125 , 37400.97265625\n",
      "Valid auc & aupr: 0.9341920854550999 0.514884503935241 ;   Test auc & aupr: 0.9349280996743342 0.6019839525720112\n",
      "step 1775 : Total_loss & DTIloss & L2_loss: 96017.19 , 421.53448486328125 , 37474.1484375\n",
      "Valid auc & aupr: 0.9343330325576744 0.5146922860434721 ;   Test auc & aupr: 0.9349280996743342 0.6019839525720112\n",
      "step 1800 : Total_loss & DTIloss & L2_loss: 95600.34 , 419.3917236328125 , 37545.62890625\n",
      "Valid auc & aupr: 0.9332413009970736 0.5145832681213559 ;   Test auc & aupr: 0.9349280996743342 0.6019839525720112\n",
      "step 1825 : Total_loss & DTIloss & L2_loss: 95048.82 , 417.85699462890625 , 37622.24609375\n",
      "Valid auc & aupr: 0.933531824616666 0.5164174239588922 ;   Test auc & aupr: 0.9360472348038207 0.6029252665444697\n",
      "step 1850 : Total_loss & DTIloss & L2_loss: 94659.47 , 415.55401611328125 , 37694.1953125\n",
      "Valid auc & aupr: 0.9343839239761864 0.5166558262922336 ;   Test auc & aupr: 0.9367197589324149 0.6028339943830063\n",
      "step 1875 : Total_loss & DTIloss & L2_loss: 94342.63 , 414.3205261230469 , 37770.38671875\n",
      "Valid auc & aupr: 0.9338024342029276 0.5168013524292518 ;   Test auc & aupr: 0.9358650908214641 0.603237484969158\n",
      "step 1900 : Total_loss & DTIloss & L2_loss: 93833.55 , 412.44580078125 , 37847.7578125\n",
      "Valid auc & aupr: 0.9338026554699648 0.5177749876033675 ;   Test auc & aupr: 0.9368062736684821 0.6034149549473417\n",
      "step 1925 : Total_loss & DTIloss & L2_loss: 93190.36 , 410.75921630859375 , 37920.7890625\n",
      "Valid auc & aupr: 0.9333930901844614 0.5166246279924057 ;   Test auc & aupr: 0.9368062736684821 0.6034149549473417\n",
      "step 1950 : Total_loss & DTIloss & L2_loss: 92952.19 , 409.04241943359375 , 37987.10546875\n",
      "Valid auc & aupr: 0.9310062826562489 0.5185189630801662 ;   Test auc & aupr: 0.9369428450997274 0.6044753399319555\n",
      "step 1975 : Total_loss & DTIloss & L2_loss: 92341.52 , 407.5081787109375 , 38060.23046875\n",
      "Valid auc & aupr: 0.9317552715765233 0.5198611559895094 ;   Test auc & aupr: 0.9373548989468891 0.6037177919451784\n",
      "step 2000 : Total_loss & DTIloss & L2_loss: 92161.55 , 405.6383361816406 , 38131.74609375\n",
      "Valid auc & aupr: 0.9316068013946904 0.5182521120828978 ;   Test auc & aupr: 0.9373548989468891 0.6037177919451784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2025 : Total_loss & DTIloss & L2_loss: 91581.51 , 404.3831787109375 , 38201.6875\n",
      "Valid auc & aupr: 0.9305219291122369 0.5192992236025927 ;   Test auc & aupr: 0.9373548989468891 0.6037177919451784\n",
      "step 2050 : Total_loss & DTIloss & L2_loss: 91295.375 , 402.50347900390625 , 38262.125\n",
      "Valid auc & aupr: 0.9288482652443032 0.5192514142063505 ;   Test auc & aupr: 0.9373548989468891 0.6037177919451784\n",
      "step 2075 : Total_loss & DTIloss & L2_loss: 91254.92 , 400.85198974609375 , 38329.03515625\n",
      "Valid auc & aupr: 0.9300707656237761 0.5207254333856691 ;   Test auc & aupr: 0.9372840299743584 0.6063333885880817\n",
      "step 2100 : Total_loss & DTIloss & L2_loss: 90800.2 , 399.6393127441406 , 38408.828125\n",
      "Valid auc & aupr: 0.9302278652200523 0.5199511487692446 ;   Test auc & aupr: 0.9372840299743584 0.6063333885880817\n",
      "step 2125 : Total_loss & DTIloss & L2_loss: 90216.97 , 398.41143798828125 , 38477.19921875\n",
      "Valid auc & aupr: 0.9306146400007436 0.5212178212168842 ;   Test auc & aupr: 0.9376316486209113 0.6068624852206114\n",
      "step 2150 : Total_loss & DTIloss & L2_loss: 89961.18 , 396.77825927734375 , 38533.74609375\n",
      "Valid auc & aupr: 0.9294631663401504 0.5195618118419436 ;   Test auc & aupr: 0.9376316486209113 0.6068624852206114\n",
      "step 2175 : Total_loss & DTIloss & L2_loss: 89484.26 , 395.41156005859375 , 38597.6875\n",
      "Valid auc & aupr: 0.9286462484395142 0.5198281323665386 ;   Test auc & aupr: 0.9376316486209113 0.6068624852206114\n",
      "step 2200 : Total_loss & DTIloss & L2_loss: 89248.06 , 394.19927978515625 , 38655.90625\n",
      "Valid auc & aupr: 0.929376872195717 0.5210327315370505 ;   Test auc & aupr: 0.9376316486209113 0.6068624852206114\n",
      "step 2225 : Total_loss & DTIloss & L2_loss: 89175.75 , 392.68963623046875 , 38717.046875\n",
      "Valid auc & aupr: 0.9289496055472531 0.5211716374186733 ;   Test auc & aupr: 0.9376316486209113 0.6068624852206114\n",
      "step 2250 : Total_loss & DTIloss & L2_loss: 88659.8 , 391.67144775390625 , 38784.23046875\n",
      "Valid auc & aupr: 0.9281499464755038 0.5220506350317425 ;   Test auc & aupr: 0.9380436780977247 0.6075862242286121\n",
      "step 2275 : Total_loss & DTIloss & L2_loss: 88267.58 , 390.30706787109375 , 38834.578125\n",
      "Valid auc & aupr: 0.9284802981617577 0.5220939723400603 ;   Test auc & aupr: 0.9386406297765882 0.6077796200534621\n",
      "step 2300 : Total_loss & DTIloss & L2_loss: 87996.78 , 389.26318359375 , 38887.171875\n",
      "Valid auc & aupr: 0.9274133485093018 0.5212772449423245 ;   Test auc & aupr: 0.9386406297765882 0.6077796200534621\n",
      "step 2325 : Total_loss & DTIloss & L2_loss: 87707.16 , 387.99053955078125 , 38941.06640625\n",
      "Valid auc & aupr: 0.9279762518514519 0.5209529770742819 ;   Test auc & aupr: 0.9386406297765882 0.6077796200534621\n",
      "step 2350 : Total_loss & DTIloss & L2_loss: 87638.016 , 387.2419128417969 , 38994.171875\n",
      "Valid auc & aupr: 0.9279600993577503 0.521869479710513 ;   Test auc & aupr: 0.9386406297765882 0.6077796200534621\n",
      "step 2375 : Total_loss & DTIloss & L2_loss: 87203.09 , 385.65203857421875 , 39048.08203125\n",
      "Valid auc & aupr: 0.9280941871821777 0.5232867311255852 ;   Test auc & aupr: 0.9376647922944468 0.6085274153792095\n",
      "step 2400 : Total_loss & DTIloss & L2_loss: 86971.73 , 384.8199157714844 , 39099.5234375\n",
      "Valid auc & aupr: 0.9265419989175617 0.5222233564352255 ;   Test auc & aupr: 0.9376647922944468 0.6085274153792095\n",
      "step 2425 : Total_loss & DTIloss & L2_loss: 87088.945 , 383.12744140625 , 39150.27734375\n",
      "Valid auc & aupr: 0.927873141412206 0.5218667567830184 ;   Test auc & aupr: 0.9376647922944468 0.6085274153792095\n",
      "step 2450 : Total_loss & DTIloss & L2_loss: 86443.62 , 382.70098876953125 , 39210.03515625\n",
      "Valid auc & aupr: 0.9279406278584935 0.5227682266304269 ;   Test auc & aupr: 0.9376647922944468 0.6085274153792095\n",
      "step 2475 : Total_loss & DTIloss & L2_loss: 86233.19 , 381.36846923828125 , 39263.26953125\n",
      "Valid auc & aupr: 0.9269493515326945 0.523516475140424 ;   Test auc & aupr: 0.9372380187569797 0.6101164192061408\n",
      "step 2500 : Total_loss & DTIloss & L2_loss: 86064.78 , 380.64202880859375 , 39308.98046875\n",
      "Valid auc & aupr: 0.9269132850056621 0.5233747523768987 ;   Test auc & aupr: 0.9372380187569797 0.6101164192061408\n",
      "step 2525 : Total_loss & DTIloss & L2_loss: 86235.125 , 379.87518310546875 , 39376.3828125\n",
      "Valid auc & aupr: 0.9268338501393761 0.5229164187399696 ;   Test auc & aupr: 0.9372380187569797 0.6101164192061408\n",
      "step 2550 : Total_loss & DTIloss & L2_loss: 85569.39 , 378.77642822265625 , 39437.77734375\n",
      "Valid auc & aupr: 0.9263415309820318 0.5219346848683779 ;   Test auc & aupr: 0.9372380187569797 0.6101164192061408\n",
      "step 2575 : Total_loss & DTIloss & L2_loss: 85257.83 , 377.6340026855469 , 39474.234375\n",
      "Valid auc & aupr: 0.926420744581281 0.5233964681191824 ;   Test auc & aupr: 0.9372380187569797 0.6101164192061408\n",
      "step 2600 : Total_loss & DTIloss & L2_loss: 84945.664 , 377.08099365234375 , 39515.26171875\n",
      "Valid auc & aupr: 0.9255633348128722 0.5243425611555501 ;   Test auc & aupr: 0.9363084361956989 0.6106135304433605\n",
      "step 2625 : Total_loss & DTIloss & L2_loss: 85115.64 , 376.07611083984375 , 39558.5546875\n",
      "Valid auc & aupr: 0.9267046301897629 0.5239154415638473 ;   Test auc & aupr: 0.9363084361956989 0.6106135304433605\n",
      "step 2650 : Total_loss & DTIloss & L2_loss: 84975.86 , 375.0738525390625 , 39614.3984375\n",
      "Valid auc & aupr: 0.925936169770232 0.5233160613672707 ;   Test auc & aupr: 0.9363084361956989 0.6106135304433605\n",
      "step 2675 : Total_loss & DTIloss & L2_loss: 84429.45 , 374.0697021484375 , 39658.3671875\n",
      "Valid auc & aupr: 0.9239190994608608 0.5231942396254107 ;   Test auc & aupr: 0.9363084361956989 0.6106135304433605\n",
      "step 2700 : Total_loss & DTIloss & L2_loss: 84199.49 , 373.55560302734375 , 39698.125\n",
      "Valid auc & aupr: 0.9234823183298057 0.5227242473676568 ;   Test auc & aupr: 0.9363084361956989 0.6106135304433605\n",
      "step 2725 : Total_loss & DTIloss & L2_loss: 84382.62 , 372.33349609375 , 39742.03125\n",
      "Valid auc & aupr: 0.9255451909158375 0.5231251348560293 ;   Test auc & aupr: 0.9363084361956989 0.6106135304433605\n",
      "step 2750 : Total_loss & DTIloss & L2_loss: 83810.336 , 371.4614562988281 , 39786.8828125\n",
      "Valid auc & aupr: 0.9239290564775262 0.5229867796212475 ;   Test auc & aupr: 0.9363084361956989 0.6106135304433605\n",
      "step 2775 : Total_loss & DTIloss & L2_loss: 83574.625 , 370.6256103515625 , 39825.71484375\n",
      "Early Stopping\n",
      "len(fpr): 156\n",
      "len(recall): 48155\n",
      "Time spent in this fold: 1638.90318608284\n",
      "--------------------------------------------------------------\n",
      "round  1  of  1 : KFold  6  of 10\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:00<00:00, 797.48it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_Di_emb = th.tensor(Drug_Di_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:01<00:00, 607.20it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_PDi_emb = th.tensor(Drug_PDi_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:00<00:00, 834.91it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_D2_emb = th.tensor(Drug_D2_emb).to(self.device)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 708/708 [00:01<00:00, 638.99it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:190: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Drug_D4_emb = th.tensor(Drug_D4_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:01<00:00, 803.99it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:230: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_Di_emb = th.tensor(Protein_Di_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:02<00:00, 719.15it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:248: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_DiDP_emb = th.tensor(Protein_DiDP_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:02<00:00, 752.47it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:265: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_DDiP_emb = th.tensor(Protein_DDiP_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1512/1512 [00:02<00:00, 688.52it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:303: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Protein_P2_emb = th.tensor(Protein_P2_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:06<00:00, 859.24it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:358: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_P_emb = th.tensor(Disease_P_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:07<00:00, 740.10it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:393: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_DPDi_emb = th.tensor(Disease_DPDi_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:08<00:00, 683.69it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:430: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_Di2_emb = th.tensor(Disease_Di2_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5603/5603 [00:07<00:00, 707.62it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:448: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Disease_Di3_emb = th.tensor(Disease_Di3_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:04<00:00, 853.39it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:470: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_D_emb = th.tensor(Sideeffect_D_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:05<00:00, 702.07it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:490: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_Si1_emb = th.tensor(Sideeffect_Si1_emb).to(self.device)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4192/4192 [00:06<00:00, 691.96it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2104\\247274658.py:508: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Sideeffect_Si2_emb = th.tensor(Sideeffect_Si2_emb).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : Total_loss & DTIloss & L2_loss: 2072431.2 , 1646.767578125 , 120158.90625\n",
      "Valid auc & aupr: 0.5381881863073004 0.0020207215320403715 ;   Test auc & aupr: 0.5272051070891589 0.002125988426029931\n",
      "step 25 : Total_loss & DTIloss & L2_loss: 1142238.5 , 4365.8857421875 , 117235.25\n",
      "Valid auc & aupr: 0.749767253668448 0.007453578179105176 ;   Test auc & aupr: 0.730256094341712 0.006423455905142364\n",
      "step 50 : Total_loss & DTIloss & L2_loss: 849404.25 , 2041.3812255859375 , 108906.9296875\n",
      "Valid auc & aupr: 0.8661674771450116 0.04524836840272414 ;   Test auc & aupr: 0.8643199386101181 0.06772493804009601\n",
      "step 75 : Total_loss & DTIloss & L2_loss: 801671.0 , 4957.634765625 , 99119.6015625\n",
      "Valid auc & aupr: 0.8210521436964326 0.0248433775368562 ;   Test auc & aupr: 0.8643199386101181 0.06772493804009601\n",
      "step 100 : Total_loss & DTIloss & L2_loss: 758157.1 , 1821.94921875 , 88998.234375\n",
      "Valid auc & aupr: 0.8656114163453564 0.03146139238154334 ;   Test auc & aupr: 0.8643199386101181 0.06772493804009601\n",
      "step 125 : Total_loss & DTIloss & L2_loss: 725959.0 , 1994.450927734375 , 80300.2578125\n",
      "Valid auc & aupr: 0.8474055924258481 0.046581715872204124 ;   Test auc & aupr: 0.8370322404109504 0.0638796075411441\n",
      "step 150 : Total_loss & DTIloss & L2_loss: 698037.75 , 1612.10498046875 , 74142.34375\n",
      "Valid auc & aupr: 0.9005664579716246 0.0564346581638104 ;   Test auc & aupr: 0.9039764902175473 0.06880410948587692\n",
      "step 175 : Total_loss & DTIloss & L2_loss: 677296.9 , 1607.0927734375 , 68075.984375\n",
      "Valid auc & aupr: 0.9047416114864949 0.09637453055573152 ;   Test auc & aupr: 0.9040729967963713 0.08676834286849197\n",
      "step 200 : Total_loss & DTIloss & L2_loss: 660101.5 , 1596.849609375 , 62289.60546875\n",
      "Valid auc & aupr: 0.9078048939363191 0.12271400124408313 ;   Test auc & aupr: 0.907701156753199 0.11118415429838081\n",
      "step 225 : Total_loss & DTIloss & L2_loss: 638096.75 , 1584.982421875 , 57296.3984375\n",
      "Valid auc & aupr: 0.9044454799860703 0.12201908538404707 ;   Test auc & aupr: 0.907701156753199 0.11118415429838081\n",
      "step 250 : Total_loss & DTIloss & L2_loss: 619247.1 , 1567.794921875 , 52614.2109375\n",
      "Valid auc & aupr: 0.8988342640681252 0.1445559473650696 ;   Test auc & aupr: 0.9152855015534636 0.15384538749134505\n",
      "step 275 : Total_loss & DTIloss & L2_loss: 603283.06 , 1542.995361328125 , 48867.78515625\n",
      "Valid auc & aupr: 0.8957657203987504 0.1801409535102835 ;   Test auc & aupr: 0.9236832067245004 0.18608226655899443\n",
      "step 300 : Total_loss & DTIloss & L2_loss: 570933.75 , 1505.963134765625 , 45695.14453125\n",
      "Valid auc & aupr: 0.9058326882076027 0.2638556599892224 ;   Test auc & aupr: 0.9290985174249939 0.23455790921170616\n",
      "step 325 : Total_loss & DTIloss & L2_loss: 534769.7 , 1431.064453125 , 41555.70703125\n",
      "Valid auc & aupr: 0.9192410314996732 0.4004814223322156 ;   Test auc & aupr: 0.9355205915794794 0.3261874796082289\n",
      "step 350 : Total_loss & DTIloss & L2_loss: 498944.1 , 1361.376708984375 , 39319.7109375\n",
      "Valid auc & aupr: 0.9226903872508125 0.4500497520576525 ;   Test auc & aupr: 0.9367683534067016 0.36910274306873503\n",
      "step 375 : Total_loss & DTIloss & L2_loss: 455732.06 , 1291.09423828125 , 37697.20703125\n",
      "Valid auc & aupr: 0.9235419532152314 0.4770639783709977 ;   Test auc & aupr: 0.9386488182135794 0.4011349399344155\n",
      "step 400 : Total_loss & DTIloss & L2_loss: 413196.2 , 1217.3399658203125 , 36381.21484375\n",
      "Valid auc & aupr: 0.9268780674788988 0.5007663604302618 ;   Test auc & aupr: 0.9465795631429874 0.42861720258707736\n",
      "step 425 : Total_loss & DTIloss & L2_loss: 372632.03 , 1151.888916015625 , 35301.87109375\n",
      "Valid auc & aupr: 0.9397520211851774 0.5261199806730028 ;   Test auc & aupr: 0.9528099888793228 0.466368331884524\n",
      "step 450 : Total_loss & DTIloss & L2_loss: 339092.4 , 1094.8251953125 , 34490.32421875\n",
      "Valid auc & aupr: 0.9511934199680819 0.5450576170219296 ;   Test auc & aupr: 0.9555753410289043 0.49216165604783996\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import time\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "\n",
    "def loda_data():\n",
    "    network_path = 'E:/Alzahra-University/GADTI-main/data/'\n",
    "    \n",
    "\n",
    "    drug_drug = np.loadtxt(network_path + '/' + 'mat_drug_drug.txt')\n",
    "    true_drug = 708\n",
    "    drug_chemical = np.loadtxt(network_path + '/' + 'Similarity_Matrix_Drugs.txt')\n",
    "    drug_chemical = drug_chemical[:true_drug, :true_drug]\n",
    "    drug_disease = np.loadtxt(network_path + '/' + 'mat_drug_disease.txt')\n",
    "    drug_sideeffect = np.loadtxt(network_path + '/' + 'mat_drug_se.txt')\n",
    "\n",
    "    protein_protein = np.loadtxt(network_path + '/' + 'mat_protein_protein.txt')\n",
    "    protein_sequence = np.loadtxt(network_path + '/' + 'Similarity_Matrix_Proteins.txt')\n",
    "    protein_disease = np.loadtxt(network_path + '/' + 'mat_protein_disease.txt')\n",
    "\n",
    "    num_drug = len(drug_drug)\n",
    "    num_protein = len(protein_protein)\n",
    "\n",
    "    # Removed the self-loop\n",
    "    drug_chemical = drug_chemical - np.identity(num_drug)\n",
    "    protein_sequence = protein_sequence / 100.\n",
    "    protein_sequence = protein_sequence - np.identity(num_protein)\n",
    "\n",
    "    drug_protein = np.loadtxt(network_path + '/' + 'mat_drug_protein.txt')\n",
    "\n",
    "    # Removed DTIs with similar drugs or proteins\n",
    "    #drug_protein = np.loadtxt(network_path + 'mat_drug_protein_homo_protein_drug.txt')\n",
    "\n",
    "    print(\"Load data finished.\")\n",
    "\n",
    "    return drug_drug, drug_chemical, drug_disease, drug_sideeffect, protein_protein, protein_sequence, \\\n",
    "           protein_disease, drug_protein\n",
    "\n",
    "\n",
    "def ConstructGraph(drug_drug, drug_chemical, drug_disease, drug_sideeffect, protein_protein, protein_sequence,\n",
    "                   protein_disease, drug_protein):\n",
    "    num_drug = len(drug_drug)\n",
    "    num_protein = len(protein_protein)\n",
    "    num_disease = len(drug_disease.T)\n",
    "    num_sideeffect = len(drug_sideeffect.T)\n",
    "\n",
    "    list_drug = []\n",
    "    for i in range(num_drug):\n",
    "        list_drug.append((i, i))\n",
    "\n",
    "    list_protein = []\n",
    "    for i in range(num_protein):\n",
    "        list_protein.append((i, i))\n",
    "\n",
    "    list_disease = []\n",
    "    for i in range(num_disease):\n",
    "        list_disease.append((i, i))\n",
    "\n",
    "    list_sideeffect = []\n",
    "    for i in range(num_sideeffect):\n",
    "        list_sideeffect.append((i, i))\n",
    "\n",
    "    list_DDI = []\n",
    "    for row in range(num_drug):\n",
    "        for col in range(num_drug):\n",
    "            if drug_drug[row, col] > 0:\n",
    "                list_DDI.append((row, col))\n",
    "\n",
    "    list_PPI = []\n",
    "    for row in range(num_protein):\n",
    "        for col in range(num_protein):\n",
    "            if protein_protein[row, col] > 0:\n",
    "                list_PPI.append((row, col))\n",
    "\n",
    "    list_drug_protein = []\n",
    "    list_protein_drug = []\n",
    "    for row in range(num_drug):\n",
    "        for col in range(num_protein):\n",
    "            if drug_protein[row, col] > 0:\n",
    "                list_drug_protein.append((row, col))\n",
    "                list_protein_drug.append((col, row))\n",
    "\n",
    "    list_drug_sideeffect = []\n",
    "    list_sideeffect_drug = []\n",
    "    for row in range(num_drug):\n",
    "        for col in range(num_sideeffect):\n",
    "            if drug_sideeffect[row, col] > 0:\n",
    "                list_drug_sideeffect.append((row, col))\n",
    "                list_sideeffect_drug.append((col, row))\n",
    "\n",
    "    list_drug_disease = []\n",
    "    list_disease_drug = []\n",
    "    for row in range(num_drug):\n",
    "        for col in range(num_disease):\n",
    "            if drug_disease[row, col] > 0:\n",
    "                list_drug_disease.append((row, col))\n",
    "                list_disease_drug.append((col, row))\n",
    "\n",
    "    list_protein_disease = []\n",
    "    list_disease_protein = []\n",
    "    for row in range(num_protein):\n",
    "        for col in range(num_disease):\n",
    "            if protein_disease[row, col] > 0:\n",
    "                list_protein_disease.append((row, col))\n",
    "                list_disease_protein.append((col, row))\n",
    "\n",
    "    g_HIN = dgl.heterograph({('disease', 'disease_disease virtual', 'disease'): list_disease,\n",
    "                             ('drug', 'drug_drug virtual', 'drug'): list_drug,\n",
    "                             ('protein', 'protein_protein virtual', 'protein'): list_protein,\n",
    "                             ('sideeffect', 'sideeffect_sideeffect virtual', 'sideeffect'): list_sideeffect,\n",
    "                             ('drug', 'drug_drug interaction', 'drug'): list_DDI, \\\n",
    "                             ('protein', 'protein_protein interaction', 'protein'): list_PPI, \\\n",
    "                             ('drug', 'drug_protein interaction', 'protein'): list_drug_protein, \\\n",
    "                             ('protein', 'protein_drug interaction', 'drug'): list_protein_drug, \\\n",
    "                             ('drug', 'drug_sideeffect association', 'sideeffect'): list_drug_sideeffect, \\\n",
    "                             ('sideeffect', 'sideeffect_drug association', 'drug'): list_sideeffect_drug, \\\n",
    "                             ('drug', 'drug_disease association', 'disease'): list_drug_disease, \\\n",
    "                             ('disease', 'disease_drug association', 'drug'): list_disease_drug, \\\n",
    "                             ('protein', 'protein_disease association', 'disease'): list_protein_disease, \\\n",
    "                             ('disease', 'disease_protein association', 'protein'): list_disease_protein})\n",
    "\n",
    "    g = g_HIN.edge_type_subgraph(['drug_drug interaction', 'protein_protein interaction',\n",
    "                                  'drug_protein interaction', 'protein_drug interaction',\n",
    "                                  'drug_sideeffect association', 'sideeffect_drug association',\n",
    "                                  'drug_disease association', 'disease_drug association',\n",
    "                                  'protein_disease association', 'disease_protein association'\n",
    "                                  ])\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "def TrainAndEvaluate(DTItrain, DTIvalid, DTItest, args, drug_drug, drug_chemical, drug_disease,\n",
    "                     drug_sideeffect, protein_protein, protein_sequence, protein_disease):\n",
    "    device = th.device(args.device)\n",
    "\n",
    "    # Numbers of different nodes\n",
    "    num_disease = len(drug_disease.T)\n",
    "    num_drug = len(drug_drug)\n",
    "    num_protein = len(protein_protein)\n",
    "    num_sideeffect = len(drug_sideeffect.T)\n",
    "\n",
    "    drug_protein = th.zeros((num_drug, num_protein))\n",
    "    mask = th.zeros((num_drug, num_protein)).to(device)\n",
    "    for ele in DTItrain:\n",
    "        drug_protein[ele[0], ele[1]] = ele[2]\n",
    "        mask[ele[0], ele[1]] = 1\n",
    "\n",
    "    best_valid_aupr = 0.\n",
    "    # best_valid_auc = 0\n",
    "    test_aupr = 0.\n",
    "    test_auc = 0.\n",
    "    patience = 0.\n",
    "\n",
    "    pos = np.count_nonzero(DTItest[:, 2])\n",
    "    neg = np.size(DTItest[:, 2]) - pos\n",
    "    xy_roc_sampling = []\n",
    "    xy_pr_sampling = []\n",
    "\n",
    "    g = ConstructGraph(drug_drug, drug_chemical, drug_disease, drug_sideeffect, protein_protein, protein_sequence,\n",
    "                       protein_disease, drug_protein)\n",
    "    g.to('cuda')\n",
    "\n",
    "    drug_drug = th.tensor(drug_drug).to(device)\n",
    "    drug_chemical = th.tensor(drug_chemical).to(device)\n",
    "    drug_disease = th.tensor(drug_disease).to(device)\n",
    "    drug_sideeffect = th.tensor(drug_sideeffect).to(device)\n",
    "    protein_protein = th.tensor(protein_protein).to(device)\n",
    "    protein_sequence = th.tensor(protein_sequence).to(device)\n",
    "    protein_disease = th.tensor(protein_disease).to(device)\n",
    "    drug_protein = drug_protein.to(device)\n",
    "\n",
    "    model = GRDTI(g, num_disease, num_drug, num_protein, num_sideeffect, args)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = th.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    for i in range(args.epochs):\n",
    "\n",
    "        model.train()\n",
    "        tloss, dtiloss, l2loss, dp_re, DTI_p = model(drug_drug, drug_chemical, drug_disease, drug_sideeffect,\n",
    "                                                     protein_protein, protein_sequence, protein_disease,\n",
    "                                                     drug_protein, mask)\n",
    "\n",
    "        results = dp_re.detach().cpu()\n",
    "        optimizer.zero_grad()\n",
    "        loss = tloss\n",
    "        loss.backward()\n",
    "        th.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            with th.no_grad():\n",
    "                print(\"step\", i, \":\", \"Total_loss & DTIloss & L2_loss:\", loss.cpu().data.numpy(), \",\", dtiloss.item(),\n",
    "                      \",\", l2loss.item())\n",
    "\n",
    "                pred_list = []\n",
    "                ground_truth = []\n",
    "\n",
    "                for ele in DTIvalid:\n",
    "                    pred_list.append(results[ele[0], ele[1]])\n",
    "                    ground_truth.append(ele[2])\n",
    "\n",
    "                valid_auc = roc_auc_score(ground_truth, pred_list)\n",
    "                valid_aupr = average_precision_score(ground_truth, pred_list)\n",
    "\n",
    "                if valid_aupr >= best_valid_aupr:\n",
    "                    best_valid_aupr = valid_aupr\n",
    "                    # best_valid_auc = valid_auc\n",
    "                    best_DTI_potential = DTI_p\n",
    "                    patience = 0\n",
    "\n",
    "                    # Calculating AUC & AUPR (pos:neg=1:10)\n",
    "                    '''db = []\n",
    "                    xy_roc = []\n",
    "                    xy_pr = []\n",
    "                    for ele in DTItest:\n",
    "                        db.append([results[ele[0], ele[1]], ele[2]])\n",
    "\n",
    "                    db = sorted(db, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "                    tp, fp = 0., 0.\n",
    "                    for i_db in range(len(db)):\n",
    "                        if db[i_db][0]:\n",
    "                            if db[i_db][1]:\n",
    "                                tp = tp + 1\n",
    "                            else:\n",
    "                                fp = fp + 1\n",
    "                            xy_roc.append([fp / neg, tp / pos])\n",
    "                            xy_pr.append([tp / pos, tp / (tp + fp)])\n",
    "\n",
    "                    test_auc = 0.\n",
    "                    prev_x = 0.\n",
    "                    for x, y in xy_roc:\n",
    "                        if x != prev_x:\n",
    "                            test_auc += (x - prev_x) * y\n",
    "                            prev_x = x\n",
    "\n",
    "                    test_aupr = 0.\n",
    "                    prev_x = 0.\n",
    "                    for x, y in xy_pr:\n",
    "                        if x != prev_x:\n",
    "                            test_aupr += (x - prev_x) * y\n",
    "                            prev_x = x'''\n",
    "\n",
    "                    # All unknown DTI pairs all treated as negative examples\n",
    "                    pred_list = []\n",
    "                    ground_truth = []\n",
    "                    for ele in DTItest:\n",
    "                        pred_list.append(results[ele[0], ele[1]])\n",
    "                        ground_truth.append(ele[2])\n",
    "                    test_auc = roc_auc_score(ground_truth, pred_list)\n",
    "                    test_aupr = average_precision_score(ground_truth, pred_list)\n",
    "\n",
    "                else:\n",
    "                    patience += 1\n",
    "                    if patience > args.patience:\n",
    "                        print(\"Early Stopping\")\n",
    "\n",
    "                        # sampling (pos:neg=1:10) for averaging and plotting\n",
    "                        '''xy_roc_sampling = []\n",
    "                        xy_pr_sampling = []\n",
    "                        for i_xy in range(len(xy_roc)):\n",
    "                            if i_xy % 10 == 0:\n",
    "                                xy_roc_sampling.append(xy_roc[i_xy])\n",
    "                                xy_pr_sampling.append(xy_pr[i_xy])'''\n",
    "\n",
    "                        # Record data for sampling, averaging and plotting.\n",
    "                        # All unknown DTI pairs all treated as negative examples\n",
    "                        #t1 = time.localtime()\n",
    "                        #time_creat_txt = str(t1.tm_year) + '_' + str(t1.tm_mon) + '_' + str(t1.tm_mday) + '_' + str(\n",
    "                        #    t1.tm_hour) + '_' + str(t1.tm_min)\n",
    "\n",
    "                        fpr, tpr, threshold = roc_curve(ground_truth, pred_list)\n",
    "                        print(\"len(fpr):\", len(fpr))\n",
    "                        np.savetxt('fpr_.csv', fpr)\n",
    "                        np.savetxt('tpr_.csv', tpr)\n",
    "                        np.savetxt('ROC_threshold_.csv', threshold)\n",
    "                        precision, recall, threshold = precision_recall_curve(ground_truth, pred_list)\n",
    "                        print(\"len(recall):\", len(recall))\n",
    "                        np.savetxt('precision_.csv', precision)\n",
    "                        np.savetxt('recall_.csv', recall)\n",
    "                        np.savetxt('PRC_threshold_.csv', threshold)\n",
    "                        \n",
    "                        break\n",
    "\n",
    "                print('Valid auc & aupr:', valid_auc, valid_aupr, \";  \", 'Test auc & aupr:', test_auc, test_aupr)\n",
    "\n",
    "    return test_auc, test_aupr, xy_roc_sampling, xy_pr_sampling, best_DTI_potential\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    drug_d, drug_ch, drug_di, drug_side, protein_p, protein_seq, protein_di, dti_original = loda_data()\n",
    "\n",
    "    # sampling\n",
    "    whole_positive_index = []\n",
    "    whole_negative_index = []\n",
    "    for i in range(np.shape(dti_original)[0]):\n",
    "        for j in range(np.shape(dti_original)[1]):\n",
    "            if int(dti_original[i][j]) == 1:\n",
    "                whole_positive_index.append([i, j])\n",
    "            elif int(dti_original[i][j]) == 0:\n",
    "                whole_negative_index.append([i, j])\n",
    "\n",
    "    # pos:neg=1:10\n",
    "    '''negative_sample_index = np.random.choice(np.arange(len(whole_negative_index)),\n",
    "                                             size=10 * len(whole_positive_index), replace=False)'''\n",
    "\n",
    "    # All unknown DTI pairs all treated as negative examples\n",
    "    negative_sample_index = np.random.choice(np.arange(len(whole_negative_index)),\n",
    "                                             size=len(whole_negative_index), replace=False)\n",
    "\n",
    "    data_set = np.zeros((len(negative_sample_index) + len(whole_positive_index), 3), dtype=int)\n",
    "    count = 0\n",
    "    for i in whole_positive_index:\n",
    "        data_set[count][0] = i[0]\n",
    "        data_set[count][1] = i[1]\n",
    "        data_set[count][2] = 1\n",
    "        count += 1\n",
    "    for i in negative_sample_index:\n",
    "        data_set[count][0] = whole_negative_index[i][0]\n",
    "        data_set[count][1] = whole_negative_index[i][1]\n",
    "        data_set[count][2] = 0\n",
    "        count += 1\n",
    "\n",
    "    test_auc_round = []\n",
    "    test_aupr_round = []\n",
    "    tpr_mean = []\n",
    "    fpr = []\n",
    "    precision_mean = []\n",
    "    recall = []\n",
    "\n",
    "    rounds = args.rounds\n",
    "    for r in range(rounds):\n",
    "        print(\"----------------------------------------\")\n",
    "\n",
    "        test_auc_fold = []\n",
    "        test_aupr_fold = []\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=10, random_state=None, shuffle=True)\n",
    "        k_fold = 0\n",
    "\n",
    "        for train_index, test_index in kf.split(data_set[:, :2], data_set[:, 2]):\n",
    "            train = data_set[train_index]\n",
    "            DTItest = data_set[test_index]\n",
    "            DTItrain, DTIvalid = train_test_split(train, test_size=0.05, random_state=None)\n",
    "            \n",
    "            '''X_train = DTItrain[:, :2]\n",
    "            y_train = DTItrain[:, 2]\n",
    "            #counter = Counter(y_train)\n",
    "            #print(counter)\n",
    "            undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
    "            X_train_underSample, y_train_underSample  = undersample.fit_resample(X_train, y_train)\n",
    "            #counter = Counter(X_train_underSample)\n",
    "            #print(counter)\n",
    "            DTItrain = np.hstack((X_train_underSample, np.vstack(y_train_underSample)))'''\n",
    "\n",
    "            k_fold += 1\n",
    "            print(\"--------------------------------------------------------------\")\n",
    "            print(\"round \", r + 1, \" of \", rounds, \":\", \"KFold \", k_fold, \" of 10\")\n",
    "            print(\"--------------------------------------------------------------\")\n",
    "\n",
    "            time_roundStart = time.time()\n",
    "\n",
    "            t_auc, t_aupr, xy_roc, xy_pr, DTI_potential = TrainAndEvaluate(DTItrain, DTIvalid, DTItest, args, drug_d,\n",
    "                                                                           drug_ch, drug_di, drug_side, protein_p,\n",
    "                                                                           protein_seq, protein_di)\n",
    "\n",
    "            time_roundEnd = time.time()\n",
    "            print(\"Time spent in this fold:\", time_roundEnd - time_roundStart)\n",
    "            test_auc_fold.append(t_auc)\n",
    "            test_aupr_fold.append(t_aupr)\n",
    "\n",
    "            order_txt1 = 'DTI_potential_' + 'r' + str(r + 1) + '_f' + str(k_fold) + '.csv'\n",
    "            np.savetxt(order_txt1, DTI_potential.detach().cpu().numpy(), fmt='%-.4f', delimiter=',')\n",
    "            top_values, top_indices = th.topk(DTI_potential, 40)\n",
    "            order_txt2 = 'top40_' + 'r' + str(r + 1) + '_f' + str(k_fold) + '.csv'\n",
    "            np.savetxt(order_txt2, top_indices.detach().cpu().numpy(), fmt='%d', delimiter=',')\n",
    "\n",
    "            # pos:neg=1:10\n",
    "            '''if not fpr:\n",
    "                fpr = [_v[0] for _v in xy_roc]\n",
    "            if not recall:\n",
    "                recall = [_v[0] for _v in xy_pr]\n",
    "\n",
    "            temp = [_v[1] for _v in xy_roc]\n",
    "            tpr_mean.append(temp)\n",
    "            temp = [_v[1] for _v in xy_pr]\n",
    "            precision_mean.append(temp)'''\n",
    "\n",
    "        print(\"Training and evaluation is OK.\")\n",
    "\n",
    "        test_auc_round.append(np.mean(test_auc_fold))\n",
    "        test_aupr_round.append(np.mean(test_aupr_fold))\n",
    "\n",
    "    #t1 = time.localtime()\n",
    "    #time_creat_txt = str(t1.tm_year) + '_' + str(t1.tm_mon) + '_' + str(t1.tm_mday) + '_' + str(t1.tm_hour) + '_' + str(\n",
    "    #    t1.tm_min)\n",
    "    \n",
    "    np.savetxt('test_auc_' , test_auc_round)\n",
    "    np.savetxt('test_aupr_' , test_aupr_round)\n",
    "    \n",
    "\n",
    "    # pos:neg=1:10\n",
    "    '''tpr = (np.mean(np.array(tpr_mean), axis=0)).tolist()\n",
    "    precision = (np.mean(np.array(precision_mean), axis=0)).tolist()\n",
    "\n",
    "    np.savetxt('fpr.csv', fpr, fmt='%-.4f', delimiter=',')\n",
    "    np.savetxt('tpr.csv', tpr, fmt='%-.4f', delimiter=',')\n",
    "    np.savetxt('recall.csv', recall, fmt='%-.4f', delimiter=',')\n",
    "    np.savetxt('precision.csv', precision, fmt='%-.4f', delimiter=',')'''\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    print(args)\n",
    "\n",
    "    start = time.time()\n",
    "    main(args)\n",
    "    end = time.time()\n",
    "    print(\"Total time:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddbe374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "'''network_path = 'MSCMF/'\n",
    "fpr_o = np.loadtxt(network_path + 'fpr.csv', delimiter=',')\n",
    "tpr_o = np.loadtxt(network_path + 'tpr.csv', delimiter=',')\n",
    "recall_o = np.loadtxt(network_path + 'recall.csv', delimiter=',')\n",
    "precision_o = np.loadtxt(network_path + 'precision.csv', delimiter=',')'''\n",
    "\n",
    "network_path = ''\n",
    "fpr = np.loadtxt(network_path +'fpr_.csv', delimiter=',')\n",
    "tpr = np.loadtxt(network_path +'tpr_.csv', delimiter=',')\n",
    "recall = np.loadtxt(network_path +'recall_.csv', delimiter=',')\n",
    "precision = np.loadtxt(network_path +'precision_.csv', delimiter=',')\n",
    "\n",
    "'''roc, prc = [], []\n",
    "auc, aupr = [], []\n",
    "color = ['blue', 'orange', 'gray', 'gold', 'royalblue']\n",
    "label = ['MSCMF', 'TL_HGBI', 'DTINet', 'NeoDTI', 'GADTI']\n",
    "for i in range(5):\n",
    "    roc.append(np.loadtxt('roc' + str(i + 1) + '.csv'))\n",
    "    prc.append(np.loadtxt('prc' + str(i + 1) + '.csv'))\n",
    "for i in range(5):\n",
    "    for x, y in roc[i]:\n",
    "        test_auc = 0.\n",
    "        prev_x = 0.\n",
    "        if x != prev_x:\n",
    "            test_auc += (x - prev_x) * y\n",
    "            prev_x = x\n",
    "        auc.append(test_auc)\n",
    "    for x, y in prc[i]:\n",
    "        test_aupr = 0.\n",
    "        prev_x = 0.\n",
    "        if x != prev_x:\n",
    "            test_aupr += (x - prev_x) * y\n",
    "            prev_x = x\n",
    "        aupr.append(test_aupr)\n",
    "plt.title(\"ROC curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "for i in range(5):\n",
    "    x = [x_[0] for x_ in roc[i]]\n",
    "    y = [x_[1] for x_ in roc[i]]\n",
    "    plt.plot(x, y, linewidth='1', label=label[i], color=color[i])\n",
    "    # plt.plot(x, y, linewidth='1', label=\"test\", color=' coral ', linestyle=':', marker='|')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.title(\"PR curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "for i in range(5):\n",
    "    x = [x_[0] for x_ in prc[i]]\n",
    "    y = [x_[1] for x_ in prc[i]]\n",
    "    plt.plot(x, y, linewidth='1', label=label[i], color=color[i])\n",
    "plt.legend()\n",
    "plt.show()'''\n",
    "\n",
    "'''fpr = []\n",
    "tpr = []\n",
    "recall=[]\n",
    "precision=[]\n",
    "for i in (range(len(fpr_o))):\n",
    "    if i % 10== 0:\n",
    "        fpr.append(fpr_o[i])\n",
    "        tpr.append(tpr_o[i])\n",
    "for i in (range(len(recall_o))):\n",
    "    if i % 10== 0:\n",
    "        recall.append(recall_o[i])\n",
    "        precision.append(precision_o[i])\n",
    "np.savetxt('fpr1.csv', fpr, fmt='%-.4f', delimiter=',')\n",
    "np.savetxt('tpr1.csv', tpr, fmt='%-.4f', delimiter=',')\n",
    "np.savetxt('recall1.csv', recall, fmt='%-.4f', delimiter=',')\n",
    "np.savetxt('precision1.csv', precision, fmt='%-.4f', delimiter=',')'''\n",
    "\n",
    "auc = 0.\n",
    "prev_x = 0.\n",
    "for i in range(len(fpr)):\n",
    "    if fpr[i] != prev_x:\n",
    "        auc += (fpr[i] - prev_x) * tpr[i]\n",
    "        prev_x = fpr[i]\n",
    "\n",
    "aupr = 0.\n",
    "prev_x = 0.\n",
    "for i in range(len(recall)):\n",
    "    if recall[i] != prev_x:\n",
    "        aupr += (recall[i] - prev_x) * precision[i]\n",
    "        prev_x = recall[i]\n",
    "\n",
    "plt.title(\"ROC curve of %s (AUC = %.4f)\" % ('GADTI', auc))\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.plot(fpr, tpr, 'r')\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"PR curve of %s (AUPR = %.4f)\" % ('GADTI', aupr))\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.plot(recall, precision)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57609565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
